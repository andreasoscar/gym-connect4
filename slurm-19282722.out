04/07/2022 02:05:45 AM [INFO]: Preparing model for multi-process MCTS...
04/07/2022 02:05:45 AM [INFO]: Loaded cc4_current_net__iter3.pth.tar model.
04/07/2022 02:05:45 AM [INFO]: Spawning 32 processes...
START TIME:  02:05:31
04/07/2022 02:05:55 AM [INFO]: [CPU: 0]: Starting MCTS self-play...
04/07/2022 02:05:55 AM [INFO]: [CPU: 3]: Starting MCTS self-play...
04/07/2022 02:05:55 AM [INFO]: [CPU: 8]: Starting MCTS self-play...
04/07/2022 02:05:55 AM [INFO]: [CPU: 6]: Starting MCTS self-play...
04/07/2022 02:05:55 AM [INFO]: [CPU: 7]: Starting MCTS self-play...
04/07/2022 02:05:55 AM [INFO]: [CPU: 1]: Starting MCTS self-play...
04/07/2022 02:05:55 AM [INFO]: [CPU: 4]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:55 AM [INFO]: [CPU: 6]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:55 AM [INFO]: [CPU: 0]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:55 AM [INFO]: [CPU: 4]: Game 0
04/07/2022 02:05:55 AM [INFO]: [CPU: 8]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:55 AM [INFO]: [CPU: 3]: Game 0
04/07/2022 02:05:55 AM [INFO]: [CPU: 7]: Game 0
04/07/2022 02:05:55 AM [INFO]: [CPU: 1]: Game 0
04/07/2022 02:05:55 AM [INFO]: [CPU: 2]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:55 AM [INFO]: [CPU: 2]: Game 0
04/07/2022 02:05:55 AM [INFO]: [CPU: 10]: Starting MCTS self-play...
04/07/2022 02:05:55 AM [INFO]: [CPU: 5]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:55 AM [INFO]: [CPU: 5]: Game 0
04/07/2022 02:05:55 AM [INFO]: [CPU: 10]: Game 0
04/07/2022 02:05:56 AM [INFO]: [CPU: 9]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:56 AM [INFO]: [CPU: 9]: Game 0
04/07/2022 02:05:57 AM [INFO]: [CPU: 13]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:57 AM [INFO]: [CPU: 13]: Game 0
04/07/2022 02:05:57 AM [INFO]: [CPU: 11]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:57 AM [INFO]: [CPU: 11]: Game 0
04/07/2022 02:05:58 AM [INFO]: [CPU: 12]: Starting MCTS self-play...
04/07/2022 02:05:58 AM [INFO]: [CPU: 15]: Starting MCTS self-play...
04/07/2022 02:05:58 AM [INFO]: [CPU: 14]: Starting MCTS self-play...
04/07/2022 02:05:58 AM [INFO]: [CPU: 19]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:58 AM [INFO]: [CPU: 12]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:58 AM [INFO]: [CPU: 15]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:58 AM [INFO]: [CPU: 19]: Game 0
04/07/2022 02:05:58 AM [INFO]: [CPU: 17]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:58 AM [INFO]: [CPU: 14]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:58 AM [INFO]: [CPU: 17]: Game 0
04/07/2022 02:05:58 AM [INFO]: [CPU: 20]: Starting MCTS self-play...
04/07/2022 02:05:58 AM [INFO]: [CPU: 18]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:58 AM [INFO]: [CPU: 20]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:58 AM [INFO]: [CPU: 18]: Game 0
04/07/2022 02:05:58 AM [INFO]: [CPU: 16]: Starting MCTS self-play...
04/07/2022 02:05:58 AM [INFO]: [CPU: 22]: Starting MCTS self-play...
04/07/2022 02:05:58 AM [INFO]: [CPU: 21]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:58 AM [INFO]: [CPU: 16]: Game 0
04/07/2022 02:05:58 AM [INFO]: [CPU: 23]: Starting MCTS self-play...
04/07/2022 02:05:58 AM [INFO]: [CPU: 27]: Starting MCTS self-play...
04/07/2022 02:05:58 AM [INFO]: [CPU: 25]: Starting MCTS self-play...
04/07/2022 02:05:58 AM [INFO]: [CPU: 31]: Starting MCTS self-play...
04/07/2022 02:05:58 AM [INFO]: [CPU: 30]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:58 AM [INFO]: [CPU: 24]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:58 AM [INFO]: [CPU: 31]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:58 AM [INFO]: [CPU: 27]: Game 0
04/07/2022 02:05:58 AM [INFO]: [CPU: 22]: Game 0
04/07/2022 02:05:58 AM [INFO]: [CPU: 25]: Game 0
04/07/2022 02:05:58 AM [INFO]: [CPU: 23]: Game 0
04/07/2022 02:05:58 AM [INFO]: [CPU: 21]: Game 0
04/07/2022 02:05:58 AM [INFO]: [CPU: 26]: Starting MCTS self-play...
04/07/2022 02:05:58 AM [INFO]: [CPU: 29]: Starting MCTS self-play...
04/07/2022 02:05:58 AM [INFO]: [CPU: 28]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:58 AM [INFO]: [CPU: 30]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:58 AM [INFO]: [CPU: 29]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:58 AM [INFO]: [CPU: 26]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:58 AM [INFO]: [CPU: 24]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 02:05:58 AM [INFO]: [CPU: 28]: Game 0
  0%|          | 0/10 [00:01<?, ?it/s]
  0%|          | 0/10 [00:01<?, ?it/s]  0%|          | 0/10 [00:01<?, ?it/s]
  0%|          | 0/10 [00:01<?, ?it/s]  0%|          | 0/10 [00:01<?, ?it/s]  0%|          | 0/10 [00:01<?, ?it/s]
  0%|          | 0/10 [00:01<?, ?it/s]  0%|          | 0/10 [00:01<?, ?it/s]  0%|          | 0/10 [00:01<?, ?it/s]  0%|          | 0/10 [00:01<?, ?it/s]
Process Process-16:
  0%|          | 0/10 [00:01<?, ?it/s]  0%|          | 0/10 [00:01<?, ?it/s]

  0%|          | 0/10 [00:01<?, ?it/s]Process Process-26:



Process Process-27:




Process Process-29:
Process Process-25:
Process Process-21:
Process Process-32:
Process Process-19:
  0%|          | 0/10 [00:01<?, ?it/s]  0%|          | 0/10 [00:02<?, ?it/s]
  0%|          | 0/10 [00:03<?, ?it/s]
Process Process-28:
Process Process-12:

Process Process-18:
Process Process-20:
Process Process-23:
Process Process-17:
Process Process-14:
Process Process-7:
  0%|          | 0/10 [00:03<?, ?it/s]
  0%|          | 0/10 [00:03<?, ?it/s]
  0%|          | 0/10 [00:01<?, ?it/s]
Process Process-15:
Process Process-1:
Process Process-8:
  0%|          | 0/10 [00:03<?, ?it/s]  0%|          | 0/10 [00:03<?, ?it/s]
  0%|          | 0/10 [00:01<?, ?it/s]

Process Process-24:
  0%|          | 0/10 [00:03<?, ?it/s]
Process Process-9:
Process Process-6:
Process Process-11:
  0%|          | 0/10 [00:01<?, ?it/s]
Process Process-31:
  0%|          | 0/10 [00:03<?, ?it/s]
Process Process-3:
  0%|          | 0/10 [00:03<?, ?it/s]
Process Process-4:
  0%|          | 0/10 [00:03<?, ?it/s]
Process Process-5:
  0%|          | 0/10 [00:01<?, ?it/s]
Process Process-22:
  0%|          | 0/10 [00:01<?, ?it/s]
Process Process-13:
  0%|          | 0/10 [00:03<?, ?it/s]
Process Process-10:
  0%|          | 0/10 [00:03<?, ?it/s]
Process Process-2:
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
Traceback (most recent call last):
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
Traceback (most recent call last):
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  0%|          | 0/10 [00:01<?, ?it/s]RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()

Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
Process Process-30:
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
04/07/2022 02:06:05 AM [INFO]: Finished multi-process MCTS!
04/07/2022 02:06:05 AM [INFO]: Loading training data...
/home/x_aolss/gym_connect4/gym-connect4/train_c4.py:141: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  datasets = np.array(datasets)
04/07/2022 02:06:06 AM [INFO]: Loaded data from ./datasets/iter_3/.
04/07/2022 02:06:06 AM [INFO]: Loaded checkpoint model cc4_current_net__iter3.pth.tar.
04/07/2022 02:06:06 AM [INFO]: Starting training process...
04/07/2022 02:07:20 AM [INFO]: Finished Training!
FINISHED SELF PLAY:  02:06:05
Update step size: 4
[Iteration 3] Process ID: 131596 [Epoch: 1,   128/ 591 points] total loss per batch: 1.207
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0048, 0.0000, 0.9952, 0.0000],
       device='cuda:0')
Policy pred: tensor([1.6679e-04, 4.1507e-05, 2.7667e-04, 7.4065e-03, 3.0414e-06, 9.9179e-01,
        3.1930e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999871253967285
 
[Iteration 3] Process ID: 131596 [Epoch: 1,   256/ 591 points] total loss per batch: 1.368
Policy (actual, predicted): 6 0
Policy data: tensor([0.1360, 0.1442, 0.1419, 0.1419, 0.1454, 0.1383, 0.1524],
       device='cuda:0')
Policy pred: tensor([0.4235, 0.0949, 0.0999, 0.0726, 0.0782, 0.1302, 0.1008],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.0026906663551926613
 
[Iteration 3] Process ID: 131596 [Epoch: 1,   384/ 591 points] total loss per batch: 1.292
Policy (actual, predicted): 0 0
Policy data: tensor([0.6687, 0.0664, 0.1092, 0.0295, 0.0412, 0.0526, 0.0324],
       device='cuda:0')
Policy pred: tensor([0.6445, 0.0631, 0.1382, 0.0318, 0.0443, 0.0485, 0.0296],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998722076416016
 
[Iteration 3] Process ID: 131596 [Epoch: 1,   512/ 591 points] total loss per batch: 1.330
Policy (actual, predicted): 6 6
Policy data: tensor([0.0935, 0.1762, 0.1476, 0.1350, 0.1613, 0.0579, 0.2285],
       device='cuda:0')
Policy pred: tensor([0.0828, 0.1784, 0.1490, 0.1565, 0.1550, 0.0826, 0.1958],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999165534973145
 
[Iteration 3] Process ID: 131596 [Epoch: 2,   128/ 591 points] total loss per batch: 1.179
Policy (actual, predicted): 4 6
Policy data: tensor([0.1419, 0.1454, 0.1407, 0.1407, 0.1489, 0.1360, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1326, 0.1243, 0.1087, 0.1015, 0.1072, 0.0897, 0.3359],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.4546414315700531
 
[Iteration 3] Process ID: 131596 [Epoch: 2,   256/ 591 points] total loss per batch: 1.449
Policy (actual, predicted): 2 2
Policy data: tensor([0.2808, 0.0266, 0.5874, 0.0173, 0.0220, 0.0471, 0.0189],
       device='cuda:0')
Policy pred: tensor([0.2279, 0.0186, 0.6294, 0.0115, 0.0476, 0.0409, 0.0242],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9941685795783997
 
[Iteration 3] Process ID: 131596 [Epoch: 2,   384/ 591 points] total loss per batch: 1.347
Policy (actual, predicted): 1 1
Policy data: tensor([4.3506e-03, 9.6874e-01, 3.9572e-03, 1.5618e-04, 1.6343e-02, 1.5618e-04,
        6.3011e-03], device='cuda:0')
Policy pred: tensor([0.0142, 0.9418, 0.0083, 0.0058, 0.0160, 0.0043, 0.0096],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 2,   512/ 591 points] total loss per batch: 1.299
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0065, 0.0143, 0.0403, 0.0079, 0.0345, 0.0243, 0.8722],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.2661095857620239
 
[Iteration 3] Process ID: 131596 [Epoch: 3,   128/ 591 points] total loss per batch: 1.287
Policy (actual, predicted): 1 1
Policy data: tensor([0.0650, 0.3209, 0.0485, 0.1777, 0.1139, 0.1247, 0.1491],
       device='cuda:0')
Policy pred: tensor([0.0467, 0.3131, 0.0471, 0.1886, 0.1629, 0.0841, 0.1575],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999996423721313
 
[Iteration 3] Process ID: 131596 [Epoch: 3,   256/ 591 points] total loss per batch: 1.338
Policy (actual, predicted): 0 2
Policy data: tensor([0.2785, 0.1397, 0.1668, 0.0548, 0.1527, 0.0548, 0.1527],
       device='cuda:0')
Policy pred: tensor([0.2169, 0.1250, 0.2314, 0.0633, 0.1296, 0.0583, 0.1755],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999996423721313
 
[Iteration 3] Process ID: 131596 [Epoch: 3,   384/ 591 points] total loss per batch: 1.285
Policy (actual, predicted): 3 6
Policy data: tensor([0.0729, 0.1156, 0.0729, 0.3226, 0.1056, 0.0368, 0.2738],
       device='cuda:0')
Policy pred: tensor([0.1289, 0.1032, 0.0721, 0.2615, 0.1031, 0.0405, 0.2908],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999972581863403
 
[Iteration 3] Process ID: 131596 [Epoch: 3,   512/ 591 points] total loss per batch: 1.299
Policy (actual, predicted): 4 2
Policy data: tensor([0.1477, 0.1466, 0.1477, 0.1277, 0.1524, 0.1325, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1451, 0.1439, 0.2019, 0.1369, 0.1469, 0.1103, 0.1150],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999571442604065
 
[Iteration 3] Process ID: 131596 [Epoch: 4,   128/ 591 points] total loss per batch: 1.215
Policy (actual, predicted): 5 5
Policy data: tensor([0.1119, 0.1398, 0.1302, 0.0668, 0.0871, 0.3796, 0.0846],
       device='cuda:0')
Policy pred: tensor([0.1056, 0.1380, 0.1945, 0.0872, 0.1151, 0.2572, 0.1024],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999995231628418
 
[Iteration 3] Process ID: 131596 [Epoch: 4,   256/ 591 points] total loss per batch: 1.338
Policy (actual, predicted): 0 0
Policy data: tensor([0.4173, 0.0708, 0.3789, 0.0123, 0.0154, 0.0748, 0.0305],
       device='cuda:0')
Policy pred: tensor([0.4788, 0.0671, 0.3346, 0.0167, 0.0131, 0.0631, 0.0266],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.13832512497901917
 
[Iteration 3] Process ID: 131596 [Epoch: 4,   384/ 591 points] total loss per batch: 1.281
Policy (actual, predicted): 2 2
Policy data: tensor([0.0767, 0.2093, 0.5781, 0.0074, 0.0686, 0.0336, 0.0263],
       device='cuda:0')
Policy pred: tensor([0.0950, 0.2846, 0.3730, 0.0196, 0.0957, 0.0471, 0.0850],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999977350234985
 
[Iteration 3] Process ID: 131596 [Epoch: 4,   512/ 591 points] total loss per batch: 1.296
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.3818, 0.0441, 0.3392, 0.0833, 0.1516, 0.0000],
       device='cuda:0')
Policy pred: tensor([0.0008, 0.4771, 0.0316, 0.2850, 0.0511, 0.1540, 0.0006],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 5,   128/ 591 points] total loss per batch: 1.378
Policy (actual, predicted): 3 3
Policy data: tensor([0.1339, 0.0000, 0.1733, 0.3619, 0.0000, 0.0283, 0.3026],
       device='cuda:0')
Policy pred: tensor([6.2784e-02, 1.0269e-05, 2.9493e-01, 3.3727e-01, 1.2711e-04, 4.7570e-02,
        2.5731e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999997615814209
 
[Iteration 3] Process ID: 131596 [Epoch: 5,   256/ 591 points] total loss per batch: 1.395
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 1.4355e-15, 1.8845e-19, 7.8128e-12, 1.5633e-17,
        5.8033e-20], device='cuda:0')
Policy pred: tensor([4.6710e-06, 9.9200e-01, 3.9911e-04, 3.5052e-05, 7.0489e-03, 9.8858e-05,
        4.1055e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999091625213623
 
[Iteration 3] Process ID: 131596 [Epoch: 5,   384/ 591 points] total loss per batch: 1.232
Policy (actual, predicted): 1 1
Policy data: tensor([0.0934, 0.6501, 0.1337, 0.0188, 0.0173, 0.0412, 0.0455],
       device='cuda:0')
Policy pred: tensor([0.1397, 0.3948, 0.1297, 0.0707, 0.0949, 0.0779, 0.0924],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.00039807704160921276
 
[Iteration 3] Process ID: 131596 [Epoch: 5,   512/ 591 points] total loss per batch: 1.316
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 4.8059e-10, 1.6616e-09, 2.7480e-17, 1.0000e+00, 5.0091e-09,
        4.7592e-06], device='cuda:0')
Policy pred: tensor([2.9313e-06, 7.8451e-03, 6.1418e-04, 3.7633e-04, 9.8786e-01, 1.2558e-04,
        3.1769e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 6,   128/ 591 points] total loss per batch: 1.385
Policy (actual, predicted): 0 0
Policy data: tensor([0.8966, 0.0098, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.8068, 0.0319, 0.0333, 0.0230, 0.0379, 0.0296, 0.0375],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.02593919076025486
 
[Iteration 3] Process ID: 131596 [Epoch: 6,   256/ 591 points] total loss per batch: 1.266
Policy (actual, predicted): 6 6
Policy data: tensor([2.9859e-01, 9.0344e-02, 3.8911e-03, 1.2060e-04, 1.5514e-01, 2.5303e-04,
        4.5166e-01], device='cuda:0')
Policy pred: tensor([0.2162, 0.0777, 0.0237, 0.0047, 0.1790, 0.0039, 0.4948],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999858140945435
 
[Iteration 3] Process ID: 131596 [Epoch: 6,   384/ 591 points] total loss per batch: 1.254
Policy (actual, predicted): 6 6
Policy data: tensor([0.0480, 0.0177, 0.0193, 0.0111, 0.0509, 0.1023, 0.7508],
       device='cuda:0')
Policy pred: tensor([0.0651, 0.0355, 0.0410, 0.0167, 0.0851, 0.0829, 0.6737],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999462962150574
 
[Iteration 3] Process ID: 131596 [Epoch: 6,   512/ 591 points] total loss per batch: 1.331
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1337, 0.3425, 0.1595, 0.0836, 0.0875, 0.0988, 0.0944],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.02298618108034134
 
[Iteration 3] Process ID: 131596 [Epoch: 7,   128/ 591 points] total loss per batch: 1.389
Policy (actual, predicted): 4 1
Policy data: tensor([0.1466, 0.1454, 0.1512, 0.1277, 0.1536, 0.1336, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1326, 0.4046, 0.1345, 0.0709, 0.0946, 0.0786, 0.0841],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.1580793559551239
 
[Iteration 3] Process ID: 131596 [Epoch: 7,   256/ 591 points] total loss per batch: 1.296
Policy (actual, predicted): 1 6
Policy data: tensor([0.1140, 0.2503, 0.1140, 0.1492, 0.0864, 0.0359, 0.2503],
       device='cuda:0')
Policy pred: tensor([0.1198, 0.2304, 0.1078, 0.1286, 0.1074, 0.0603, 0.2457],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999997019767761
 
[Iteration 3] Process ID: 131596 [Epoch: 7,   384/ 591 points] total loss per batch: 1.358
Policy (actual, predicted): 2 2
Policy data: tensor([0.0081, 0.0135, 0.9478, 0.0063, 0.0063, 0.0118, 0.0063],
       device='cuda:0')
Policy pred: tensor([0.0230, 0.0202, 0.9227, 0.0079, 0.0089, 0.0106, 0.0067],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9976531863212585
 
[Iteration 3] Process ID: 131596 [Epoch: 7,   512/ 591 points] total loss per batch: 1.210
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.0000, 0.2066, 0.1309, 0.0933, 0.2496, 0.3196],
       device='cuda:0')
Policy pred: tensor([2.1125e-06, 4.0526e-06, 1.8848e-01, 1.1037e-01, 7.6078e-02, 2.6416e-01,
        3.6092e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 8,   128/ 591 points] total loss per batch: 1.294
Policy (actual, predicted): 0 0
Policy data: tensor([0.6231, 0.1329, 0.1020, 0.0218, 0.0187, 0.0494, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.5455, 0.1807, 0.1106, 0.0301, 0.0329, 0.0478, 0.0525],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.022496972233057022
 
[Iteration 3] Process ID: 131596 [Epoch: 8,   256/ 591 points] total loss per batch: 1.288
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 0.0000e+00, 4.3964e-19, 8.8631e-17, 2.7601e-13,
        6.2062e-15], device='cuda:0')
Policy pred: tensor([2.4409e-05, 9.9485e-01, 3.2395e-05, 1.7771e-03, 3.2454e-03, 2.5057e-05,
        4.5915e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 8,   384/ 591 points] total loss per batch: 1.198
Policy (actual, predicted): 6 6
Policy data: tensor([6.0353e-19, 0.0000e+00, 0.0000e+00, 1.4753e-15, 2.2723e-22, 1.4070e-21,
        1.0000e+00], device='cuda:0')
Policy pred: tensor([1.5717e-03, 1.4381e-05, 7.4717e-05, 3.4993e-03, 8.3975e-03, 2.4567e-04,
        9.8620e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999635815620422
 
[Iteration 3] Process ID: 131596 [Epoch: 8,   512/ 591 points] total loss per batch: 1.314
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000e+00, 0.0000e+00, 1.0000e+00, 5.1967e-16, 3.1903e-16, 4.3982e-15,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([1.1276e-07, 5.3999e-06, 9.9990e-01, 1.5362e-05, 3.8834e-06, 5.8061e-05,
        1.8680e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999993443489075
 
[Iteration 3] Process ID: 131596 [Epoch: 9,   128/ 591 points] total loss per batch: 1.234
Policy (actual, predicted): 3 3
Policy data: tensor([0.1339, 0.0000, 0.1733, 0.3619, 0.0000, 0.0283, 0.3026],
       device='cuda:0')
Policy pred: tensor([1.2821e-01, 2.8171e-04, 2.2329e-01, 3.3454e-01, 6.7270e-04, 3.1131e-02,
        2.8188e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999694228172302
 
[Iteration 3] Process ID: 131596 [Epoch: 9,   256/ 591 points] total loss per batch: 1.261
Policy (actual, predicted): 1 0
Policy data: tensor([0.1442, 0.1477, 0.1336, 0.1419, 0.1395, 0.1454, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1925, 0.1386, 0.1146, 0.1398, 0.1536, 0.1198, 0.1411],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999972581863403
 
[Iteration 3] Process ID: 131596 [Epoch: 9,   384/ 591 points] total loss per batch: 1.287
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 1.4009e-03, 6.8765e-01, 3.2255e-06, 2.6170e-05,
        3.1092e-01], device='cuda:0')
Policy pred: tensor([3.9733e-04, 8.4277e-05, 1.3979e-02, 7.4480e-01, 6.8020e-03, 1.6234e-03,
        2.3231e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9992172718048096
 
[Iteration 3] Process ID: 131596 [Epoch: 9,   512/ 591 points] total loss per batch: 1.324
Policy (actual, predicted): 0 0
Policy data: tensor([0.1536, 0.1419, 0.1454, 0.1289, 0.1501, 0.1372, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1748, 0.1221, 0.1542, 0.1399, 0.1534, 0.1246, 0.1310],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9918383359909058
 
[Iteration 3] Process ID: 131596 [Epoch: 10,   128/ 591 points] total loss per batch: 1.282
Policy (actual, predicted): 1 6
Policy data: tensor([0.1383, 0.1477, 0.1372, 0.1442, 0.1442, 0.1419, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1198, 0.1137, 0.1098, 0.1193, 0.1194, 0.1211, 0.2968],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.1411459743976593
 
[Iteration 3] Process ID: 131596 [Epoch: 10,   256/ 591 points] total loss per batch: 1.309
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000e+00, 0.0000e+00, 6.5395e-09, 2.8452e-17, 2.0306e-13, 1.0000e+00,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([1.0874e-05, 4.1213e-07, 2.8716e-04, 3.8013e-06, 1.8693e-06, 9.9969e-01,
        7.2336e-06], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999938011169434
 
[Iteration 3] Process ID: 131596 [Epoch: 10,   384/ 591 points] total loss per batch: 1.215
Policy (actual, predicted): 2 2
Policy data: tensor([0.2808, 0.0266, 0.5874, 0.0173, 0.0220, 0.0471, 0.0189],
       device='cuda:0')
Policy pred: tensor([0.2474, 0.0347, 0.5960, 0.0192, 0.0243, 0.0525, 0.0259],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9980025291442871
 
[Iteration 3] Process ID: 131596 [Epoch: 10,   512/ 591 points] total loss per batch: 1.266
Policy (actual, predicted): 6 0
Policy data: tensor([0.1407, 0.1419, 0.1442, 0.1372, 0.1454, 0.1325, 0.1582],
       device='cuda:0')
Policy pred: tensor([0.1545, 0.1499, 0.1460, 0.1298, 0.1471, 0.1296, 0.1430],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9995545744895935
 
[Iteration 3] Process ID: 131596 [Epoch: 11,   128/ 591 points] total loss per batch: 1.187
Policy (actual, predicted): 4 1
Policy data: tensor([0.1466, 0.1466, 0.1477, 0.1325, 0.1524, 0.1325, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1191, 0.4107, 0.1471, 0.0690, 0.0739, 0.0858, 0.0944],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.004533106926828623
 
[Iteration 3] Process ID: 131596 [Epoch: 11,   256/ 591 points] total loss per batch: 1.272
Policy (actual, predicted): 6 1
Policy data: tensor([0.1360, 0.1512, 0.1442, 0.1419, 0.1430, 0.1313, 0.1524],
       device='cuda:0')
Policy pred: tensor([0.1485, 0.1610, 0.1566, 0.1354, 0.1104, 0.1279, 0.1600],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9996716976165771
 
[Iteration 3] Process ID: 131596 [Epoch: 11,   384/ 591 points] total loss per batch: 1.243
Policy (actual, predicted): 6 1
Policy data: tensor([0.1383, 0.1454, 0.1395, 0.1407, 0.1465, 0.1407, 0.1489],
       device='cuda:0')
Policy pred: tensor([0.1230, 0.1727, 0.1417, 0.1485, 0.1583, 0.1364, 0.1194],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9332199096679688
 
[Iteration 3] Process ID: 131596 [Epoch: 11,   512/ 591 points] total loss per batch: 1.339
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1044, 0.4165, 0.1405, 0.0740, 0.0877, 0.0874, 0.0896],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.008434389717876911
 
[Iteration 3] Process ID: 131596 [Epoch: 12,   128/ 591 points] total loss per batch: 1.224
Policy (actual, predicted): 4 1
Policy data: tensor([0.1454, 0.1489, 0.1477, 0.1301, 0.1559, 0.1325, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1346, 0.3877, 0.1388, 0.0691, 0.0785, 0.0873, 0.1039],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.006506090518087149
 
[Iteration 3] Process ID: 131596 [Epoch: 12,   256/ 591 points] total loss per batch: 1.252
Policy (actual, predicted): 2 2
Policy data: tensor([0.1977, 0.1435, 0.5532, 0.0247, 0.0232, 0.0407, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.1566, 0.1382, 0.6078, 0.0195, 0.0188, 0.0421, 0.0171],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9960998296737671
 
[Iteration 3] Process ID: 131596 [Epoch: 12,   384/ 591 points] total loss per batch: 1.329
Policy (actual, predicted): 6 6
Policy data: tensor([0.1430, 0.1466, 0.1384, 0.1454, 0.1360, 0.1313, 0.1594],
       device='cuda:0')
Policy pred: tensor([0.1590, 0.1487, 0.1230, 0.1454, 0.1324, 0.1297, 0.1619],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999852180480957
 
[Iteration 3] Process ID: 131596 [Epoch: 12,   512/ 591 points] total loss per batch: 1.330
Policy (actual, predicted): 4 6
Policy data: tensor([0.1419, 0.1454, 0.1407, 0.1407, 0.1489, 0.1360, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.0999, 0.1111, 0.0920, 0.1025, 0.1099, 0.0975, 0.3870],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.5223690271377563
 
[Iteration 3] Process ID: 131596 [Epoch: 13,   128/ 591 points] total loss per batch: 1.222
Policy (actual, predicted): 1 1
Policy data: tensor([2.9505e-19, 9.9981e-01, 6.4039e-15, 1.9154e-04, 6.0232e-16, 1.0200e-20,
        6.4039e-15], device='cuda:0')
Policy pred: tensor([7.2415e-04, 9.9617e-01, 2.9161e-04, 2.0330e-03, 1.9059e-05, 5.3439e-04,
        2.2939e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999992847442627
 
[Iteration 3] Process ID: 131596 [Epoch: 13,   256/ 591 points] total loss per batch: 1.298
Policy (actual, predicted): 0 0
Policy data: tensor([0.2442, 0.1105, 0.2058, 0.0569, 0.1584, 0.0918, 0.1325],
       device='cuda:0')
Policy pred: tensor([0.2539, 0.1145, 0.2016, 0.0588, 0.1537, 0.0798, 0.1377],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999992251396179
 
[Iteration 3] Process ID: 131596 [Epoch: 13,   384/ 591 points] total loss per batch: 1.287
Policy (actual, predicted): 1 1
Policy data: tensor([0.0934, 0.6501, 0.1337, 0.0188, 0.0173, 0.0412, 0.0455],
       device='cuda:0')
Policy pred: tensor([0.1238, 0.4145, 0.1454, 0.0723, 0.0819, 0.0814, 0.0806],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.012005561031401157
 
[Iteration 3] Process ID: 131596 [Epoch: 13,   512/ 591 points] total loss per batch: 1.257
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 1.7811e-07, 3.3510e-18, 3.6495e-20, 7.6289e-24, 7.6289e-24,
        3.6495e-20], device='cuda:0')
Policy pred: tensor([9.8905e-01, 5.6340e-03, 5.5553e-04, 6.3801e-04, 7.5680e-04, 9.2104e-04,
        2.4439e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9996342658996582
 
[Iteration 3] Process ID: 131596 [Epoch: 14,   128/ 591 points] total loss per batch: 1.282
Policy (actual, predicted): 4 4
Policy data: tensor([0.1442, 0.1383, 0.1442, 0.1325, 0.1559, 0.1372, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1328, 0.1249, 0.1451, 0.1445, 0.1600, 0.1375, 0.1551],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999290704727173
 
[Iteration 3] Process ID: 131596 [Epoch: 14,   256/ 591 points] total loss per batch: 1.256
Policy (actual, predicted): 6 6
Policy data: tensor([0.1145, 0.0262, 0.0247, 0.0217, 0.2126, 0.0364, 0.5639],
       device='cuda:0')
Policy pred: tensor([0.1572, 0.0560, 0.0307, 0.0251, 0.2633, 0.0463, 0.4216],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999403357505798
 
[Iteration 3] Process ID: 131596 [Epoch: 14,   384/ 591 points] total loss per batch: 1.266
Policy (actual, predicted): 4 4
Policy data: tensor([0.1533, 0.1533, 0.0975, 0.0550, 0.2571, 0.0668, 0.2170],
       device='cuda:0')
Policy pred: tensor([0.1123, 0.1373, 0.0946, 0.0586, 0.2831, 0.0612, 0.2529],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999978542327881
 
[Iteration 3] Process ID: 131596 [Epoch: 14,   512/ 591 points] total loss per batch: 1.346
Policy (actual, predicted): 2 2
Policy data: tensor([0.1415, 0.0489, 0.5765, 0.0170, 0.0419, 0.0760, 0.0983],
       device='cuda:0')
Policy pred: tensor([0.1193, 0.0450, 0.5940, 0.0183, 0.0370, 0.0782, 0.1082],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999333620071411
 
[Iteration 3] Process ID: 131596 [Epoch: 15,   128/ 591 points] total loss per batch: 1.329
Policy (actual, predicted): 4 4
Policy data: tensor([0.2080, 0.2265, 0.1344, 0.0433, 0.2464, 0.0775, 0.0640],
       device='cuda:0')
Policy pred: tensor([0.2092, 0.2043, 0.1315, 0.0425, 0.2643, 0.0813, 0.0670],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999998807907104
 
[Iteration 3] Process ID: 131596 [Epoch: 15,   256/ 591 points] total loss per batch: 1.091
Policy (actual, predicted): 2 2
Policy data: tensor([0.0146, 0.0715, 0.7924, 0.0112, 0.0162, 0.0730, 0.0211],
       device='cuda:0')
Policy pred: tensor([0.0136, 0.0474, 0.8630, 0.0065, 0.0126, 0.0440, 0.0128],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9978041648864746
 
[Iteration 3] Process ID: 131596 [Epoch: 15,   384/ 591 points] total loss per batch: 1.368
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.3345, 0.2337, 0.0341, 0.2172, 0.0621, 0.1185],
       device='cuda:0')
Policy pred: tensor([4.0490e-05, 3.4058e-01, 2.3598e-01, 3.6916e-02, 2.0668e-01, 5.9649e-02,
        1.2015e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 15,   512/ 591 points] total loss per batch: 1.264
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 4.6359e-12, 0.0000e+00, 1.6805e-17, 0.0000e+00, 3.7417e-17,
        6.7988e-07], device='cuda:0')
Policy pred: tensor([9.9877e-01, 7.0112e-04, 1.4491e-06, 1.5211e-04, 1.8098e-07, 1.7768e-05,
        3.5644e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 16,   128/ 591 points] total loss per batch: 1.358
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0078, 0.0144, 0.0202, 0.0074, 0.0315, 0.0178, 0.9008],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.33368176221847534
 
[Iteration 3] Process ID: 131596 [Epoch: 16,   256/ 591 points] total loss per batch: 1.155
Policy (actual, predicted): 2 2
Policy data: tensor([0.0774, 0.0375, 0.7471, 0.0285, 0.0330, 0.0375, 0.0390],
       device='cuda:0')
Policy pred: tensor([0.1013, 0.0341, 0.7308, 0.0257, 0.0278, 0.0447, 0.0357],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9947759509086609
 
[Iteration 3] Process ID: 131596 [Epoch: 16,   384/ 591 points] total loss per batch: 1.267
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.1822, 0.4010, 0.0431, 0.2273, 0.0556, 0.0909],
       device='cuda:0')
Policy pred: tensor([3.4212e-05, 1.7330e-01, 4.1301e-01, 4.9770e-02, 2.1461e-01, 5.5341e-02,
        9.3938e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 16,   512/ 591 points] total loss per batch: 1.350
Policy (actual, predicted): 2 2
Policy data: tensor([0.1917, 0.1435, 0.5590, 0.0247, 0.0232, 0.0407, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.2005, 0.1515, 0.5426, 0.0269, 0.0222, 0.0383, 0.0179],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9986376762390137
 
[Iteration 3] Process ID: 131596 [Epoch: 17,   128/ 591 points] total loss per batch: 1.202
Policy (actual, predicted): 1 1
Policy data: tensor([4.2423e-17, 1.0000e+00, 2.3890e-18, 2.3656e-16, 1.1161e-17, 3.8584e-19,
        1.0008e-18], device='cuda:0')
Policy pred: tensor([2.5649e-05, 9.9821e-01, 6.6719e-04, 3.9603e-04, 2.7147e-04, 3.5015e-04,
        8.0913e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999369978904724
 
[Iteration 3] Process ID: 131596 [Epoch: 17,   256/ 591 points] total loss per batch: 1.180
Policy (actual, predicted): 6 6
Policy data: tensor([0.2673, 0.0000, 0.1837, 0.0324, 0.0000, 0.1958, 0.3208],
       device='cuda:0')
Policy pred: tensor([2.4005e-01, 2.3981e-05, 1.7434e-01, 3.6522e-02, 3.7173e-06, 1.7842e-01,
        3.7064e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 17,   384/ 591 points] total loss per batch: 1.317
Policy (actual, predicted): 6 6
Policy data: tensor([0.3659, 0.0000, 0.0484, 0.0321, 0.0000, 0.0000, 0.5535],
       device='cuda:0')
Policy pred: tensor([4.4010e-01, 4.1850e-05, 4.6933e-02, 3.5992e-02, 8.5734e-07, 2.3972e-04,
        4.7669e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 17,   512/ 591 points] total loss per batch: 1.367
Policy (actual, predicted): 4 4
Policy data: tensor([0.1017, 0.1906, 0.0697, 0.0768, 0.2265, 0.1600, 0.1747],
       device='cuda:0')
Policy pred: tensor([0.0958, 0.1890, 0.0679, 0.0868, 0.2335, 0.1586, 0.1684],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999595880508423
 
[Iteration 3] Process ID: 131596 [Epoch: 18,   128/ 591 points] total loss per batch: 1.237
Policy (actual, predicted): 1 1
Policy data: tensor([8.3551e-02, 5.2405e-01, 3.2238e-04, 4.7825e-02, 1.4135e-02, 6.8652e-07,
        3.3012e-01], device='cuda:0')
Policy pred: tensor([0.0752, 0.5117, 0.0028, 0.0379, 0.0123, 0.0006, 0.3596],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999960720539093
 
[Iteration 3] Process ID: 131596 [Epoch: 18,   256/ 591 points] total loss per batch: 1.233
Policy (actual, predicted): 2 2
Policy data: tensor([3.2442e-01, 2.0857e-02, 5.1191e-01, 1.1861e-05, 1.4092e-01, 3.1682e-04,
        1.5668e-03], device='cuda:0')
Policy pred: tensor([0.3260, 0.0272, 0.5480, 0.0016, 0.0928, 0.0014, 0.0030],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999352097511292
 
[Iteration 3] Process ID: 131596 [Epoch: 18,   384/ 591 points] total loss per batch: 1.305
Policy (actual, predicted): 0 0
Policy data: tensor([0.6244, 0.1329, 0.1033, 0.0218, 0.0187, 0.0466, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.5864, 0.1499, 0.1167, 0.0230, 0.0198, 0.0545, 0.0497],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.007929829880595207
 
[Iteration 3] Process ID: 131596 [Epoch: 18,   512/ 591 points] total loss per batch: 1.387
Policy (actual, predicted): 2 2
Policy data: tensor([1.9169e-19, 2.4568e-10, 1.0000e+00, 5.5242e-12, 6.3201e-07, 6.6271e-21,
        1.8024e-14], device='cuda:0')
Policy pred: tensor([1.5410e-04, 7.2455e-04, 9.9811e-01, 9.4056e-04, 1.0018e-05, 1.5850e-05,
        4.3428e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 19,   128/ 591 points] total loss per batch: 1.255
Policy (actual, predicted): 0 2
Policy data: tensor([0.4737, 0.0441, 0.4088, 0.0092, 0.0125, 0.0250, 0.0265],
       device='cuda:0')
Policy pred: tensor([0.3815, 0.0449, 0.5050, 0.0084, 0.0121, 0.0249, 0.0231],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9996964335441589
 
[Iteration 3] Process ID: 131596 [Epoch: 19,   256/ 591 points] total loss per batch: 1.270
Policy (actual, predicted): 4 6
Policy data: tensor([0.1477, 0.1477, 0.1442, 0.1313, 0.1535, 0.1336, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1480, 0.1453, 0.1541, 0.1227, 0.1470, 0.1031, 0.1799],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9995856285095215
 
[Iteration 3] Process ID: 131596 [Epoch: 19,   384/ 591 points] total loss per batch: 1.253
Policy (actual, predicted): 2 2
Policy data: tensor([1.1003e-06, 2.9133e-06, 9.9991e-01, 3.8581e-19, 7.9434e-05, 1.0247e-15,
        1.9120e-06], device='cuda:0')
Policy pred: tensor([4.9491e-05, 4.1187e-04, 9.9775e-01, 7.3412e-05, 1.3157e-03, 1.0202e-04,
        2.9368e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999992251396179
 
[Iteration 3] Process ID: 131596 [Epoch: 19,   512/ 591 points] total loss per batch: 1.261
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 4.8059e-10, 1.6616e-09, 2.7480e-17, 1.0000e+00, 5.0091e-09,
        4.7592e-06], device='cuda:0')
Policy pred: tensor([1.0146e-07, 1.5764e-03, 4.0095e-05, 1.4326e-05, 9.9801e-01, 5.3167e-06,
        3.5129e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 20,   128/ 591 points] total loss per batch: 1.224
Policy (actual, predicted): 4 4
Policy data: tensor([1.4790e-15, 5.9791e-20, 0.0000e+00, 1.6494e-14, 1.0000e+00, 1.9882e-16,
        4.5641e-14], device='cuda:0')
Policy pred: tensor([8.1793e-05, 2.6083e-05, 1.1213e-06, 2.5169e-06, 9.9987e-01, 1.0365e-05,
        1.1591e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 20,   256/ 591 points] total loss per batch: 1.293
Policy (actual, predicted): 4 4
Policy data: tensor([0.1659, 0.0505, 0.1521, 0.0340, 0.3792, 0.0376, 0.1807],
       device='cuda:0')
Policy pred: tensor([0.1478, 0.0476, 0.1559, 0.0394, 0.4180, 0.0349, 0.1563],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 20,   384/ 591 points] total loss per batch: 1.117
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 0.0000e+00, 4.7631e-08, 0.0000e+00, 2.2985e-07,
        4.4360e-07], device='cuda:0')
Policy pred: tensor([3.7550e-08, 9.9978e-01, 3.4110e-07, 6.6964e-07, 6.2177e-11, 2.7533e-06,
        2.1304e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 20,   512/ 591 points] total loss per batch: 1.300
Policy (actual, predicted): 2 6
Policy data: tensor([0.1465, 0.1360, 0.1489, 0.1348, 0.1477, 0.1372, 0.1489],
       device='cuda:0')
Policy pred: tensor([0.1458, 0.1375, 0.1487, 0.1388, 0.1381, 0.1414, 0.1497],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999725222587585
 
[Iteration 3] Process ID: 131596 [Epoch: 21,   128/ 591 points] total loss per batch: 1.305
Policy (actual, predicted): 0 0
Policy data: tensor([0.4774, 0.0218, 0.3718, 0.0156, 0.0743, 0.0188, 0.0203],
       device='cuda:0')
Policy pred: tensor([0.4234, 0.0412, 0.3735, 0.0201, 0.0894, 0.0234, 0.0290],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999588131904602
 
[Iteration 3] Process ID: 131596 [Epoch: 21,   256/ 591 points] total loss per batch: 1.338
Policy (actual, predicted): 4 4
Policy data: tensor([0.1998, 0.1998, 0.1681, 0.0306, 0.2573, 0.0459, 0.0984],
       device='cuda:0')
Policy pred: tensor([0.2104, 0.1723, 0.1662, 0.0288, 0.2720, 0.0403, 0.1100],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998807907104
 
[Iteration 3] Process ID: 131596 [Epoch: 21,   384/ 591 points] total loss per batch: 1.152
Policy (actual, predicted): 0 0
Policy data: tensor([9.7685e-01, 0.0000e+00, 2.5326e-03, 5.5271e-05, 5.5271e-05, 1.6543e-05,
        2.0495e-02], device='cuda:0')
Policy pred: tensor([9.6507e-01, 1.6765e-06, 1.6432e-03, 1.5540e-03, 2.7056e-04, 1.2918e-03,
        3.0170e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999991059303284
 
[Iteration 3] Process ID: 131596 [Epoch: 21,   512/ 591 points] total loss per batch: 1.318
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0062, 0.0124, 0.0163, 0.0060, 0.0199, 0.0207, 0.9184],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.2936648726463318
 
[Iteration 3] Process ID: 131596 [Epoch: 22,   128/ 591 points] total loss per batch: 1.192
Policy (actual, predicted): 1 1
Policy data: tensor([0.0934, 0.6501, 0.1337, 0.0188, 0.0173, 0.0412, 0.0455],
       device='cuda:0')
Policy pred: tensor([0.1071, 0.4235, 0.1367, 0.0718, 0.0812, 0.0851, 0.0946],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.0031707105226814747
 
[Iteration 3] Process ID: 131596 [Epoch: 22,   256/ 591 points] total loss per batch: 1.198
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 0.0000e+00, 4.4611e-19, 8.9935e-17, 3.6315e-13,
        6.2976e-15], device='cuda:0')
Policy pred: tensor([1.5009e-05, 9.9525e-01, 2.1312e-05, 1.5010e-03, 3.1031e-03, 1.3478e-05,
        9.8886e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 22,   384/ 591 points] total loss per batch: 1.309
Policy (actual, predicted): 0 0
Policy data: tensor([0.1501, 0.1407, 0.1477, 0.1325, 0.1501, 0.1348, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1946, 0.1599, 0.1531, 0.0838, 0.1427, 0.1213, 0.1446],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 22,   512/ 591 points] total loss per batch: 1.361
Policy (actual, predicted): 1 1
Policy data: tensor([0.0934, 0.6501, 0.1337, 0.0188, 0.0173, 0.0412, 0.0455],
       device='cuda:0')
Policy pred: tensor([0.1215, 0.4301, 0.1278, 0.0678, 0.0781, 0.0833, 0.0913],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.014013591222465038
 
[Iteration 3] Process ID: 131596 [Epoch: 23,   128/ 591 points] total loss per batch: 1.123
Policy (actual, predicted): 2 2
Policy data: tensor([0.0217, 0.1514, 0.4403, 0.0057, 0.3482, 0.0156, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.0220, 0.1704, 0.5506, 0.0048, 0.2270, 0.0102, 0.0149],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.5066314935684204
 
[Iteration 3] Process ID: 131596 [Epoch: 23,   256/ 591 points] total loss per batch: 1.336
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1222, 0.3966, 0.1421, 0.0740, 0.0872, 0.0937, 0.0842],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.0031178046483546495
 
[Iteration 3] Process ID: 131596 [Epoch: 23,   384/ 591 points] total loss per batch: 1.292
Policy (actual, predicted): 0 4
Policy data: tensor([0.1559, 0.1512, 0.1466, 0.1289, 0.1454, 0.1325, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1528, 0.1426, 0.1279, 0.1213, 0.1705, 0.1367, 0.1482],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999992847442627
 
[Iteration 3] Process ID: 131596 [Epoch: 23,   512/ 591 points] total loss per batch: 1.257
Policy (actual, predicted): 2 4
Policy data: tensor([6.7748e-02, 2.6026e-01, 2.8020e-01, 6.6160e-05, 2.8020e-01, 2.7363e-04,
        1.1125e-01], device='cuda:0')
Policy pred: tensor([0.0636, 0.2567, 0.2503, 0.0038, 0.3147, 0.0028, 0.1081],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999992251396179
 
[Iteration 3] Process ID: 131596 [Epoch: 24,   128/ 591 points] total loss per batch: 1.261
Policy (actual, predicted): 6 0
Policy data: tensor([0.1407, 0.1442, 0.1442, 0.1407, 0.1465, 0.1360, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.6569, 0.0567, 0.0543, 0.0433, 0.0733, 0.0502, 0.0651],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.07302220165729523
 
[Iteration 3] Process ID: 131596 [Epoch: 24,   256/ 591 points] total loss per batch: 1.279
Policy (actual, predicted): 3 4
Policy data: tensor([0.1372, 0.1454, 0.1419, 0.1477, 0.1442, 0.1372, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1295, 0.1273, 0.1296, 0.1566, 0.1828, 0.1231, 0.1511],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9962397217750549
 
[Iteration 3] Process ID: 131596 [Epoch: 24,   384/ 591 points] total loss per batch: 1.224
Policy (actual, predicted): 0 6
Policy data: tensor([0.1501, 0.1430, 0.1454, 0.1372, 0.1501, 0.1313, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1293, 0.1972, 0.0924, 0.0680, 0.0698, 0.1041, 0.3392],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.02883211337029934
 
[Iteration 3] Process ID: 131596 [Epoch: 24,   512/ 591 points] total loss per batch: 1.340
Policy (actual, predicted): 5 5
Policy data: tensor([0.0890, 0.1051, 0.1601, 0.1001, 0.1112, 0.3270, 0.1075],
       device='cuda:0')
Policy pred: tensor([0.0890, 0.0854, 0.1648, 0.1013, 0.1150, 0.3341, 0.1104],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999996423721313
 
[Iteration 3] Process ID: 131596 [Epoch: 25,   128/ 591 points] total loss per batch: 1.296
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9906e-01, 0.0000e+00, 9.3862e-04,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([9.3076e-06, 1.2612e-05, 7.4473e-06, 9.9955e-01, 1.7205e-06, 3.3828e-04,
        7.8022e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 25,   256/ 591 points] total loss per batch: 1.267
Policy (actual, predicted): 4 1
Policy data: tensor([0.1512, 0.1466, 0.1501, 0.1277, 0.1536, 0.1301, 0.1407],
       device='cuda:0')
Policy pred: tensor([0.1193, 0.4019, 0.1444, 0.0710, 0.0760, 0.0883, 0.0991],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.0029467556159943342
 
[Iteration 3] Process ID: 131596 [Epoch: 25,   384/ 591 points] total loss per batch: 1.337
Policy (actual, predicted): 4 5
Policy data: tensor([0.1372, 0.1465, 0.1442, 0.1313, 0.1535, 0.1419, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1230, 0.1287, 0.1377, 0.1547, 0.1537, 0.1620, 0.1403],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 25,   512/ 591 points] total loss per batch: 1.207
Policy (actual, predicted): 4 4
Policy data: tensor([0.1533, 0.1533, 0.0975, 0.0550, 0.2571, 0.0668, 0.2170],
       device='cuda:0')
Policy pred: tensor([0.1651, 0.1587, 0.1043, 0.0593, 0.2456, 0.0750, 0.1920],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999775886535645
 
[Iteration 3] Process ID: 131596 [Epoch: 26,   128/ 591 points] total loss per batch: 1.148
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000e+00, 0.0000e+00, 9.9718e-01, 1.1090e-08, 5.5423e-13, 2.8185e-03,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([5.5273e-06, 7.5901e-05, 9.9529e-01, 1.0275e-04, 2.2465e-05, 4.4791e-03,
        2.6541e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999983906745911
 
[Iteration 3] Process ID: 131596 [Epoch: 26,   256/ 591 points] total loss per batch: 1.315
Policy (actual, predicted): 6 6
Policy data: tensor([0.0935, 0.1762, 0.1476, 0.1350, 0.1613, 0.0579, 0.2285],
       device='cuda:0')
Policy pred: tensor([0.0897, 0.1505, 0.1529, 0.1476, 0.1649, 0.0653, 0.2291],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999931454658508
 
[Iteration 3] Process ID: 131596 [Epoch: 26,   384/ 591 points] total loss per batch: 1.400
Policy (actual, predicted): 4 4
Policy data: tensor([0.1279, 0.1400, 0.1532, 0.1532, 0.2175, 0.0803, 0.1279],
       device='cuda:0')
Policy pred: tensor([0.1456, 0.1265, 0.1481, 0.1389, 0.2263, 0.0854, 0.1292],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999994039535522
 
[Iteration 3] Process ID: 131596 [Epoch: 26,   512/ 591 points] total loss per batch: 1.201
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000, 0.1705, 0.2630, 0.0242, 0.4474, 0.0948, 0.0000],
       device='cuda:0')
Policy pred: tensor([3.0884e-04, 1.7985e-01, 2.8996e-01, 3.3505e-02, 3.6369e-01, 1.3247e-01,
        2.0533e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999995231628418
 
[Iteration 3] Process ID: 131596 [Epoch: 27,   128/ 591 points] total loss per batch: 1.289
Policy (actual, predicted): 0 0
Policy data: tensor([0.8949, 0.0116, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.6947, 0.0440, 0.0480, 0.0401, 0.0684, 0.0463, 0.0584],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.06507796049118042
 
[Iteration 3] Process ID: 131596 [Epoch: 27,   256/ 591 points] total loss per batch: 1.217
Policy (actual, predicted): 0 0
Policy data: tensor([0.6244, 0.1329, 0.1033, 0.0218, 0.0187, 0.0466, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.6448, 0.1252, 0.0931, 0.0211, 0.0171, 0.0490, 0.0498],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.00018326938152313232
 
[Iteration 3] Process ID: 131596 [Epoch: 27,   384/ 591 points] total loss per batch: 1.351
Policy (actual, predicted): 6 6
Policy data: tensor([0.0926, 0.1207, 0.1029, 0.0514, 0.0332, 0.0374, 0.5617],
       device='cuda:0')
Policy pred: tensor([0.0686, 0.1030, 0.1205, 0.0361, 0.0371, 0.0310, 0.6038],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999613165855408
 
[Iteration 3] Process ID: 131596 [Epoch: 27,   512/ 591 points] total loss per batch: 1.257
Policy (actual, predicted): 6 6
Policy data: tensor([0.1991, 0.0808, 0.1826, 0.0451, 0.1673, 0.0888, 0.2362],
       device='cuda:0')
Policy pred: tensor([0.2184, 0.0789, 0.1716, 0.0676, 0.1374, 0.0812, 0.2449],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 28,   128/ 591 points] total loss per batch: 1.279
Policy (actual, predicted): 4 6
Policy data: tensor([0.1477, 0.1430, 0.1407, 0.1336, 0.1594, 0.1336, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1470, 0.1510, 0.1316, 0.1391, 0.1316, 0.1410, 0.1587],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 28,   256/ 591 points] total loss per batch: 1.416
Policy (actual, predicted): 4 4
Policy data: tensor([0.1442, 0.1383, 0.1442, 0.1325, 0.1559, 0.1372, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1477, 0.1331, 0.1366, 0.1460, 0.1582, 0.1413, 0.1371],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9994479417800903
 
[Iteration 3] Process ID: 131596 [Epoch: 28,   384/ 591 points] total loss per batch: 1.142
Policy (actual, predicted): 1 1
Policy data: tensor([0.0330, 0.6655, 0.0208, 0.0094, 0.0192, 0.2410, 0.0111],
       device='cuda:0')
Policy pred: tensor([0.0478, 0.6283, 0.0277, 0.0113, 0.0261, 0.2401, 0.0187],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9992982149124146
 
[Iteration 3] Process ID: 131596 [Epoch: 28,   512/ 591 points] total loss per batch: 1.241
Policy (actual, predicted): 2 1
Policy data: tensor([0.1465, 0.1372, 0.1489, 0.1348, 0.1454, 0.1477, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1288, 0.1683, 0.1590, 0.1315, 0.1466, 0.1321, 0.1338],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999023079872131
 
[Iteration 3] Process ID: 131596 [Epoch: 29,   128/ 591 points] total loss per batch: 1.298
Policy (actual, predicted): 0 0
Policy data: tensor([9.9979e-01, 2.0900e-04, 0.0000e+00, 1.5909e-15, 3.7529e-17, 2.4134e-14,
        9.0839e-12], device='cuda:0')
Policy pred: tensor([9.9864e-01, 2.3569e-04, 7.6895e-06, 3.9714e-04, 2.1446e-04, 3.4836e-05,
        4.7115e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999963641166687
 
[Iteration 3] Process ID: 131596 [Epoch: 29,   256/ 591 points] total loss per batch: 1.278
Policy (actual, predicted): 6 6
Policy data: tensor([0.2005, 0.0000, 0.0790, 0.0339, 0.2685, 0.0857, 0.3324],
       device='cuda:0')
Policy pred: tensor([1.9700e-01, 1.2402e-04, 1.0192e-01, 4.2497e-02, 2.5518e-01, 8.1776e-02,
        3.2150e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998807907104
 
[Iteration 3] Process ID: 131596 [Epoch: 29,   384/ 591 points] total loss per batch: 1.256
Policy (actual, predicted): 6 1
Policy data: tensor([0.1407, 0.1442, 0.1419, 0.1336, 0.1407, 0.1384, 0.1605],
       device='cuda:0')
Policy pred: tensor([0.1422, 0.1609, 0.1569, 0.1275, 0.1122, 0.1419, 0.1585],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999987483024597
 
[Iteration 3] Process ID: 131596 [Epoch: 29,   512/ 591 points] total loss per batch: 1.233
Policy (actual, predicted): 0 0
Policy data: tensor([0.2785, 0.1397, 0.1668, 0.0548, 0.1527, 0.0548, 0.1527],
       device='cuda:0')
Policy pred: tensor([0.2630, 0.1361, 0.1569, 0.0592, 0.1702, 0.0592, 0.1553],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 30,   128/ 591 points] total loss per batch: 1.329
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2306, 0.1787, 0.2956, 0.0646, 0.2306, 0.0000],
       device='cuda:0')
Policy pred: tensor([9.7187e-05, 2.6677e-01, 1.8284e-01, 2.8429e-01, 7.7676e-02, 1.8749e-01,
        8.3095e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999451041221619
 
[Iteration 3] Process ID: 131596 [Epoch: 30,   256/ 591 points] total loss per batch: 1.259
Policy (actual, predicted): 1 1
Policy data: tensor([0.0609, 0.3484, 0.1650, 0.0609, 0.0609, 0.0306, 0.2732],
       device='cuda:0')
Policy pred: tensor([0.0683, 0.3329, 0.1676, 0.0656, 0.0799, 0.0294, 0.2563],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 30,   384/ 591 points] total loss per batch: 1.310
Policy (actual, predicted): 1 1
Policy data: tensor([5.4135e-02, 8.1486e-01, 7.9576e-04, 2.0640e-03, 1.8411e-03, 1.1031e-01,
        1.6000e-02], device='cuda:0')
Policy pred: tensor([0.0633, 0.8289, 0.0026, 0.0023, 0.0024, 0.0889, 0.0116],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999982714653015
 
[Iteration 3] Process ID: 131596 [Epoch: 30,   512/ 591 points] total loss per batch: 1.238
Policy (actual, predicted): 6 1
Policy data: tensor([0.1407, 0.1477, 0.1465, 0.1383, 0.1430, 0.1336, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1477, 0.1611, 0.1422, 0.1434, 0.1363, 0.1199, 0.1493],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.957577645778656
 
[Iteration 3] Process ID: 131596 [Epoch: 31,   128/ 591 points] total loss per batch: 1.269
Policy (actual, predicted): 4 0
Policy data: tensor([0.1348, 0.1442, 0.1430, 0.1442, 0.1489, 0.1395, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.7376, 0.0377, 0.0412, 0.0341, 0.0571, 0.0391, 0.0532],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.05040369927883148
 
[Iteration 3] Process ID: 131596 [Epoch: 31,   256/ 591 points] total loss per batch: 1.424
Policy (actual, predicted): 6 4
Policy data: tensor([0.1430, 0.1442, 0.1348, 0.1454, 0.1407, 0.1454, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1410, 0.1332, 0.1360, 0.1241, 0.1695, 0.1541, 0.1420],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999918341636658
 
[Iteration 3] Process ID: 131596 [Epoch: 31,   384/ 591 points] total loss per batch: 1.252
Policy (actual, predicted): 1 1
Policy data: tensor([0.0943, 0.2116, 0.1628, 0.1940, 0.0710, 0.1035, 0.1628],
       device='cuda:0')
Policy pred: tensor([0.0905, 0.2543, 0.1676, 0.1635, 0.0726, 0.1011, 0.1504],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999972581863403
 
[Iteration 3] Process ID: 131596 [Epoch: 31,   512/ 591 points] total loss per batch: 1.176
Policy (actual, predicted): 6 6
Policy data: tensor([0.1419, 0.1442, 0.1372, 0.1465, 0.1372, 0.1395, 0.1535],
       device='cuda:0')
Policy pred: tensor([0.1013, 0.1179, 0.1119, 0.1093, 0.1100, 0.1108, 0.3388],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.20025277137756348
 
[Iteration 3] Process ID: 131596 [Epoch: 32,   128/ 591 points] total loss per batch: 1.229
Policy (actual, predicted): 0 0
Policy data: tensor([0.4737, 0.0441, 0.4088, 0.0092, 0.0125, 0.0250, 0.0265],
       device='cuda:0')
Policy pred: tensor([0.5507, 0.0386, 0.3448, 0.0087, 0.0126, 0.0185, 0.0261],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998628497123718
 
[Iteration 3] Process ID: 131596 [Epoch: 32,   256/ 591 points] total loss per batch: 1.172
Policy (actual, predicted): 6 6
Policy data: tensor([0.1583, 0.1887, 0.1448, 0.1210, 0.1210, 0.0422, 0.2242],
       device='cuda:0')
Policy pred: tensor([0.1761, 0.1740, 0.1670, 0.1051, 0.1246, 0.0476, 0.2055],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998807907104
 
[Iteration 3] Process ID: 131596 [Epoch: 32,   384/ 591 points] total loss per batch: 1.399
Policy (actual, predicted): 1 0
Policy data: tensor([0.1372, 0.1477, 0.1454, 0.1419, 0.1442, 0.1360, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.8197, 0.0288, 0.0288, 0.0228, 0.0412, 0.0260, 0.0326],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.06534065306186676
 
[Iteration 3] Process ID: 131596 [Epoch: 32,   512/ 591 points] total loss per batch: 1.328
Policy (actual, predicted): 1 1
Policy data: tensor([8.6393e-18, 1.0000e+00, 8.6393e-18, 1.7223e-17, 2.9168e-22, 3.2070e-20,
        3.2070e-20], device='cuda:0')
Policy pred: tensor([2.8989e-04, 9.9870e-01, 5.2253e-04, 3.4014e-04, 7.7286e-05, 5.0896e-05,
        1.4753e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999343752861023
 
[Iteration 3] Process ID: 131596 [Epoch: 33,   128/ 591 points] total loss per batch: 1.075
Policy (actual, predicted): 4 4
Policy data: tensor([0.0734, 0.2334, 0.0410, 0.0551, 0.3249, 0.1063, 0.1658],
       device='cuda:0')
Policy pred: tensor([0.0805, 0.2072, 0.0427, 0.0496, 0.3302, 0.1095, 0.1804],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999955296516418
 
[Iteration 3] Process ID: 131596 [Epoch: 33,   256/ 591 points] total loss per batch: 1.204
Policy (actual, predicted): 0 0
Policy data: tensor([7.1246e-01, 3.4085e-05, 1.2956e-04, 1.2066e-13, 2.8738e-01, 5.8889e-11,
        2.5576e-12], device='cuda:0')
Policy pred: tensor([0.7072, 0.0055, 0.0037, 0.0011, 0.2706, 0.0010, 0.0109],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 33,   384/ 591 points] total loss per batch: 1.354
Policy (actual, predicted): 2 2
Policy data: tensor([0.1829, 0.1103, 0.5624, 0.0123, 0.0123, 0.0421, 0.0776],
       device='cuda:0')
Policy pred: tensor([0.2238, 0.1500, 0.4693, 0.0130, 0.0186, 0.0458, 0.0795],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9761348962783813
 
[Iteration 3] Process ID: 131596 [Epoch: 33,   512/ 591 points] total loss per batch: 1.372
Policy (actual, predicted): 0 0
Policy data: tensor([0.4255, 0.0000, 0.2600, 0.0674, 0.0000, 0.0000, 0.2471],
       device='cuda:0')
Policy pred: tensor([4.0583e-01, 4.9324e-05, 2.6380e-01, 8.2164e-02, 1.2183e-05, 8.8476e-04,
        2.4726e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 34,   128/ 591 points] total loss per batch: 1.284
Policy (actual, predicted): 4 4
Policy data: tensor([0.1904, 0.0932, 0.1600, 0.0432, 0.3153, 0.0639, 0.1341],
       device='cuda:0')
Policy pred: tensor([0.1642, 0.1086, 0.1522, 0.0443, 0.3279, 0.0719, 0.1309],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 34,   256/ 591 points] total loss per batch: 1.399
Policy (actual, predicted): 4 0
Policy data: tensor([0.1501, 0.1430, 0.1442, 0.1313, 0.1559, 0.1360, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1585, 0.1413, 0.1296, 0.1360, 0.1454, 0.1489, 0.1402],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9665288329124451
 
[Iteration 3] Process ID: 131596 [Epoch: 34,   384/ 591 points] total loss per batch: 1.168
Policy (actual, predicted): 1 1
Policy data: tensor([0.2195, 0.2730, 0.0182, 0.1893, 0.0000, 0.1107, 0.1893],
       device='cuda:0')
Policy pred: tensor([2.2675e-01, 2.4754e-01, 2.0108e-02, 1.9194e-01, 1.3291e-05, 1.0496e-01,
        2.0868e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999997019767761
 
[Iteration 3] Process ID: 131596 [Epoch: 34,   512/ 591 points] total loss per batch: 1.202
Policy (actual, predicted): 2 2
Policy data: tensor([0.1917, 0.1435, 0.5590, 0.0247, 0.0232, 0.0407, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.1897, 0.1634, 0.5431, 0.0215, 0.0245, 0.0383, 0.0195],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9936295747756958
 
[Iteration 3] Process ID: 131596 [Epoch: 35,   128/ 591 points] total loss per batch: 1.324
Policy (actual, predicted): 4 4
Policy data: tensor([0.0476, 0.2453, 0.0525, 0.1117, 0.2667, 0.1020, 0.1743],
       device='cuda:0')
Policy pred: tensor([0.0502, 0.2200, 0.0538, 0.1146, 0.2813, 0.1139, 0.1662],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 35,   256/ 591 points] total loss per batch: 1.249
Policy (actual, predicted): 6 6
Policy data: tensor([0.1145, 0.0262, 0.0247, 0.0217, 0.2126, 0.0364, 0.5639],
       device='cuda:0')
Policy pred: tensor([0.0922, 0.0442, 0.0216, 0.0274, 0.2681, 0.0388, 0.5078],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999888002872467
 
[Iteration 3] Process ID: 131596 [Epoch: 35,   384/ 591 points] total loss per batch: 1.207
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 1.7811e-07, 3.3510e-18, 3.6495e-20, 7.6289e-24, 7.6289e-24,
        3.6495e-20], device='cuda:0')
Policy pred: tensor([9.9338e-01, 3.7413e-03, 2.5163e-04, 2.5639e-04, 1.7775e-04, 5.7017e-04,
        1.6273e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998679161071777
 
[Iteration 3] Process ID: 131596 [Epoch: 35,   512/ 591 points] total loss per batch: 1.284
Policy (actual, predicted): 6 6
Policy data: tensor([0.1561, 0.1427, 0.1304, 0.1427, 0.1427, 0.0989, 0.1864],
       device='cuda:0')
Policy pred: tensor([0.1580, 0.1235, 0.1138, 0.1394, 0.1563, 0.1204, 0.1885],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999976754188538
 
[Iteration 3] Process ID: 131596 [Epoch: 36,   128/ 591 points] total loss per batch: 1.250
Policy (actual, predicted): 2 2
Policy data: tensor([1.1003e-06, 2.9133e-06, 9.9991e-01, 3.8581e-19, 7.9434e-05, 1.0247e-15,
        1.9120e-06], device='cuda:0')
Policy pred: tensor([1.0633e-05, 1.7667e-04, 9.9954e-01, 9.9057e-06, 1.6559e-04, 2.0359e-05,
        7.8730e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999995231628418
 
[Iteration 3] Process ID: 131596 [Epoch: 36,   256/ 591 points] total loss per batch: 1.309
Policy (actual, predicted): 2 2
Policy data: tensor([0.0217, 0.1514, 0.4403, 0.0057, 0.3482, 0.0156, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.0249, 0.1504, 0.4362, 0.0109, 0.3280, 0.0227, 0.0267],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.5774140357971191
 
[Iteration 3] Process ID: 131596 [Epoch: 36,   384/ 591 points] total loss per batch: 1.309
Policy (actual, predicted): 6 6
Policy data: tensor([0.1395, 0.1465, 0.1348, 0.1489, 0.1407, 0.1383, 0.1512],
       device='cuda:0')
Policy pred: tensor([0.1509, 0.1428, 0.1341, 0.1424, 0.1303, 0.1402, 0.1593],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9980253577232361
 
[Iteration 3] Process ID: 131596 [Epoch: 36,   512/ 591 points] total loss per batch: 1.160
Policy (actual, predicted): 2 2
Policy data: tensor([0.0215, 0.0263, 0.8668, 0.0114, 0.0311, 0.0182, 0.0247],
       device='cuda:0')
Policy pred: tensor([0.0124, 0.0168, 0.9195, 0.0043, 0.0215, 0.0120, 0.0135],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.2952336072921753
 
[Iteration 3] Process ID: 131596 [Epoch: 37,   128/ 591 points] total loss per batch: 1.377
Policy (actual, predicted): 3 6
Policy data: tensor([0.0955, 0.1501, 0.0539, 0.2739, 0.0955, 0.0792, 0.2519],
       device='cuda:0')
Policy pred: tensor([0.1062, 0.1329, 0.0569, 0.2549, 0.1005, 0.0764, 0.2723],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 37,   256/ 591 points] total loss per batch: 1.207
Policy (actual, predicted): 2 2
Policy data: tensor([2.7122e-01, 2.4647e-02, 4.8035e-01, 1.9650e-04, 1.8649e-01, 1.5233e-03,
        3.5576e-02], device='cuda:0')
Policy pred: tensor([0.2669, 0.0176, 0.5475, 0.0031, 0.1358, 0.0042, 0.0250],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999992251396179
 
[Iteration 3] Process ID: 131596 [Epoch: 37,   384/ 591 points] total loss per batch: 1.216
Policy (actual, predicted): 2 2
Policy data: tensor([0.1155, 0.0361, 0.7418, 0.0128, 0.0160, 0.0255, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.1036, 0.0348, 0.7522, 0.0119, 0.0191, 0.0304, 0.0480],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9923012852668762
 
[Iteration 3] Process ID: 131596 [Epoch: 37,   512/ 591 points] total loss per batch: 1.219
Policy (actual, predicted): 4 6
Policy data: tensor([0.1466, 0.1419, 0.1489, 0.1313, 0.1524, 0.1360, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1553, 0.1354, 0.1376, 0.1234, 0.1529, 0.1346, 0.1608],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9906888604164124
 
[Iteration 3] Process ID: 131596 [Epoch: 38,   128/ 591 points] total loss per batch: 1.339
Policy (actual, predicted): 0 1
Policy data: tensor([0.1512, 0.1512, 0.1384, 0.1277, 0.1501, 0.1336, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1635, 0.1679, 0.1350, 0.1317, 0.1499, 0.1342, 0.1179],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999582171440125
 
[Iteration 3] Process ID: 131596 [Epoch: 38,   256/ 591 points] total loss per batch: 1.250
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.1798, 0.0882, 0.2429, 0.2255, 0.1318, 0.1318],
       device='cuda:0')
Policy pred: tensor([1.8338e-04, 1.6826e-01, 8.7710e-02, 2.3633e-01, 1.9579e-01, 1.4996e-01,
        1.6177e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999995231628418
 
[Iteration 3] Process ID: 131596 [Epoch: 38,   384/ 591 points] total loss per batch: 1.280
Policy (actual, predicted): 6 2
Policy data: tensor([0.1579, 0.1354, 0.2204, 0.0586, 0.0990, 0.0599, 0.2688],
       device='cuda:0')
Policy pred: tensor([0.1711, 0.1665, 0.2410, 0.0499, 0.0946, 0.0615, 0.2153],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9843025207519531
 
[Iteration 3] Process ID: 131596 [Epoch: 38,   512/ 591 points] total loss per batch: 1.203
Policy (actual, predicted): 0 0
Policy data: tensor([7.4563e-01, 4.7480e-04, 2.5335e-01, 7.2741e-10, 6.9555e-05, 3.4502e-07,
        4.7480e-04], device='cuda:0')
Policy pred: tensor([0.7837, 0.0046, 0.2017, 0.0012, 0.0047, 0.0021, 0.0020],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999999463558197
 
[Iteration 3] Process ID: 131596 [Epoch: 39,   128/ 591 points] total loss per batch: 1.273
Policy (actual, predicted): 3 3
Policy data: tensor([0.1010, 0.0631, 0.0695, 0.3116, 0.2641, 0.0695, 0.1211],
       device='cuda:0')
Policy pred: tensor([0.1115, 0.0893, 0.0812, 0.2732, 0.2474, 0.0724, 0.1250],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 39,   256/ 591 points] total loss per batch: 1.316
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 2.3553e-09, 7.0268e-20, 2.6710e-19, 1.4689e-13,
        1.5992e-09], device='cuda:0')
Policy pred: tensor([5.7191e-08, 9.9870e-01, 7.9030e-04, 8.9500e-06, 2.0637e-04, 2.6061e-05,
        2.6403e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999960660934448
 
[Iteration 3] Process ID: 131596 [Epoch: 39,   384/ 591 points] total loss per batch: 1.324
Policy (actual, predicted): 6 6
Policy data: tensor([0.1454, 0.1372, 0.1360, 0.1360, 0.1489, 0.1430, 0.1535],
       device='cuda:0')
Policy pred: tensor([0.1399, 0.1286, 0.1564, 0.1236, 0.1540, 0.1250, 0.1726],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999992251396179
 
[Iteration 3] Process ID: 131596 [Epoch: 39,   512/ 591 points] total loss per batch: 1.190
Policy (actual, predicted): 4 2
Policy data: tensor([0.1501, 0.1454, 0.1501, 0.1242, 0.1524, 0.1337, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1460, 0.1409, 0.1617, 0.1185, 0.1516, 0.1241, 0.1573],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9986096620559692
 
[Iteration 3] Process ID: 131596 [Epoch: 40,   128/ 591 points] total loss per batch: 1.281
Policy (actual, predicted): 4 4
Policy data: tensor([0.1017, 0.1906, 0.0697, 0.0768, 0.2265, 0.1600, 0.1747],
       device='cuda:0')
Policy pred: tensor([0.1142, 0.2192, 0.0617, 0.0789, 0.2306, 0.1446, 0.1507],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9995477199554443
 
[Iteration 3] Process ID: 131596 [Epoch: 40,   256/ 591 points] total loss per batch: 1.176
Policy (actual, predicted): 6 6
Policy data: tensor([0.1524, 0.1431, 0.1477, 0.1313, 0.1442, 0.1266, 0.1547],
       device='cuda:0')
Policy pred: tensor([0.1509, 0.1357, 0.1516, 0.1325, 0.1440, 0.1246, 0.1609],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999695420265198
 
[Iteration 3] Process ID: 131596 [Epoch: 40,   384/ 591 points] total loss per batch: 1.378
Policy (actual, predicted): 0 0
Policy data: tensor([9.9660e-01, 0.0000e+00, 9.8604e-05, 1.8227e-05, 1.8227e-05, 3.9518e-05,
        3.2289e-03], device='cuda:0')
Policy pred: tensor([9.9511e-01, 1.3082e-05, 9.2471e-04, 1.1598e-03, 5.7179e-05, 9.8252e-04,
        1.7535e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999995827674866
 
[Iteration 3] Process ID: 131596 [Epoch: 40,   512/ 591 points] total loss per batch: 1.326
Policy (actual, predicted): 1 1
Policy data: tensor([0.0650, 0.3209, 0.0485, 0.1777, 0.1139, 0.1247, 0.1491],
       device='cuda:0')
Policy pred: tensor([0.0739, 0.3584, 0.0501, 0.1771, 0.1197, 0.0927, 0.1281],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999858736991882
 
[Iteration 3] Process ID: 131596 [Epoch: 41,   128/ 591 points] total loss per batch: 1.280
Policy (actual, predicted): 6 6
Policy data: tensor([0.1579, 0.1354, 0.2204, 0.0586, 0.0990, 0.0599, 0.2688],
       device='cuda:0')
Policy pred: tensor([0.1172, 0.1090, 0.2272, 0.0643, 0.1242, 0.0493, 0.3088],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9759008288383484
 
[Iteration 3] Process ID: 131596 [Epoch: 41,   256/ 591 points] total loss per batch: 1.388
Policy (actual, predicted): 6 6
Policy data: tensor([0.1419, 0.1477, 0.1360, 0.1430, 0.1430, 0.1383, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1455, 0.1568, 0.1182, 0.1340, 0.1332, 0.1454, 0.1669],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9979879260063171
 
[Iteration 3] Process ID: 131596 [Epoch: 41,   384/ 591 points] total loss per batch: 1.278
Policy (actual, predicted): 2 2
Policy data: tensor([0.2505, 0.0671, 0.2990, 0.2149, 0.0902, 0.0433, 0.0351],
       device='cuda:0')
Policy pred: tensor([0.1890, 0.0733, 0.3171, 0.2363, 0.0951, 0.0531, 0.0361],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9942561388015747
 
[Iteration 3] Process ID: 131596 [Epoch: 41,   512/ 591 points] total loss per batch: 1.238
Policy (actual, predicted): 3 3
Policy data: tensor([8.3669e-04, 4.3403e-02, 2.7490e-03, 9.5136e-01, 1.2638e-03, 2.9462e-06,
        3.8919e-04], device='cuda:0')
Policy pred: tensor([2.8766e-03, 5.0298e-02, 3.4300e-03, 9.3451e-01, 3.8411e-03, 3.2582e-04,
        4.7145e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999994039535522
 
[Iteration 3] Process ID: 131596 [Epoch: 42,   128/ 591 points] total loss per batch: 1.276
Policy (actual, predicted): 1 1
Policy data: tensor([0.1454, 0.1524, 0.1313, 0.1419, 0.1395, 0.1430, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1494, 0.1562, 0.1463, 0.1308, 0.1508, 0.1331, 0.1334],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999992251396179
 
[Iteration 3] Process ID: 131596 [Epoch: 42,   256/ 591 points] total loss per batch: 1.244
Policy (actual, predicted): 2 2
Policy data: tensor([0.1466, 0.1466, 0.1501, 0.1301, 0.1466, 0.1360, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1515, 0.1373, 0.1583, 0.1306, 0.1507, 0.1397, 0.1318],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999982714653015
 
[Iteration 3] Process ID: 131596 [Epoch: 42,   384/ 591 points] total loss per batch: 1.258
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 4.6359e-12, 0.0000e+00, 1.6805e-17, 0.0000e+00, 3.7417e-17,
        6.7988e-07], device='cuda:0')
Policy pred: tensor([9.9924e-01, 4.4475e-04, 1.1181e-06, 5.0958e-05, 1.6761e-07, 1.0971e-05,
        2.5059e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999998807907104
 
[Iteration 3] Process ID: 131596 [Epoch: 42,   512/ 591 points] total loss per batch: 1.297
Policy (actual, predicted): 4 4
Policy data: tensor([0.0165, 0.0231, 0.0114, 0.0079, 0.8601, 0.0627, 0.0182],
       device='cuda:0')
Policy pred: tensor([0.0241, 0.0290, 0.0196, 0.0121, 0.8187, 0.0658, 0.0307],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9846018552780151
 
[Iteration 3] Process ID: 131596 [Epoch: 43,   128/ 591 points] total loss per batch: 1.361
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.3345, 0.2337, 0.0341, 0.2172, 0.0621, 0.1185],
       device='cuda:0')
Policy pred: tensor([5.7652e-05, 3.4304e-01, 2.3456e-01, 3.8197e-02, 1.9424e-01, 6.1558e-02,
        1.2835e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998807907104
 
[Iteration 3] Process ID: 131596 [Epoch: 43,   256/ 591 points] total loss per batch: 1.284
Policy (actual, predicted): 6 6
Policy data: tensor([0.1932, 0.1356, 0.0778, 0.1239, 0.1621, 0.0778, 0.2296],
       device='cuda:0')
Policy pred: tensor([0.1943, 0.1278, 0.0731, 0.1149, 0.1570, 0.0725, 0.2605],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999540448188782
 
[Iteration 3] Process ID: 131596 [Epoch: 43,   384/ 591 points] total loss per batch: 1.206
Policy (actual, predicted): 0 0
Policy data: tensor([0.8765, 0.0149, 0.0079, 0.0097, 0.0199, 0.0526, 0.0183],
       device='cuda:0')
Policy pred: tensor([0.8540, 0.0165, 0.0117, 0.0129, 0.0275, 0.0573, 0.0200],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9693126678466797
 
[Iteration 3] Process ID: 131596 [Epoch: 43,   512/ 591 points] total loss per batch: 1.207
Policy (actual, predicted): 1 1
Policy data: tensor([0.2079, 0.4663, 0.0840, 0.0056, 0.0904, 0.0775, 0.0683],
       device='cuda:0')
Policy pred: tensor([0.2444, 0.4677, 0.0667, 0.0101, 0.0793, 0.0766, 0.0550],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.980048418045044
 
[Iteration 3] Process ID: 131596 [Epoch: 44,   128/ 591 points] total loss per batch: 1.414
Policy (actual, predicted): 6 6
Policy data: tensor([0.0215, 0.0524, 0.0199, 0.0079, 0.0263, 0.0079, 0.8641],
       device='cuda:0')
Policy pred: tensor([0.0125, 0.0563, 0.0103, 0.0038, 0.0199, 0.0062, 0.8910],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999626874923706
 
[Iteration 3] Process ID: 131596 [Epoch: 44,   256/ 591 points] total loss per batch: 1.296
Policy (actual, predicted): 6 6
Policy data: tensor([0.0982, 0.3116, 0.0799, 0.1372, 0.0000, 0.0000, 0.3732],
       device='cuda:0')
Policy pred: tensor([1.0358e-01, 2.9574e-01, 1.0072e-01, 1.3493e-01, 3.2134e-05, 3.0451e-04,
        3.6469e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999998807907104
 
[Iteration 3] Process ID: 131596 [Epoch: 44,   384/ 591 points] total loss per batch: 1.217
Policy (actual, predicted): 1 1
Policy data: tensor([1.0917e-12, 1.0000e+00, 1.0766e-17, 4.9118e-20, 2.1140e-09, 5.0296e-17,
        3.5801e-14], device='cuda:0')
Policy pred: tensor([2.0724e-03, 9.9432e-01, 1.1300e-03, 2.2470e-04, 3.1746e-04, 3.2569e-04,
        1.6081e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999580979347229
 
[Iteration 3] Process ID: 131596 [Epoch: 44,   512/ 591 points] total loss per batch: 1.176
Policy (actual, predicted): 4 4
Policy data: tensor([1.2920e-18, 4.4606e-07, 0.0000e+00, 3.3988e-19, 9.9999e-01, 3.1209e-17,
        4.6129e-06], device='cuda:0')
Policy pred: tensor([7.4992e-06, 2.1580e-03, 1.3038e-03, 8.8583e-04, 9.9256e-01, 7.6941e-04,
        2.3171e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999964833259583
 
[Iteration 3] Process ID: 131596 [Epoch: 45,   128/ 591 points] total loss per batch: 1.314
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.0000, 0.1095, 0.0000, 0.0000, 0.1585, 0.7320],
       device='cuda:0')
Policy pred: tensor([1.2138e-04, 5.2864e-05, 1.1631e-01, 2.3536e-04, 1.0335e-04, 1.6053e-01,
        7.2265e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999862551689148
 
[Iteration 3] Process ID: 131596 [Epoch: 45,   256/ 591 points] total loss per batch: 1.183
Policy (actual, predicted): 6 6
Policy data: tensor([0.1141, 0.2796, 0.0190, 0.1674, 0.0000, 0.0975, 0.3223],
       device='cuda:0')
Policy pred: tensor([1.0044e-01, 3.0160e-01, 1.9387e-02, 1.8004e-01, 4.8113e-05, 9.3599e-02,
        3.0489e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999760985374451
 
[Iteration 3] Process ID: 131596 [Epoch: 45,   384/ 591 points] total loss per batch: 1.386
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 1.7251e-07, 8.4322e-16, 2.3063e-21, 2.4183e-15, 6.9951e-14,
        2.4764e-22], device='cuda:0')
Policy pred: tensor([9.9985e-01, 4.0092e-05, 8.8821e-07, 6.6446e-07, 9.5521e-05, 1.1966e-06,
        1.0437e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 45,   512/ 591 points] total loss per batch: 1.248
Policy (actual, predicted): 1 4
Policy data: tensor([0.1501, 0.1536, 0.1419, 0.1301, 0.1501, 0.1325, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1558, 0.1543, 0.1409, 0.1402, 0.1602, 0.1288, 0.1197],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 46,   128/ 591 points] total loss per batch: 1.290
Policy (actual, predicted): 6 6
Policy data: tensor([0.2005, 0.0000, 0.0790, 0.0339, 0.2685, 0.0857, 0.3324],
       device='cuda:0')
Policy pred: tensor([2.0970e-01, 5.3229e-05, 8.4786e-02, 3.0252e-02, 2.8085e-01, 9.1894e-02,
        3.0247e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 46,   256/ 591 points] total loss per batch: 1.304
Policy (actual, predicted): 5 5
Policy data: tensor([0.0941, 0.0812, 0.0903, 0.0508, 0.0928, 0.5187, 0.0721],
       device='cuda:0')
Policy pred: tensor([0.0488, 0.0567, 0.0600, 0.0486, 0.0589, 0.6545, 0.0725],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999987006187439
 
[Iteration 3] Process ID: 131596 [Epoch: 46,   384/ 591 points] total loss per batch: 1.174
Policy (actual, predicted): 1 1
Policy data: tensor([2.9877e-02, 7.9537e-01, 3.2440e-02, 3.2749e-03, 1.1032e-03, 4.6567e-07,
        1.3793e-01], device='cuda:0')
Policy pred: tensor([0.0298, 0.7822, 0.0358, 0.0115, 0.0066, 0.0015, 0.1326],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999877214431763
 
[Iteration 3] Process ID: 131596 [Epoch: 46,   512/ 591 points] total loss per batch: 1.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0220, 0.0062, 0.0081, 0.0062, 0.0186, 0.9308, 0.0081],
       device='cuda:0')
Policy pred: tensor([0.0318, 0.0076, 0.0076, 0.0056, 0.0178, 0.9224, 0.0073],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9922114014625549
 
[Iteration 3] Process ID: 131596 [Epoch: 47,   128/ 591 points] total loss per batch: 1.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.6231, 0.1329, 0.1020, 0.0218, 0.0187, 0.0494, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.6396, 0.1457, 0.0930, 0.0198, 0.0156, 0.0426, 0.0436],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.01067297998815775
 
[Iteration 3] Process ID: 131596 [Epoch: 47,   256/ 591 points] total loss per batch: 1.267
Policy (actual, predicted): 4 4
Policy data: tensor([0.1372, 0.1465, 0.1442, 0.1313, 0.1535, 0.1419, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1206, 0.1408, 0.1335, 0.1524, 0.1670, 0.1403, 0.1455],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 47,   384/ 591 points] total loss per batch: 1.197
Policy (actual, predicted): 1 1
Policy data: tensor([4.1290e-05, 9.9996e-01, 0.0000e+00, 1.1552e-13, 0.0000e+00, 1.1171e-14,
        5.6815e-08], device='cuda:0')
Policy pred: tensor([1.1381e-03, 9.9805e-01, 1.9541e-05, 4.4286e-05, 1.0618e-05, 4.5806e-05,
        6.8819e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 47,   512/ 591 points] total loss per batch: 1.165
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 0.0000e+00, 4.7631e-08, 0.0000e+00, 2.2985e-07,
        4.4360e-07], device='cuda:0')
Policy pred: tensor([1.5848e-08, 9.9967e-01, 5.5536e-07, 2.1836e-06, 1.4156e-09, 1.0990e-05,
        3.1898e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 48,   128/ 591 points] total loss per batch: 1.373
Policy (actual, predicted): 2 2
Policy data: tensor([2.7122e-01, 2.4647e-02, 4.8035e-01, 1.9650e-04, 1.8649e-01, 1.5233e-03,
        3.5576e-02], device='cuda:0')
Policy pred: tensor([0.2648, 0.0232, 0.5135, 0.0037, 0.1612, 0.0049, 0.0287],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999994039535522
 
[Iteration 3] Process ID: 131596 [Epoch: 48,   256/ 591 points] total loss per batch: 1.326
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9961e-01, 0.0000e+00, 3.8781e-04,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([1.6191e-06, 3.8520e-06, 6.5419e-06, 9.9955e-01, 5.3397e-05, 2.7192e-04,
        1.0876e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999997615814209
 
[Iteration 3] Process ID: 131596 [Epoch: 48,   384/ 591 points] total loss per batch: 1.154
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000e+00, 4.7419e-05, 9.9995e-01, 1.2444e-19, 4.7303e-19, 4.7303e-19,
        4.7303e-19], device='cuda:0')
Policy pred: tensor([2.1908e-08, 9.8638e-05, 9.9964e-01, 2.5538e-06, 2.5088e-05, 2.8896e-06,
        2.3025e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999934434890747
 
[Iteration 3] Process ID: 131596 [Epoch: 48,   512/ 591 points] total loss per batch: 1.263
Policy (actual, predicted): 1 1
Policy data: tensor([0.2079, 0.4663, 0.0840, 0.0056, 0.0904, 0.0775, 0.0683],
       device='cuda:0')
Policy pred: tensor([0.1957, 0.4857, 0.0840, 0.0080, 0.0847, 0.0729, 0.0689],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9935460686683655
 
[Iteration 3] Process ID: 131596 [Epoch: 49,   128/ 591 points] total loss per batch: 1.270
Policy (actual, predicted): 2 5
Policy data: tensor([0.1372, 0.1419, 0.1500, 0.1419, 0.1430, 0.1407, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1332, 0.1511, 0.1475, 0.1370, 0.1409, 0.1567, 0.1336],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9982988834381104
 
[Iteration 3] Process ID: 131596 [Epoch: 49,   256/ 591 points] total loss per batch: 1.169
Policy (actual, predicted): 0 0
Policy data: tensor([0.2785, 0.1397, 0.1668, 0.0548, 0.1527, 0.0548, 0.1527],
       device='cuda:0')
Policy pred: tensor([0.2653, 0.1471, 0.1674, 0.0551, 0.1381, 0.0607, 0.1664],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 49,   384/ 591 points] total loss per batch: 1.391
Policy (actual, predicted): 2 2
Policy data: tensor([0.0217, 0.1514, 0.4426, 0.0057, 0.3459, 0.0156, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.0281, 0.1969, 0.4767, 0.0100, 0.2473, 0.0199, 0.0211],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.5434921979904175
 
[Iteration 3] Process ID: 131596 [Epoch: 49,   512/ 591 points] total loss per batch: 1.210
Policy (actual, predicted): 0 0
Policy data: tensor([0.3128, 0.0000, 0.2017, 0.1191, 0.0000, 0.1891, 0.1773],
       device='cuda:0')
Policy pred: tensor([2.8669e-01, 5.9996e-05, 1.9752e-01, 1.2391e-01, 1.5235e-04, 1.9751e-01,
        1.9416e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999988675117493
 
[Iteration 3] Process ID: 131596 [Epoch: 50,   128/ 591 points] total loss per batch: 1.337
Policy (actual, predicted): 0 0
Policy data: tensor([0.9549, 0.0082, 0.0082, 0.0063, 0.0100, 0.0063, 0.0063],
       device='cuda:0')
Policy pred: tensor([0.9644, 0.0058, 0.0045, 0.0044, 0.0096, 0.0054, 0.0059],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9990649819374084
 
[Iteration 3] Process ID: 131596 [Epoch: 50,   256/ 591 points] total loss per batch: 1.256
Policy (actual, predicted): 1 1
Policy data: tensor([4.3506e-03, 9.6874e-01, 3.9572e-03, 1.5618e-04, 1.6343e-02, 1.5618e-04,
        6.3011e-03], device='cuda:0')
Policy pred: tensor([0.0076, 0.9624, 0.0052, 0.0010, 0.0129, 0.0016, 0.0092],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 50,   384/ 591 points] total loss per batch: 1.173
Policy (actual, predicted): 0 0
Policy data: tensor([7.6937e-01, 6.6311e-02, 4.4232e-02, 1.6654e-05, 1.0559e-01, 2.8256e-04,
        1.4194e-02], device='cuda:0')
Policy pred: tensor([0.6933, 0.0935, 0.0441, 0.0024, 0.1399, 0.0056, 0.0213],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999949336051941
 
[Iteration 3] Process ID: 131596 [Epoch: 50,   512/ 591 points] total loss per batch: 1.367
Policy (actual, predicted): 6 6
Policy data: tensor([0.1291, 0.1977, 0.0124, 0.0124, 0.0124, 0.0619, 0.5741],
       device='cuda:0')
Policy pred: tensor([0.1348, 0.1771, 0.0900, 0.0854, 0.0823, 0.1031, 0.3273],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.0017094670329242945
 
[Iteration 3] Process ID: 131596 [Epoch: 51,   128/ 591 points] total loss per batch: 1.192
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1087, 0.4698, 0.1339, 0.0600, 0.0718, 0.0712, 0.0846],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.0011600437574088573
 
[Iteration 3] Process ID: 131596 [Epoch: 51,   256/ 591 points] total loss per batch: 1.228
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000e+00, 0.0000e+00, 6.5395e-09, 2.8452e-17, 2.0306e-13, 1.0000e+00,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([3.7657e-06, 2.9786e-07, 5.8764e-05, 5.5176e-06, 4.8663e-07, 9.9993e-01,
        1.3531e-06], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999634027481079
 
[Iteration 3] Process ID: 131596 [Epoch: 51,   384/ 591 points] total loss per batch: 1.281
Policy (actual, predicted): 6 6
Policy data: tensor([0.0515, 0.2798, 0.0515, 0.0993, 0.1190, 0.0423, 0.3567],
       device='cuda:0')
Policy pred: tensor([0.0454, 0.2898, 0.0486, 0.0913, 0.1141, 0.0329, 0.3779],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999995231628418
 
[Iteration 3] Process ID: 131596 [Epoch: 51,   512/ 591 points] total loss per batch: 1.351
Policy (actual, predicted): 0 0
Policy data: tensor([0.2507, 0.1776, 0.1360, 0.1360, 0.0780, 0.0858, 0.1360],
       device='cuda:0')
Policy pred: tensor([0.2607, 0.1775, 0.1262, 0.1419, 0.0865, 0.0917, 0.1155],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999234080314636
 
[Iteration 3] Process ID: 131596 [Epoch: 52,   128/ 591 points] total loss per batch: 1.315
Policy (actual, predicted): 6 6
Policy data: tensor([0.1360, 0.1465, 0.1336, 0.1489, 0.1442, 0.1407, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1168, 0.1210, 0.1187, 0.1223, 0.1110, 0.1156, 0.2946],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.1733047217130661
 
[Iteration 3] Process ID: 131596 [Epoch: 52,   256/ 591 points] total loss per batch: 1.125
Policy (actual, predicted): 2 2
Policy data: tensor([0.1155, 0.0361, 0.7418, 0.0128, 0.0160, 0.0255, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.1066, 0.0306, 0.7695, 0.0105, 0.0148, 0.0203, 0.0477],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9781522750854492
 
[Iteration 3] Process ID: 131596 [Epoch: 52,   384/ 591 points] total loss per batch: 1.293
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 1.0168e-10, 1.8056e-09, 1.0412e-17, 1.0168e-20, 1.6816e-18,
        5.8635e-19], device='cuda:0')
Policy pred: tensor([9.9933e-01, 2.6972e-04, 3.3820e-04, 4.7846e-06, 1.4321e-05, 7.3780e-06,
        3.3495e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999964237213135
 
[Iteration 3] Process ID: 131596 [Epoch: 52,   512/ 591 points] total loss per batch: 1.305
Policy (actual, predicted): 0 0
Policy data: tensor([0.3856, 0.2287, 0.0586, 0.0211, 0.1959, 0.0437, 0.0665],
       device='cuda:0')
Policy pred: tensor([0.3351, 0.2402, 0.0686, 0.0287, 0.1943, 0.0548, 0.0782],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999666810035706
 
[Iteration 3] Process ID: 131596 [Epoch: 53,   128/ 591 points] total loss per batch: 1.365
Policy (actual, predicted): 3 3
Policy data: tensor([0.0562, 0.0617, 0.0423, 0.6655, 0.0617, 0.0316, 0.0811],
       device='cuda:0')
Policy pred: tensor([0.0645, 0.0695, 0.0495, 0.6256, 0.0697, 0.0337, 0.0874],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999956488609314
 
[Iteration 3] Process ID: 131596 [Epoch: 53,   256/ 591 points] total loss per batch: 1.184
Policy (actual, predicted): 0 0
Policy data: tensor([0.2710, 0.1486, 0.1135, 0.0484, 0.1931, 0.0484, 0.1771],
       device='cuda:0')
Policy pred: tensor([0.2863, 0.1620, 0.1032, 0.0391, 0.2043, 0.0413, 0.1638],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999997019767761
 
[Iteration 3] Process ID: 131596 [Epoch: 53,   384/ 591 points] total loss per batch: 1.305
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 7.3711e-12, 0.0000e+00, 2.3437e-13, 0.0000e+00, 4.7764e-14,
        1.9392e-12], device='cuda:0')
Policy pred: tensor([9.9996e-01, 1.1500e-05, 4.0867e-09, 1.5111e-07, 9.4793e-13, 8.8182e-08,
        2.6502e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 53,   512/ 591 points] total loss per batch: 1.215
Policy (actual, predicted): 0 0
Policy data: tensor([9.9480e-01, 2.4485e-05, 1.0343e-15, 2.9215e-17, 3.2123e-04, 3.6062e-16,
        4.8568e-03], device='cuda:0')
Policy pred: tensor([9.9403e-01, 3.8383e-04, 2.3052e-05, 1.6767e-05, 1.0968e-04, 2.8161e-05,
        5.4124e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 54,   128/ 591 points] total loss per batch: 1.304
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000, 0.0000, 0.2250, 0.0784, 0.3889, 0.0682, 0.2395],
       device='cuda:0')
Policy pred: tensor([1.3297e-05, 1.3779e-05, 2.2417e-01, 6.3730e-02, 4.0238e-01, 6.5713e-02,
        2.4398e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999783635139465
 
[Iteration 3] Process ID: 131596 [Epoch: 54,   256/ 591 points] total loss per batch: 1.333
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000, 0.0000, 0.2546, 0.1031, 0.3662, 0.0512, 0.2248],
       device='cuda:0')
Policy pred: tensor([2.6652e-05, 4.7223e-05, 2.8110e-01, 9.5122e-02, 3.7743e-01, 4.0345e-02,
        2.0594e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999781847000122
 
[Iteration 3] Process ID: 131596 [Epoch: 54,   384/ 591 points] total loss per batch: 1.231
Policy (actual, predicted): 2 2
Policy data: tensor([0.0468, 0.0409, 0.7888, 0.0226, 0.0527, 0.0226, 0.0257],
       device='cuda:0')
Policy pred: tensor([0.0450, 0.0384, 0.8146, 0.0174, 0.0429, 0.0208, 0.0209],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999465346336365
 
[Iteration 3] Process ID: 131596 [Epoch: 54,   512/ 591 points] total loss per batch: 1.200
Policy (actual, predicted): 0 0
Policy data: tensor([0.6687, 0.0664, 0.1092, 0.0295, 0.0412, 0.0526, 0.0324],
       device='cuda:0')
Policy pred: tensor([0.6556, 0.0642, 0.1171, 0.0306, 0.0390, 0.0526, 0.0408],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998695254325867
 
[Iteration 3] Process ID: 131596 [Epoch: 55,   128/ 591 points] total loss per batch: 1.321
Policy (actual, predicted): 3 3
Policy data: tensor([0.1407, 0.1465, 0.1372, 0.1512, 0.1372, 0.1383, 0.1489],
       device='cuda:0')
Policy pred: tensor([0.1357, 0.1350, 0.1371, 0.1586, 0.1346, 0.1549, 0.1441],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9951097369194031
 
[Iteration 3] Process ID: 131596 [Epoch: 55,   256/ 591 points] total loss per batch: 1.309
Policy (actual, predicted): 0 0
Policy data: tensor([0.4056, 0.0692, 0.3775, 0.0274, 0.0154, 0.0745, 0.0304],
       device='cuda:0')
Policy pred: tensor([0.3871, 0.0827, 0.3705, 0.0189, 0.0194, 0.0840, 0.0374],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.1727551817893982
 
[Iteration 3] Process ID: 131596 [Epoch: 55,   384/ 591 points] total loss per batch: 1.189
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 1.0168e-10, 1.8056e-09, 1.0412e-17, 1.0168e-20, 1.6816e-18,
        5.8635e-19], device='cuda:0')
Policy pred: tensor([9.9983e-01, 6.2382e-05, 9.5483e-05, 7.1340e-07, 2.1543e-06, 1.1401e-06,
        8.6484e-06], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999997615814209
 
[Iteration 3] Process ID: 131596 [Epoch: 55,   512/ 591 points] total loss per batch: 1.263
Policy (actual, predicted): 2 2
Policy data: tensor([0.0468, 0.0409, 0.7888, 0.0226, 0.0527, 0.0226, 0.0257],
       device='cuda:0')
Policy pred: tensor([0.0509, 0.0396, 0.8071, 0.0182, 0.0430, 0.0198, 0.0214],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999464750289917
 
[Iteration 3] Process ID: 131596 [Epoch: 56,   128/ 591 points] total loss per batch: 1.258
Policy (actual, predicted): 1 0
Policy data: tensor([0.1419, 0.1465, 0.1419, 0.1430, 0.1454, 0.1360, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1553, 0.1473, 0.1331, 0.1428, 0.1379, 0.1421, 0.1415],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9995604753494263
 
[Iteration 3] Process ID: 131596 [Epoch: 56,   256/ 591 points] total loss per batch: 1.352
Policy (actual, predicted): 1 1
Policy data: tensor([0.3208, 0.3748, 0.0242, 0.0137, 0.1310, 0.0257, 0.1098],
       device='cuda:0')
Policy pred: tensor([0.2739, 0.5141, 0.0079, 0.0031, 0.1137, 0.0104, 0.0768],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999996423721313
 
[Iteration 3] Process ID: 131596 [Epoch: 56,   384/ 591 points] total loss per batch: 1.224
Policy (actual, predicted): 5 5
Policy data: tensor([6.2106e-05, 8.1482e-04, 7.7706e-11, 1.5128e-13, 1.1037e-10, 9.9470e-01,
        4.4263e-03], device='cuda:0')
Policy pred: tensor([1.5794e-03, 4.3870e-03, 9.0277e-04, 6.1224e-04, 1.2398e-03, 9.8894e-01,
        2.3431e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998807907104
 
[Iteration 3] Process ID: 131596 [Epoch: 56,   512/ 591 points] total loss per batch: 1.189
Policy (actual, predicted): 3 3
Policy data: tensor([0.1687, 0.0000, 0.1583, 0.3738, 0.0000, 0.0219, 0.2773],
       device='cuda:0')
Policy pred: tensor([1.9645e-01, 1.7284e-04, 1.5588e-01, 3.8108e-01, 2.7104e-05, 2.2101e-02,
        2.4429e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999961853027344
 
[Iteration 3] Process ID: 131596 [Epoch: 57,   128/ 591 points] total loss per batch: 1.327
Policy (actual, predicted): 0 0
Policy data: tensor([0.3448, 0.1150, 0.1150, 0.0548, 0.2934, 0.0273, 0.0498],
       device='cuda:0')
Policy pred: tensor([0.3198, 0.1067, 0.1310, 0.0532, 0.3029, 0.0333, 0.0530],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999977350234985
 
[Iteration 3] Process ID: 131596 [Epoch: 57,   256/ 591 points] total loss per batch: 1.184
Policy (actual, predicted): 0 0
Policy data: tensor([0.8966, 0.0098, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.7808, 0.0334, 0.0350, 0.0279, 0.0499, 0.0311, 0.0419],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.07024446129798889
 
[Iteration 3] Process ID: 131596 [Epoch: 57,   384/ 591 points] total loss per batch: 1.294
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 7.7439e-18, 0.0000e+00, 7.8521e-13, 0.0000e+00, 1.7656e-14,
        1.9406e-09], device='cuda:0')
Policy pred: tensor([9.9712e-01, 1.5559e-03, 3.6010e-05, 4.2079e-04, 1.9155e-06, 2.0520e-04,
        6.6015e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999986290931702
 
[Iteration 3] Process ID: 131596 [Epoch: 57,   512/ 591 points] total loss per batch: 1.247
Policy (actual, predicted): 5 5
Policy data: tensor([0.0717, 0.0227, 0.0163, 0.0129, 0.0646, 0.8006, 0.0113],
       device='cuda:0')
Policy pred: tensor([0.0538, 0.0208, 0.0191, 0.0151, 0.0574, 0.8206, 0.0131],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9996433854103088
 
[Iteration 3] Process ID: 131596 [Epoch: 58,   128/ 591 points] total loss per batch: 1.268
Policy (actual, predicted): 2 2
Policy data: tensor([0.0374, 0.0476, 0.7277, 0.0159, 0.0175, 0.0576, 0.0962],
       device='cuda:0')
Policy pred: tensor([0.0978, 0.0998, 0.3977, 0.0796, 0.1041, 0.1005, 0.1206],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9995265007019043
 
[Iteration 3] Process ID: 131596 [Epoch: 58,   256/ 591 points] total loss per batch: 1.230
Policy (actual, predicted): 4 5
Policy data: tensor([0.1466, 0.1419, 0.1489, 0.1313, 0.1524, 0.1360, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1486, 0.1274, 0.1528, 0.1311, 0.1520, 0.1554, 0.1327],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9938819408416748
 
[Iteration 3] Process ID: 131596 [Epoch: 58,   384/ 591 points] total loss per batch: 1.263
Policy (actual, predicted): 4 4
Policy data: tensor([0.1578, 0.1214, 0.1578, 0.0264, 0.2835, 0.0125, 0.2406],
       device='cuda:0')
Policy pred: tensor([0.1717, 0.1302, 0.1660, 0.0280, 0.2541, 0.0165, 0.2335],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999940991401672
 
[Iteration 3] Process ID: 131596 [Epoch: 58,   512/ 591 points] total loss per batch: 1.252
Policy (actual, predicted): 5 4
Policy data: tensor([0.0682, 0.0271, 0.0440, 0.0467, 0.3544, 0.3566, 0.1030],
       device='cuda:0')
Policy pred: tensor([0.0835, 0.0306, 0.0453, 0.0469, 0.3423, 0.3366, 0.1146],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9913172125816345
 
[Iteration 3] Process ID: 131596 [Epoch: 59,   128/ 591 points] total loss per batch: 1.390
Policy (actual, predicted): 4 4
Policy data: tensor([2.1681e-01, 1.4692e-01, 7.6242e-02, 1.0391e-04, 4.5245e-01, 1.0673e-03,
        1.0641e-01], device='cuda:0')
Policy pred: tensor([0.1701, 0.1259, 0.0886, 0.0009, 0.5273, 0.0019, 0.0851],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 59,   256/ 591 points] total loss per batch: 1.241
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.0000, 0.2208, 0.2506, 0.1943, 0.1400, 0.1943],
       device='cuda:0')
Policy pred: tensor([1.6283e-05, 1.0404e-06, 2.0187e-01, 2.6497e-01, 1.6028e-01, 1.4738e-01,
        2.2547e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999971389770508
 
[Iteration 3] Process ID: 131596 [Epoch: 59,   384/ 591 points] total loss per batch: 1.258
Policy (actual, predicted): 1 1
Policy data: tensor([4.2423e-17, 1.0000e+00, 2.3890e-18, 2.3656e-16, 1.1161e-17, 3.8584e-19,
        1.0008e-18], device='cuda:0')
Policy pred: tensor([1.8187e-04, 9.9509e-01, 2.7489e-03, 4.0413e-04, 3.6747e-04, 8.9164e-04,
        3.1867e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998969435691833
 
[Iteration 3] Process ID: 131596 [Epoch: 59,   512/ 591 points] total loss per batch: 1.214
Policy (actual, predicted): 5 5
Policy data: tensor([0.0044, 0.0044, 0.0063, 0.0044, 0.0044, 0.9699, 0.0063],
       device='cuda:0')
Policy pred: tensor([0.0029, 0.0055, 0.0074, 0.0057, 0.0044, 0.9673, 0.0068],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9814435839653015
 
[Iteration 3] Process ID: 131596 [Epoch: 60,   128/ 591 points] total loss per batch: 1.320
Policy (actual, predicted): 4 4
Policy data: tensor([0.1501, 0.1442, 0.1454, 0.1325, 0.1512, 0.1360, 0.1407],
       device='cuda:0')
Policy pred: tensor([0.1630, 0.1247, 0.1477, 0.1199, 0.1752, 0.1301, 0.1395],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9892951250076294
 
[Iteration 3] Process ID: 131596 [Epoch: 60,   256/ 591 points] total loss per batch: 1.331
Policy (actual, predicted): 1 1
Policy data: tensor([0.0330, 0.6655, 0.0208, 0.0094, 0.0192, 0.2410, 0.0111],
       device='cuda:0')
Policy pred: tensor([0.0387, 0.5904, 0.0382, 0.0097, 0.0175, 0.2914, 0.0141],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9970571398735046
 
[Iteration 3] Process ID: 131596 [Epoch: 60,   384/ 591 points] total loss per batch: 1.288
Policy (actual, predicted): 0 2
Policy data: tensor([0.1524, 0.1442, 0.1466, 0.1313, 0.1489, 0.1360, 0.1407],
       device='cuda:0')
Policy pred: tensor([0.1395, 0.1430, 0.1727, 0.1323, 0.1478, 0.1342, 0.1306],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9967424273490906
 
[Iteration 3] Process ID: 131596 [Epoch: 60,   512/ 591 points] total loss per batch: 1.318
Policy (actual, predicted): 0 0
Policy data: tensor([0.2785, 0.1397, 0.1668, 0.0548, 0.1527, 0.0548, 0.1527],
       device='cuda:0')
Policy pred: tensor([0.2125, 0.1819, 0.1376, 0.0782, 0.1620, 0.0694, 0.1584],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 61,   128/ 591 points] total loss per batch: 1.267
Policy (actual, predicted): 2 2
Policy data: tensor([1.3347e-17, 1.0152e-12, 1.0000e+00, 1.0396e-19, 5.9948e-18, 2.9366e-11,
        2.8005e-17], device='cuda:0')
Policy pred: tensor([0.0029, 0.0141, 0.9762, 0.0013, 0.0025, 0.0020, 0.0010],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9943825006484985
 
[Iteration 3] Process ID: 131596 [Epoch: 61,   256/ 591 points] total loss per batch: 1.314
Policy (actual, predicted): 6 6
Policy data: tensor([0.1419, 0.1419, 0.1465, 0.1442, 0.1419, 0.1336, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1392, 0.1415, 0.1350, 0.1485, 0.1471, 0.1210, 0.1677],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999695360660553
 
[Iteration 3] Process ID: 131596 [Epoch: 61,   384/ 591 points] total loss per batch: 1.203
Policy (actual, predicted): 6 6
Policy data: tensor([6.0353e-19, 0.0000e+00, 0.0000e+00, 1.4753e-15, 2.2723e-22, 1.4070e-21,
        1.0000e+00], device='cuda:0')
Policy pred: tensor([2.0643e-04, 4.1934e-08, 4.1914e-05, 4.8022e-02, 1.2132e-03, 2.4270e-04,
        9.5027e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998548030853271
 
[Iteration 3] Process ID: 131596 [Epoch: 61,   512/ 591 points] total loss per batch: 1.416
Policy (actual, predicted): 0 0
Policy data: tensor([0.8949, 0.0116, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.7220, 0.0455, 0.0387, 0.0300, 0.0717, 0.0397, 0.0524],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.0356135219335556
 
[Iteration 3] Process ID: 131596 [Epoch: 62,   128/ 591 points] total loss per batch: 1.210
Policy (actual, predicted): 2 2
Policy data: tensor([5.5501e-20, 5.0910e-11, 1.0000e+00, 1.8023e-19, 1.3407e-18, 5.4200e-23,
        1.4601e-20], device='cuda:0')
Policy pred: tensor([0.0037, 0.0049, 0.9745, 0.0069, 0.0039, 0.0011, 0.0049],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999285936355591
 
[Iteration 3] Process ID: 131596 [Epoch: 62,   256/ 591 points] total loss per batch: 1.235
Policy (actual, predicted): 2 2
Policy data: tensor([0.1829, 0.1103, 0.5624, 0.0123, 0.0123, 0.0421, 0.0776],
       device='cuda:0')
Policy pred: tensor([0.1871, 0.0871, 0.5779, 0.0137, 0.0175, 0.0429, 0.0738],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9480904340744019
 
[Iteration 3] Process ID: 131596 [Epoch: 62,   384/ 591 points] total loss per batch: 1.264
Policy (actual, predicted): 3 3
Policy data: tensor([0.1524, 0.1524, 0.0799, 0.2164, 0.1162, 0.1162, 0.1666],
       device='cuda:0')
Policy pred: tensor([0.1916, 0.1899, 0.0686, 0.2474, 0.0714, 0.0765, 0.1546],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999999463558197
 
[Iteration 3] Process ID: 131596 [Epoch: 62,   512/ 591 points] total loss per batch: 1.373
Policy (actual, predicted): 1 4
Policy data: tensor([0.1454, 0.1477, 0.1360, 0.1465, 0.1442, 0.1348, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.0748, 0.0647, 0.0504, 0.0478, 0.6493, 0.0626, 0.0505],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9987785816192627
 
[Iteration 3] Process ID: 131596 [Epoch: 63,   128/ 591 points] total loss per batch: 1.201
Policy (actual, predicted): 4 6
Policy data: tensor([0.1607, 0.1026, 0.1472, 0.0479, 0.2469, 0.0479, 0.2469],
       device='cuda:0')
Policy pred: tensor([0.1342, 0.1234, 0.1362, 0.0539, 0.2414, 0.0387, 0.2722],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 63,   256/ 591 points] total loss per batch: 1.281
Policy (actual, predicted): 4 4
Policy data: tensor([0.1477, 0.1430, 0.1407, 0.1336, 0.1594, 0.1336, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1451, 0.1631, 0.1390, 0.1093, 0.2022, 0.1079, 0.1335],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 63,   384/ 591 points] total loss per batch: 1.269
Policy (actual, predicted): 1 4
Policy data: tensor([0.1336, 0.1489, 0.1489, 0.1454, 0.1407, 0.1372, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1199, 0.1532, 0.1353, 0.1381, 0.1558, 0.1523, 0.1455],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999988079071045
 
[Iteration 3] Process ID: 131596 [Epoch: 63,   512/ 591 points] total loss per batch: 1.338
Policy (actual, predicted): 0 4
Policy data: tensor([0.2726, 0.1151, 0.1639, 0.0448, 0.2509, 0.0269, 0.1258],
       device='cuda:0')
Policy pred: tensor([0.2022, 0.1250, 0.2042, 0.0419, 0.2387, 0.0391, 0.1488],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999994039535522
 
[Iteration 3] Process ID: 131596 [Epoch: 64,   128/ 591 points] total loss per batch: 1.281
Policy (actual, predicted): 0 0
Policy data: tensor([0.6231, 0.1329, 0.1020, 0.0218, 0.0187, 0.0494, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.6337, 0.1188, 0.1035, 0.0229, 0.0209, 0.0495, 0.0507],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -4.013627767562866e-05
 
[Iteration 3] Process ID: 131596 [Epoch: 64,   256/ 591 points] total loss per batch: 1.317
Policy (actual, predicted): 0 0
Policy data: tensor([0.7867, 0.0395, 0.0146, 0.0041, 0.0868, 0.0425, 0.0258],
       device='cuda:0')
Policy pred: tensor([0.7166, 0.0466, 0.0284, 0.0077, 0.1127, 0.0607, 0.0273],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9997448325157166
 
[Iteration 3] Process ID: 131596 [Epoch: 64,   384/ 591 points] total loss per batch: 1.236
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1201, 0.4286, 0.1303, 0.0671, 0.0791, 0.0842, 0.0906],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.003571908688172698
 
[Iteration 3] Process ID: 131596 [Epoch: 64,   512/ 591 points] total loss per batch: 1.285
Policy (actual, predicted): 3 3
Policy data: tensor([0.1010, 0.0631, 0.0695, 0.3116, 0.2641, 0.0695, 0.1211],
       device='cuda:0')
Policy pred: tensor([0.0998, 0.0536, 0.0667, 0.3194, 0.2757, 0.0465, 0.1383],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999886751174927
 
[Iteration 3] Process ID: 131596 [Epoch: 65,   128/ 591 points] total loss per batch: 1.096
Policy (actual, predicted): 0 0
Policy data: tensor([0.3022, 0.1275, 0.1665, 0.0604, 0.1665, 0.0604, 0.1165],
       device='cuda:0')
Policy pred: tensor([0.3287, 0.1207, 0.1685, 0.0600, 0.1544, 0.0514, 0.1164],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999970197677612
 
[Iteration 3] Process ID: 131596 [Epoch: 65,   256/ 591 points] total loss per batch: 1.240
Policy (actual, predicted): 6 6
Policy data: tensor([0.2152, 0.0000, 0.2019, 0.1041, 0.0000, 0.2019, 0.2768],
       device='cuda:0')
Policy pred: tensor([2.2042e-01, 4.6473e-05, 1.9300e-01, 1.0872e-01, 5.4204e-05, 2.1413e-01,
        2.6363e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 65,   384/ 591 points] total loss per batch: 1.384
Policy (actual, predicted): 5 5
Policy data: tensor([0.0313, 0.1597, 0.0387, 0.0475, 0.0143, 0.6910, 0.0175],
       device='cuda:0')
Policy pred: tensor([0.0325, 0.2406, 0.0488, 0.0514, 0.0144, 0.5867, 0.0255],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999978542327881
 
[Iteration 3] Process ID: 131596 [Epoch: 65,   512/ 591 points] total loss per batch: 1.334
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0847, 0.1998, 0.0737, 0.4154, 0.2265],
       device='cuda:0')
Policy pred: tensor([2.5503e-04, 4.6621e-04, 1.1310e-01, 2.6313e-01, 6.0531e-02, 3.3061e-01,
        2.3191e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999912977218628
 
[Iteration 3] Process ID: 131596 [Epoch: 66,   128/ 591 points] total loss per batch: 1.194
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 3.2985e-10, 8.8103e-19, 5.0805e-17, 5.0805e-17,
        8.4022e-15], device='cuda:0')
Policy pred: tensor([2.5708e-06, 9.9893e-01, 7.8084e-04, 1.1403e-05, 1.6026e-04, 3.8608e-05,
        7.1650e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999725222587585
 
[Iteration 3] Process ID: 131596 [Epoch: 66,   256/ 591 points] total loss per batch: 1.205
Policy (actual, predicted): 0 0
Policy data: tensor([0.8949, 0.0116, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.7651, 0.0333, 0.0359, 0.0311, 0.0565, 0.0331, 0.0449],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.03499523922801018
 
[Iteration 3] Process ID: 131596 [Epoch: 66,   384/ 591 points] total loss per batch: 1.313
Policy (actual, predicted): 2 2
Policy data: tensor([0.0767, 0.2093, 0.5781, 0.0074, 0.0686, 0.0336, 0.0263],
       device='cuda:0')
Policy pred: tensor([0.1132, 0.2122, 0.5190, 0.0078, 0.0912, 0.0299, 0.0267],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999153017997742
 
[Iteration 3] Process ID: 131596 [Epoch: 66,   512/ 591 points] total loss per batch: 1.292
Policy (actual, predicted): 2 2
Policy data: tensor([0.1917, 0.1435, 0.5590, 0.0247, 0.0232, 0.0407, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.1894, 0.1547, 0.5397, 0.0280, 0.0258, 0.0447, 0.0177],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9966505765914917
 
[Iteration 3] Process ID: 131596 [Epoch: 67,   128/ 591 points] total loss per batch: 1.293
Policy (actual, predicted): 3 3
Policy data: tensor([0.0665, 0.0665, 0.0879, 0.5117, 0.0963, 0.0452, 0.1260],
       device='cuda:0')
Policy pred: tensor([0.0794, 0.0509, 0.0926, 0.5116, 0.1028, 0.0458, 0.1170],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999984085559845
 
[Iteration 3] Process ID: 131596 [Epoch: 67,   256/ 591 points] total loss per batch: 1.275
Policy (actual, predicted): 6 6
Policy data: tensor([0.1395, 0.1465, 0.1348, 0.1489, 0.1407, 0.1383, 0.1512],
       device='cuda:0')
Policy pred: tensor([0.1518, 0.1357, 0.1205, 0.1439, 0.1497, 0.1418, 0.1565],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.998322606086731
 
[Iteration 3] Process ID: 131596 [Epoch: 67,   384/ 591 points] total loss per batch: 1.265
Policy (actual, predicted): 0 0
Policy data: tensor([9.9660e-01, 0.0000e+00, 9.8604e-05, 1.8227e-05, 1.8227e-05, 3.9518e-05,
        3.2289e-03], device='cuda:0')
Policy pred: tensor([9.9354e-01, 8.2247e-05, 1.2030e-03, 7.2508e-04, 1.5523e-04, 1.0818e-03,
        3.2144e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 67,   512/ 591 points] total loss per batch: 1.244
Policy (actual, predicted): 1 1
Policy data: tensor([1.2350e-18, 1.0000e+00, 3.4590e-21, 3.7141e-22, 2.0171e-07, 6.4565e-10,
        3.8945e-16], device='cuda:0')
Policy pred: tensor([3.0990e-04, 9.9743e-01, 2.5276e-04, 4.2085e-04, 1.7927e-04, 9.2411e-05,
        1.3123e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999315738677979
 
[Iteration 3] Process ID: 131596 [Epoch: 68,   128/ 591 points] total loss per batch: 1.207
Policy (actual, predicted): 2 2
Policy data: tensor([0.2128, 0.2892, 0.4030, 0.0122, 0.0122, 0.0583, 0.0122],
       device='cuda:0')
Policy pred: tensor([0.2345, 0.2907, 0.3650, 0.0160, 0.0182, 0.0586, 0.0170],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.994962751865387
 
[Iteration 3] Process ID: 131596 [Epoch: 68,   256/ 591 points] total loss per batch: 1.195
Policy (actual, predicted): 2 4
Policy data: tensor([0.1489, 0.1430, 0.1512, 0.1336, 0.1501, 0.1336, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1574, 0.1364, 0.1248, 0.1505, 0.1624, 0.1304, 0.1381],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999706149101257
 
[Iteration 3] Process ID: 131596 [Epoch: 68,   384/ 591 points] total loss per batch: 1.396
Policy (actual, predicted): 0 0
Policy data: tensor([0.4016, 0.0000, 0.0630, 0.0000, 0.0000, 0.3145, 0.2210],
       device='cuda:0')
Policy pred: tensor([4.3755e-01, 2.4107e-05, 4.9965e-02, 2.3163e-04, 4.1725e-06, 2.9511e-01,
        2.1712e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999966025352478
 
[Iteration 3] Process ID: 131596 [Epoch: 68,   512/ 591 points] total loss per batch: 1.258
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.0788, 0.1151, 0.0103, 0.2357, 0.1455, 0.3346],
       device='cuda:0')
Policy pred: tensor([0.0695, 0.0709, 0.1357, 0.0164, 0.2286, 0.1488, 0.3301],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9864702820777893
 
[Iteration 3] Process ID: 131596 [Epoch: 69,   128/ 591 points] total loss per batch: 1.316
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9906e-01, 0.0000e+00, 9.3862e-04,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([2.4687e-06, 6.3528e-07, 7.1240e-06, 9.9993e-01, 2.5456e-07, 4.5759e-05,
        1.5758e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999868273735046
 
[Iteration 3] Process ID: 131596 [Epoch: 69,   256/ 591 points] total loss per batch: 1.321
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0023, 0.0000, 0.9977, 0.0000],
       device='cuda:0')
Policy pred: tensor([1.2798e-04, 9.6671e-07, 8.8480e-05, 1.6975e-03, 2.0523e-07, 9.9805e-01,
        3.3255e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999992847442627
 
[Iteration 3] Process ID: 131596 [Epoch: 69,   384/ 591 points] total loss per batch: 1.274
Policy (actual, predicted): 2 2
Policy data: tensor([0.1977, 0.1435, 0.5532, 0.0247, 0.0232, 0.0407, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.1848, 0.1386, 0.5594, 0.0249, 0.0237, 0.0503, 0.0184],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9971247911453247
 
[Iteration 3] Process ID: 131596 [Epoch: 69,   512/ 591 points] total loss per batch: 1.131
Policy (actual, predicted): 2 2
Policy data: tensor([0.0169, 0.0099, 0.9266, 0.0099, 0.0117, 0.0099, 0.0152],
       device='cuda:0')
Policy pred: tensor([0.0149, 0.0104, 0.9345, 0.0075, 0.0092, 0.0085, 0.0149],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9882688522338867
 
[Iteration 3] Process ID: 131596 [Epoch: 70,   128/ 591 points] total loss per batch: 1.233
Policy (actual, predicted): 6 6
Policy data: tensor([0.0117, 0.0134, 0.0099, 0.0081, 0.0168, 0.0168, 0.9233],
       device='cuda:0')
Policy pred: tensor([0.1163, 0.1149, 0.1156, 0.1221, 0.1245, 0.1170, 0.2896],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.17760425806045532
 
[Iteration 3] Process ID: 131596 [Epoch: 70,   256/ 591 points] total loss per batch: 1.305
Policy (actual, predicted): 0 0
Policy data: tensor([0.8966, 0.0098, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.7189, 0.0397, 0.0450, 0.0382, 0.0625, 0.0411, 0.0546],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.020479727536439896
 
[Iteration 3] Process ID: 131596 [Epoch: 70,   384/ 591 points] total loss per batch: 1.247
Policy (actual, predicted): 1 1
Policy data: tensor([2.9505e-19, 9.9981e-01, 6.4039e-15, 1.9154e-04, 6.0232e-16, 1.0200e-20,
        6.4039e-15], device='cuda:0')
Policy pred: tensor([5.5970e-04, 9.9761e-01, 3.6035e-04, 9.7282e-04, 1.2274e-05, 1.2983e-04,
        3.5163e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 70,   512/ 591 points] total loss per batch: 1.318
Policy (actual, predicted): 6 1
Policy data: tensor([0.1407, 0.1442, 0.1419, 0.1336, 0.1407, 0.1384, 0.1605],
       device='cuda:0')
Policy pred: tensor([0.1455, 0.1672, 0.1528, 0.1185, 0.1268, 0.1412, 0.1481],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999967813491821
 
[Iteration 3] Process ID: 131596 [Epoch: 71,   128/ 591 points] total loss per batch: 1.207
Policy (actual, predicted): 4 0
Policy data: tensor([0.1501, 0.1430, 0.1442, 0.1313, 0.1559, 0.1360, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1694, 0.1545, 0.1398, 0.1175, 0.1516, 0.1333, 0.1339],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.970404863357544
 
[Iteration 3] Process ID: 131596 [Epoch: 71,   256/ 591 points] total loss per batch: 1.182
Policy (actual, predicted): 0 0
Policy data: tensor([0.5066, 0.0453, 0.1017, 0.0668, 0.0548, 0.1130, 0.1118],
       device='cuda:0')
Policy pred: tensor([0.5324, 0.0426, 0.0876, 0.0652, 0.0491, 0.1069, 0.1161],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999947547912598
 
[Iteration 3] Process ID: 131596 [Epoch: 71,   384/ 591 points] total loss per batch: 1.356
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0048, 0.0000, 0.9952, 0.0000],
       device='cuda:0')
Policy pred: tensor([1.3206e-05, 2.5038e-06, 5.8013e-06, 1.3195e-03, 2.0548e-07, 9.9832e-01,
        3.3414e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 71,   512/ 591 points] total loss per batch: 1.204
Policy (actual, predicted): 2 2
Policy data: tensor([0.1282, 0.1678, 0.1833, 0.1536, 0.0732, 0.1536, 0.1404],
       device='cuda:0')
Policy pred: tensor([0.1189, 0.1624, 0.1938, 0.1607, 0.0777, 0.1387, 0.1477],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999997615814209
 
[Iteration 3] Process ID: 131596 [Epoch: 72,   128/ 591 points] total loss per batch: 1.262
Policy (actual, predicted): 4 4
Policy data: tensor([0.2260, 0.1036, 0.0947, 0.0365, 0.4309, 0.0365, 0.0718],
       device='cuda:0')
Policy pred: tensor([0.2143, 0.1189, 0.0891, 0.0272, 0.4461, 0.0366, 0.0678],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999995231628418
 
[Iteration 3] Process ID: 131596 [Epoch: 72,   256/ 591 points] total loss per batch: 1.431
Policy (actual, predicted): 6 6
Policy data: tensor([0.0515, 0.2798, 0.0515, 0.0993, 0.1190, 0.0423, 0.3567],
       device='cuda:0')
Policy pred: tensor([0.0585, 0.2765, 0.0524, 0.0945, 0.1209, 0.0425, 0.3548],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999998927116394
 
[Iteration 3] Process ID: 131596 [Epoch: 72,   384/ 591 points] total loss per batch: 1.177
Policy (actual, predicted): 4 4
Policy data: tensor([0.0755, 0.0567, 0.1094, 0.2027, 0.3624, 0.0624, 0.1310],
       device='cuda:0')
Policy pred: tensor([0.0925, 0.0581, 0.1042, 0.2098, 0.3439, 0.0670, 0.1246],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999971985816956
 
[Iteration 3] Process ID: 131596 [Epoch: 72,   512/ 591 points] total loss per batch: 1.213
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0053, 0.0093, 0.0184, 0.0051, 0.0206, 0.0148, 0.9265],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.31068235635757446
 
[Iteration 3] Process ID: 131596 [Epoch: 73,   128/ 591 points] total loss per batch: 1.269
Policy (actual, predicted): 1 1
Policy data: tensor([4.3506e-03, 9.6874e-01, 3.9572e-03, 1.5618e-04, 1.6343e-02, 1.5618e-04,
        6.3011e-03], device='cuda:0')
Policy pred: tensor([7.0592e-03, 9.6669e-01, 4.7607e-03, 7.7942e-04, 1.1856e-02, 1.5551e-03,
        7.2955e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 73,   256/ 591 points] total loss per batch: 1.306
Policy (actual, predicted): 4 4
Policy data: tensor([0.2228, 0.1722, 0.1008, 0.1104, 0.2863, 0.0314, 0.0762],
       device='cuda:0')
Policy pred: tensor([0.2139, 0.1974, 0.1088, 0.1263, 0.2439, 0.0328, 0.0770],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999973773956299
 
[Iteration 3] Process ID: 131596 [Epoch: 73,   384/ 591 points] total loss per batch: 1.344
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.0788, 0.1151, 0.0103, 0.2357, 0.1455, 0.3346],
       device='cuda:0')
Policy pred: tensor([0.0777, 0.0618, 0.1052, 0.0131, 0.2552, 0.1381, 0.3488],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9967046976089478
 
[Iteration 3] Process ID: 131596 [Epoch: 73,   512/ 591 points] total loss per batch: 1.226
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 1.3453e-19, 0.0000e+00, 3.8584e-19, 1.0000e+00, 5.1790e-13,
        1.0248e-15], device='cuda:0')
Policy pred: tensor([8.1723e-07, 1.1869e-04, 7.0649e-07, 8.3195e-07, 9.9987e-01, 9.6623e-07,
        3.7693e-06], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 74,   128/ 591 points] total loss per batch: 1.289
Policy (actual, predicted): 2 2
Policy data: tensor([0.1282, 0.1678, 0.1833, 0.1536, 0.0732, 0.1536, 0.1404],
       device='cuda:0')
Policy pred: tensor([0.1190, 0.1569, 0.1787, 0.1626, 0.0772, 0.1590, 0.1466],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999995231628418
 
[Iteration 3] Process ID: 131596 [Epoch: 74,   256/ 591 points] total loss per batch: 1.327
Policy (actual, predicted): 4 4
Policy data: tensor([0.0183, 0.0297, 0.0133, 0.0150, 0.8920, 0.0167, 0.0150],
       device='cuda:0')
Policy pred: tensor([0.0207, 0.0306, 0.0102, 0.0173, 0.8833, 0.0201, 0.0177],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9957877993583679
 
[Iteration 3] Process ID: 131596 [Epoch: 74,   384/ 591 points] total loss per batch: 1.283
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1200, 0.4378, 0.1419, 0.0669, 0.0744, 0.0757, 0.0833],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.005190527066588402
 
[Iteration 3] Process ID: 131596 [Epoch: 74,   512/ 591 points] total loss per batch: 1.214
Policy (actual, predicted): 1 0
Policy data: tensor([0.1454, 0.1500, 0.1465, 0.1325, 0.1477, 0.1360, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1629, 0.1529, 0.1542, 0.1307, 0.1241, 0.1353, 0.1400],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999970197677612
 
[Iteration 3] Process ID: 131596 [Epoch: 75,   128/ 591 points] total loss per batch: 1.214
Policy (actual, predicted): 6 6
Policy data: tensor([0.0953, 0.1405, 0.0144, 0.0094, 0.0144, 0.0111, 0.7149],
       device='cuda:0')
Policy pred: tensor([0.0827, 0.1298, 0.0088, 0.0045, 0.0113, 0.0075, 0.7554],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9972431063652039
 
[Iteration 3] Process ID: 131596 [Epoch: 75,   256/ 591 points] total loss per batch: 1.139
Policy (actual, predicted): 2 2
Policy data: tensor([0.1156, 0.0361, 0.7432, 0.0128, 0.0160, 0.0240, 0.0523],
       device='cuda:0')
Policy pred: tensor([0.1194, 0.0355, 0.7343, 0.0124, 0.0156, 0.0263, 0.0565],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9905520677566528
 
[Iteration 3] Process ID: 131596 [Epoch: 75,   384/ 591 points] total loss per batch: 1.282
Policy (actual, predicted): 6 6
Policy data: tensor([0.1271, 0.1174, 0.1498, 0.0625, 0.1814, 0.0573, 0.3045],
       device='cuda:0')
Policy pred: tensor([0.1292, 0.1324, 0.1413, 0.0599, 0.1828, 0.0551, 0.2992],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9952502250671387
 
[Iteration 3] Process ID: 131596 [Epoch: 75,   512/ 591 points] total loss per batch: 1.337
Policy (actual, predicted): 0 0
Policy data: tensor([7.4563e-01, 4.7480e-04, 2.5335e-01, 7.2741e-10, 6.9555e-05, 3.4502e-07,
        4.7480e-04], device='cuda:0')
Policy pred: tensor([7.7805e-01, 2.3813e-03, 2.1431e-01, 4.2239e-04, 3.1144e-03, 4.3812e-04,
        1.2820e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 76,   128/ 591 points] total loss per batch: 1.272
Policy (actual, predicted): 2 2
Policy data: tensor([0.2808, 0.0266, 0.5874, 0.0173, 0.0220, 0.0471, 0.0189],
       device='cuda:0')
Policy pred: tensor([0.2852, 0.0243, 0.5858, 0.0161, 0.0225, 0.0459, 0.0201],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9978328943252563
 
[Iteration 3] Process ID: 131596 [Epoch: 76,   256/ 591 points] total loss per batch: 1.274
Policy (actual, predicted): 4 1
Policy data: tensor([0.1512, 0.1454, 0.1477, 0.1289, 0.1524, 0.1360, 0.1384],
       device='cuda:0')
Policy pred: tensor([0.1088, 0.4698, 0.1349, 0.0610, 0.0724, 0.0744, 0.0787],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.004119503311812878
 
[Iteration 3] Process ID: 131596 [Epoch: 76,   384/ 591 points] total loss per batch: 1.237
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 2.5725e-22, 0.0000e+00, 1.0000e+00, 1.6310e-18, 1.5928e-21,
        2.9658e-14], device='cuda:0')
Policy pred: tensor([2.6357e-04, 4.1561e-04, 8.7725e-07, 9.9784e-01, 3.5436e-04, 4.3240e-04,
        6.8918e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999897480010986
 
[Iteration 3] Process ID: 131596 [Epoch: 76,   512/ 591 points] total loss per batch: 1.336
Policy (actual, predicted): 6 6
Policy data: tensor([0.1419, 0.1454, 0.1348, 0.1465, 0.1430, 0.1383, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1425, 0.1550, 0.1250, 0.1531, 0.1379, 0.1308, 0.1556],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999993443489075
 
[Iteration 3] Process ID: 131596 [Epoch: 77,   128/ 591 points] total loss per batch: 1.252
Policy (actual, predicted): 1 1
Policy data: tensor([0.0149, 0.3927, 0.1064, 0.1443, 0.0000, 0.0436, 0.2980],
       device='cuda:0')
Policy pred: tensor([1.6249e-02, 3.7280e-01, 1.0558e-01, 1.3987e-01, 1.9386e-05, 4.3258e-02,
        3.2222e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999991059303284
 
[Iteration 3] Process ID: 131596 [Epoch: 77,   256/ 591 points] total loss per batch: 1.339
Policy (actual, predicted): 2 2
Policy data: tensor([1.3347e-17, 1.0152e-12, 1.0000e+00, 1.0396e-19, 5.9948e-18, 2.9366e-11,
        2.8005e-17], device='cuda:0')
Policy pred: tensor([1.5179e-04, 9.7682e-04, 9.9782e-01, 2.4988e-04, 3.3820e-04, 2.3511e-04,
        2.2545e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.998775064945221
 
[Iteration 3] Process ID: 131596 [Epoch: 77,   384/ 591 points] total loss per batch: 1.292
Policy (actual, predicted): 3 3
Policy data: tensor([0.0738, 0.0904, 0.2061, 0.3947, 0.1288, 0.0712, 0.0351],
       device='cuda:0')
Policy pred: tensor([0.0642, 0.1028, 0.2253, 0.3488, 0.1509, 0.0723, 0.0356],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999886751174927
 
[Iteration 3] Process ID: 131596 [Epoch: 77,   512/ 591 points] total loss per batch: 1.170
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 0.0000e+00, 8.6775e-11, 0.0000e+00, 4.4870e-10,
        6.3314e-09], device='cuda:0')
Policy pred: tensor([9.0304e-09, 9.9985e-01, 9.7604e-08, 1.0092e-05, 2.3428e-09, 7.2189e-06,
        1.3066e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 78,   128/ 591 points] total loss per batch: 1.369
Policy (actual, predicted): 6 6
Policy data: tensor([0.0926, 0.1207, 0.1029, 0.0514, 0.0332, 0.0374, 0.5617],
       device='cuda:0')
Policy pred: tensor([0.0900, 0.1101, 0.1063, 0.0459, 0.0347, 0.0344, 0.5785],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998136162757874
 
[Iteration 3] Process ID: 131596 [Epoch: 78,   256/ 591 points] total loss per batch: 1.249
Policy (actual, predicted): 3 3
Policy data: tensor([1.6772e-04, 0.0000e+00, 5.7598e-04, 9.9391e-01, 6.8750e-04, 1.3587e-05,
        4.6453e-03], device='cuda:0')
Policy pred: tensor([4.4358e-04, 4.9410e-05, 7.5068e-04, 9.9175e-01, 1.3229e-03, 7.4426e-05,
        5.6079e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999533891677856
 
[Iteration 3] Process ID: 131596 [Epoch: 78,   384/ 591 points] total loss per batch: 1.261
Policy (actual, predicted): 2 0
Policy data: tensor([0.1466, 0.1489, 0.1501, 0.1313, 0.1489, 0.1301, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1576, 0.1417, 0.1386, 0.1414, 0.1313, 0.1416, 0.1478],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998714923858643
 
[Iteration 3] Process ID: 131596 [Epoch: 78,   512/ 591 points] total loss per batch: 1.221
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.0000, 0.2208, 0.2506, 0.1943, 0.1400, 0.1943],
       device='cuda:0')
Policy pred: tensor([2.3362e-05, 1.1469e-06, 2.3363e-01, 2.5982e-01, 1.7963e-01, 1.5643e-01,
        1.7046e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999994695186615
 
[Iteration 3] Process ID: 131596 [Epoch: 79,   128/ 591 points] total loss per batch: 1.371
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1246, 0.3492, 0.1521, 0.0857, 0.0980, 0.0980, 0.0924],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.0005936733796261251
 
[Iteration 3] Process ID: 131596 [Epoch: 79,   256/ 591 points] total loss per batch: 1.154
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.0000, 0.2208, 0.2506, 0.1943, 0.1400, 0.1943],
       device='cuda:0')
Policy pred: tensor([7.7006e-05, 3.7447e-06, 2.1894e-01, 2.8503e-01, 1.7255e-01, 1.2380e-01,
        1.9961e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999966621398926
 
[Iteration 3] Process ID: 131596 [Epoch: 79,   384/ 591 points] total loss per batch: 1.242
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 4.6359e-12, 0.0000e+00, 1.6805e-17, 0.0000e+00, 3.7417e-17,
        6.7988e-07], device='cuda:0')
Policy pred: tensor([9.9976e-01, 1.8786e-04, 6.1347e-07, 9.4040e-06, 1.0190e-07, 1.4275e-06,
        4.0535e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999983906745911
 
[Iteration 3] Process ID: 131596 [Epoch: 79,   512/ 591 points] total loss per batch: 1.303
Policy (actual, predicted): 1 1
Policy data: tensor([0.0943, 0.2116, 0.1628, 0.1940, 0.0710, 0.1035, 0.1628],
       device='cuda:0')
Policy pred: tensor([0.0994, 0.2180, 0.1453, 0.1996, 0.0709, 0.1157, 0.1510],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999999463558197
 
[Iteration 3] Process ID: 131596 [Epoch: 80,   128/ 591 points] total loss per batch: 1.244
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 1.3434e-10, 0.0000e+00, 1.4982e-09, 1.0000e+00, 1.8493e-18,
        1.1451e-07], device='cuda:0')
Policy pred: tensor([1.2053e-06, 1.5268e-05, 1.0091e-05, 7.9372e-04, 9.9876e-01, 2.4291e-05,
        3.9433e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999997615814209
 
[Iteration 3] Process ID: 131596 [Epoch: 80,   256/ 591 points] total loss per batch: 1.226
Policy (actual, predicted): 1 1
Policy data: tensor([0.1738, 0.6543, 0.0834, 0.0015, 0.0541, 0.0044, 0.0285],
       device='cuda:0')
Policy pred: tensor([0.1394, 0.6953, 0.0790, 0.0033, 0.0516, 0.0043, 0.0270],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999993443489075
 
[Iteration 3] Process ID: 131596 [Epoch: 80,   384/ 591 points] total loss per batch: 1.338
Policy (actual, predicted): 2 2
Policy data: tensor([0.0762, 0.1530, 0.5734, 0.0074, 0.0775, 0.0894, 0.0231],
       device='cuda:0')
Policy pred: tensor([0.0810, 0.1536, 0.5777, 0.0060, 0.0815, 0.0784, 0.0219],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9988004565238953
 
[Iteration 3] Process ID: 131596 [Epoch: 80,   512/ 591 points] total loss per batch: 1.282
Policy (actual, predicted): 0 0
Policy data: tensor([0.4016, 0.0000, 0.0630, 0.0000, 0.0000, 0.3145, 0.2210],
       device='cuda:0')
Policy pred: tensor([4.0032e-01, 2.7981e-05, 5.6564e-02, 2.8800e-04, 1.8859e-05, 3.0868e-01,
        2.3410e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998296499252319
 
[Iteration 3] Process ID: 131596 [Epoch: 81,   128/ 591 points] total loss per batch: 1.269
Policy (actual, predicted): 6 2
Policy data: tensor([0.1419, 0.1419, 0.1465, 0.1442, 0.1419, 0.1336, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1253, 0.1512, 0.1728, 0.1339, 0.1432, 0.1248, 0.1487],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998840093612671
 
[Iteration 3] Process ID: 131596 [Epoch: 81,   256/ 591 points] total loss per batch: 1.258
Policy (actual, predicted): 2 2
Policy data: tensor([0.1925, 0.1132, 0.2284, 0.0322, 0.1925, 0.0646, 0.1766],
       device='cuda:0')
Policy pred: tensor([0.1842, 0.1110, 0.2619, 0.0294, 0.1793, 0.0588, 0.1755],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 81,   384/ 591 points] total loss per batch: 1.256
Policy (actual, predicted): 2 2
Policy data: tensor([0.1419, 0.1477, 0.1489, 0.1442, 0.1336, 0.1372, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1422, 0.1442, 0.1613, 0.1356, 0.1354, 0.1417, 0.1396],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998785853385925
 
[Iteration 3] Process ID: 131596 [Epoch: 81,   512/ 591 points] total loss per batch: 1.293
Policy (actual, predicted): 6 6
Policy data: tensor([0.1209, 0.0000, 0.0712, 0.3629, 0.0000, 0.0141, 0.4309],
       device='cuda:0')
Policy pred: tensor([1.1466e-01, 5.1113e-06, 7.0840e-02, 3.6674e-01, 2.9158e-06, 1.3801e-02,
        4.3395e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 82,   128/ 591 points] total loss per batch: 1.226
Policy (actual, predicted): 1 1
Policy data: tensor([4.3506e-03, 9.6874e-01, 3.9572e-03, 1.5618e-04, 1.6343e-02, 1.5618e-04,
        6.3011e-03], device='cuda:0')
Policy pred: tensor([5.4721e-03, 9.7223e-01, 3.5788e-03, 6.4572e-04, 1.1171e-02, 9.7490e-04,
        5.9228e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 82,   256/ 591 points] total loss per batch: 1.236
Policy (actual, predicted): 2 2
Policy data: tensor([0.2505, 0.0671, 0.2990, 0.2149, 0.0902, 0.0433, 0.0351],
       device='cuda:0')
Policy pred: tensor([0.2168, 0.0665, 0.3180, 0.2496, 0.0792, 0.0393, 0.0306],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9931707382202148
 
[Iteration 3] Process ID: 131596 [Epoch: 82,   384/ 591 points] total loss per batch: 1.227
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 7.3577e-09, 1.4096e-06, 5.9101e-21, 1.7095e-19, 8.3431e-17,
        3.6594e-20], device='cuda:0')
Policy pred: tensor([9.9752e-01, 1.6764e-03, 4.6990e-04, 1.0810e-05, 5.9598e-05, 3.8138e-05,
        2.2348e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999600052833557
 
[Iteration 3] Process ID: 131596 [Epoch: 82,   512/ 591 points] total loss per batch: 1.334
Policy (actual, predicted): 6 6
Policy data: tensor([0.2152, 0.0000, 0.2019, 0.1041, 0.0000, 0.2019, 0.2768],
       device='cuda:0')
Policy pred: tensor([2.2170e-01, 9.1211e-06, 1.8270e-01, 1.0638e-01, 3.2092e-05, 2.0005e-01,
        2.8913e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 83,   128/ 591 points] total loss per batch: 1.237
Policy (actual, predicted): 4 6
Policy data: tensor([0.1501, 0.1501, 0.1442, 0.1266, 0.1571, 0.1230, 0.1489],
       device='cuda:0')
Policy pred: tensor([0.1403, 0.1399, 0.1479, 0.1291, 0.1497, 0.1349, 0.1582],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999978244304657
 
[Iteration 3] Process ID: 131596 [Epoch: 83,   256/ 591 points] total loss per batch: 1.358
Policy (actual, predicted): 6 6
Policy data: tensor([0.0184, 0.0062, 0.0299, 0.0062, 0.0062, 0.0283, 0.9049],
       device='cuda:0')
Policy pred: tensor([0.0207, 0.0077, 0.0302, 0.0092, 0.0099, 0.0293, 0.8930],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9927821755409241
 
[Iteration 3] Process ID: 131596 [Epoch: 83,   384/ 591 points] total loss per batch: 1.242
Policy (actual, predicted): 4 4
Policy data: tensor([0.0867, 0.1944, 0.0789, 0.1143, 0.2510, 0.0441, 0.2306],
       device='cuda:0')
Policy pred: tensor([0.0742, 0.2063, 0.0793, 0.1111, 0.2516, 0.0423, 0.2352],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999993443489075
 
[Iteration 3] Process ID: 131596 [Epoch: 83,   512/ 591 points] total loss per batch: 1.235
Policy (actual, predicted): 1 6
Policy data: tensor([0.1407, 0.1512, 0.1454, 0.1430, 0.1372, 0.1360, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1401, 0.1554, 0.1346, 0.1480, 0.1329, 0.1334, 0.1556],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999743700027466
 
[Iteration 3] Process ID: 131596 [Epoch: 84,   128/ 591 points] total loss per batch: 1.152
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 2.3479e-07, 6.9915e-09, 3.0720e-19, 3.1457e-16, 1.7056e-09,
        1.1677e-18], device='cuda:0')
Policy pred: tensor([9.9950e-01, 1.4962e-04, 5.0401e-06, 9.5125e-06, 2.7969e-04, 5.9032e-06,
        4.6565e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 84,   256/ 591 points] total loss per batch: 1.365
Policy (actual, predicted): 0 0
Policy data: tensor([0.8966, 0.0098, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.6534, 0.0471, 0.0523, 0.0485, 0.0819, 0.0487, 0.0680],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.05413837358355522
 
[Iteration 3] Process ID: 131596 [Epoch: 84,   384/ 591 points] total loss per batch: 1.235
Policy (actual, predicted): 1 6
Policy data: tensor([0.1395, 0.1489, 0.1419, 0.1465, 0.1407, 0.1395, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1456, 0.1529, 0.1376, 0.1302, 0.1420, 0.1291, 0.1626],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999894499778748
 
[Iteration 3] Process ID: 131596 [Epoch: 84,   512/ 591 points] total loss per batch: 1.321
Policy (actual, predicted): 3 3
Policy data: tensor([0.0665, 0.0665, 0.0879, 0.5117, 0.0963, 0.0452, 0.1260],
       device='cuda:0')
Policy pred: tensor([0.0728, 0.0698, 0.0906, 0.4816, 0.0936, 0.0537, 0.1379],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999985098838806
 
[Iteration 3] Process ID: 131596 [Epoch: 85,   128/ 591 points] total loss per batch: 1.345
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000e+00, 0.0000e+00, 2.8111e-08, 9.8017e-19, 3.0184e-09, 1.0037e-15,
        1.0000e+00], device='cuda:0')
Policy pred: tensor([3.1376e-07, 3.7054e-06, 8.8349e-04, 6.0876e-06, 2.6195e-06, 6.5325e-06,
        9.9910e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999986886978149
 
[Iteration 3] Process ID: 131596 [Epoch: 85,   256/ 591 points] total loss per batch: 1.167
Policy (actual, predicted): 6 6
Policy data: tensor([0.1360, 0.1465, 0.1336, 0.1489, 0.1442, 0.1407, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1161, 0.1170, 0.1105, 0.1101, 0.1130, 0.1085, 0.3249],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.172196626663208
 
[Iteration 3] Process ID: 131596 [Epoch: 85,   384/ 591 points] total loss per batch: 1.260
Policy (actual, predicted): 6 6
Policy data: tensor([0.1291, 0.1977, 0.0124, 0.0124, 0.0124, 0.0619, 0.5741],
       device='cuda:0')
Policy pred: tensor([0.1237, 0.2372, 0.0821, 0.0616, 0.0710, 0.0893, 0.3350],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.02064262516796589
 
[Iteration 3] Process ID: 131596 [Epoch: 85,   512/ 591 points] total loss per batch: 1.312
Policy (actual, predicted): 6 5
Policy data: tensor([0.1430, 0.1442, 0.1348, 0.1454, 0.1407, 0.1454, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1399, 0.1270, 0.1468, 0.1417, 0.1380, 0.1604, 0.1462],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999659657478333
 
[Iteration 3] Process ID: 131596 [Epoch: 86,   128/ 591 points] total loss per batch: 1.306
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.2865, 0.2094, 0.2376, 0.0823, 0.1842],
       device='cuda:0')
Policy pred: tensor([2.4182e-05, 2.3186e-05, 2.9345e-01, 2.0205e-01, 2.1889e-01, 8.7335e-02,
        1.9823e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999409914016724
 
[Iteration 3] Process ID: 131596 [Epoch: 86,   256/ 591 points] total loss per batch: 1.264
Policy (actual, predicted): 0 0
Policy data: tensor([7.4563e-01, 4.7480e-04, 2.5335e-01, 7.2741e-10, 6.9555e-05, 3.4502e-07,
        4.7480e-04], device='cuda:0')
Policy pred: tensor([7.9645e-01, 1.9589e-03, 1.9857e-01, 4.0550e-04, 1.4445e-03, 4.1636e-04,
        7.5271e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999995231628418
 
[Iteration 3] Process ID: 131596 [Epoch: 86,   384/ 591 points] total loss per batch: 1.229
Policy (actual, predicted): 6 1
Policy data: tensor([0.1419, 0.1489, 0.1395, 0.1419, 0.1383, 0.1383, 0.1512],
       device='cuda:0')
Policy pred: tensor([0.1357, 0.1557, 0.1528, 0.1454, 0.1314, 0.1337, 0.1453],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999954700469971
 
[Iteration 3] Process ID: 131596 [Epoch: 86,   512/ 591 points] total loss per batch: 1.211
Policy (actual, predicted): 0 0
Policy data: tensor([0.6244, 0.1329, 0.1033, 0.0218, 0.0187, 0.0466, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.6457, 0.1308, 0.0962, 0.0199, 0.0170, 0.0436, 0.0468],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.004625291097909212
 
[Iteration 3] Process ID: 131596 [Epoch: 87,   128/ 591 points] total loss per batch: 1.307
Policy (actual, predicted): 1 1
Policy data: tensor([0.1752, 0.2272, 0.0522, 0.1020, 0.1604, 0.1226, 0.1604],
       device='cuda:0')
Policy pred: tensor([0.1705, 0.2280, 0.0516, 0.0922, 0.1773, 0.1202, 0.1602],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9994230270385742
 
[Iteration 3] Process ID: 131596 [Epoch: 87,   256/ 591 points] total loss per batch: 1.311
Policy (actual, predicted): 6 6
Policy data: tensor([0.2673, 0.0000, 0.1837, 0.0324, 0.0000, 0.1958, 0.3208],
       device='cuda:0')
Policy pred: tensor([2.5482e-01, 1.7440e-04, 1.9835e-01, 3.6371e-02, 2.4510e-05, 1.9695e-01,
        3.1331e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 87,   384/ 591 points] total loss per batch: 1.227
Policy (actual, predicted): 1 1
Policy data: tensor([3.5696e-07, 1.0000e+00, 4.3223e-10, 5.2102e-13, 3.5401e-09, 1.2888e-08,
        1.2750e-07], device='cuda:0')
Policy pred: tensor([6.2834e-04, 9.9896e-01, 2.5050e-04, 2.1767e-05, 5.2969e-06, 1.1267e-04,
        2.4027e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 87,   512/ 591 points] total loss per batch: 1.328
Policy (actual, predicted): 0 0
Policy data: tensor([0.9052, 0.0150, 0.0234, 0.0062, 0.0184, 0.0150, 0.0167],
       device='cuda:0')
Policy pred: tensor([0.9186, 0.0115, 0.0235, 0.0056, 0.0176, 0.0106, 0.0126],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9981574416160583
 
[Iteration 3] Process ID: 131596 [Epoch: 88,   128/ 591 points] total loss per batch: 1.270
Policy (actual, predicted): 2 2
Policy data: tensor([0.0215, 0.0263, 0.8668, 0.0114, 0.0311, 0.0182, 0.0247],
       device='cuda:0')
Policy pred: tensor([0.0185, 0.0216, 0.8989, 0.0071, 0.0220, 0.0132, 0.0188],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.2676766514778137
 
[Iteration 3] Process ID: 131596 [Epoch: 88,   256/ 591 points] total loss per batch: 1.311
Policy (actual, predicted): 5 5
Policy data: tensor([0.0044, 0.0044, 0.0044, 0.0118, 0.0063, 0.9644, 0.0044],
       device='cuda:0')
Policy pred: tensor([0.0020, 0.0016, 0.0029, 0.0070, 0.0030, 0.9814, 0.0021],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999962449073792
 
[Iteration 3] Process ID: 131596 [Epoch: 88,   384/ 591 points] total loss per batch: 1.140
Policy (actual, predicted): 3 6
Policy data: tensor([0.1430, 0.1465, 0.1372, 0.1500, 0.1407, 0.1348, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1570, 0.1452, 0.1308, 0.1370, 0.1336, 0.1374, 0.1590],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9931483268737793
 
[Iteration 3] Process ID: 131596 [Epoch: 88,   512/ 591 points] total loss per batch: 1.346
Policy (actual, predicted): 6 6
Policy data: tensor([0.1419, 0.1442, 0.1372, 0.1465, 0.1372, 0.1395, 0.1535],
       device='cuda:0')
Policy pred: tensor([0.1194, 0.1336, 0.1200, 0.1216, 0.1305, 0.1176, 0.2573],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.17100116610527039
 
[Iteration 3] Process ID: 131596 [Epoch: 89,   128/ 591 points] total loss per batch: 1.244
Policy (actual, predicted): 6 6
Policy data: tensor([1.2919e-02, 0.0000e+00, 4.8996e-04, 2.4311e-01, 1.7533e-02, 2.5643e-06,
        7.2595e-01], device='cuda:0')
Policy pred: tensor([9.0992e-03, 8.9462e-06, 6.4077e-04, 2.5733e-01, 1.7366e-02, 2.1867e-04,
        7.1533e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998213648796082
 
[Iteration 3] Process ID: 131596 [Epoch: 89,   256/ 591 points] total loss per batch: 1.224
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 4.6359e-12, 0.0000e+00, 1.6805e-17, 0.0000e+00, 3.7417e-17,
        6.7988e-07], device='cuda:0')
Policy pred: tensor([9.9990e-01, 8.1072e-05, 1.0219e-07, 3.2897e-06, 1.4870e-08, 4.1703e-07,
        1.6570e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 89,   384/ 591 points] total loss per batch: 1.179
Policy (actual, predicted): 6 6
Policy data: tensor([0.1291, 0.1977, 0.0124, 0.0124, 0.0124, 0.0619, 0.5741],
       device='cuda:0')
Policy pred: tensor([0.1336, 0.1879, 0.0895, 0.0795, 0.0838, 0.1012, 0.3245],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.04920843616127968
 
[Iteration 3] Process ID: 131596 [Epoch: 89,   512/ 591 points] total loss per batch: 1.246
Policy (actual, predicted): 4 6
Policy data: tensor([0.1501, 0.1466, 0.1442, 0.1254, 0.1512, 0.1348, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1505, 0.1522, 0.1387, 0.1181, 0.1490, 0.1371, 0.1544],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999977350234985
 
[Iteration 3] Process ID: 131596 [Epoch: 90,   128/ 591 points] total loss per batch: 1.310
Policy (actual, predicted): 4 0
Policy data: tensor([0.1501, 0.1477, 0.1466, 0.1265, 0.1524, 0.1372, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1560, 0.1385, 0.1460, 0.1237, 0.1508, 0.1351, 0.1499],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999998807907104
 
[Iteration 3] Process ID: 131596 [Epoch: 90,   256/ 591 points] total loss per batch: 1.208
Policy (actual, predicted): 1 1
Policy data: tensor([0.0217, 0.8925, 0.0167, 0.0061, 0.0167, 0.0150, 0.0313],
       device='cuda:0')
Policy pred: tensor([0.0173, 0.9092, 0.0127, 0.0048, 0.0135, 0.0117, 0.0309],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9992433786392212
 
[Iteration 3] Process ID: 131596 [Epoch: 90,   384/ 591 points] total loss per batch: 1.284
Policy (actual, predicted): 6 3
Policy data: tensor([0.1419, 0.1477, 0.1360, 0.1430, 0.1430, 0.1383, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1344, 0.1468, 0.1259, 0.1631, 0.1571, 0.1383, 0.1343],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9905019998550415
 
[Iteration 3] Process ID: 131596 [Epoch: 90,   512/ 591 points] total loss per batch: 1.285
Policy (actual, predicted): 1 1
Policy data: tensor([0.0934, 0.6501, 0.1337, 0.0188, 0.0173, 0.0412, 0.0455],
       device='cuda:0')
Policy pred: tensor([0.1146, 0.3955, 0.1403, 0.0780, 0.0858, 0.0922, 0.0937],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.00017040595412254333
 
[Iteration 3] Process ID: 131596 [Epoch: 91,   128/ 591 points] total loss per batch: 1.206
Policy (actual, predicted): 2 2
Policy data: tensor([0.0484, 0.0335, 0.7996, 0.0304, 0.0320, 0.0320, 0.0242],
       device='cuda:0')
Policy pred: tensor([0.0388, 0.0365, 0.8108, 0.0232, 0.0279, 0.0336, 0.0292],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9986599683761597
 
[Iteration 3] Process ID: 131596 [Epoch: 91,   256/ 591 points] total loss per batch: 1.220
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0070, 0.0119, 0.0210, 0.0080, 0.0243, 0.0177, 0.9101],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.33005571365356445
 
[Iteration 3] Process ID: 131596 [Epoch: 91,   384/ 591 points] total loss per batch: 1.283
Policy (actual, predicted): 2 2
Policy data: tensor([0.2808, 0.0266, 0.5874, 0.0173, 0.0220, 0.0471, 0.0189],
       device='cuda:0')
Policy pred: tensor([0.2840, 0.0199, 0.5950, 0.0168, 0.0208, 0.0474, 0.0162],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9968269467353821
 
[Iteration 3] Process ID: 131596 [Epoch: 91,   512/ 591 points] total loss per batch: 1.275
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 1.3434e-10, 0.0000e+00, 1.4982e-09, 1.0000e+00, 1.8493e-18,
        1.1451e-07], device='cuda:0')
Policy pred: tensor([3.6522e-07, 1.1367e-05, 1.7561e-05, 1.2710e-04, 9.9963e-01, 3.7485e-06,
        2.1282e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999966025352478
 
[Iteration 3] Process ID: 131596 [Epoch: 92,   128/ 591 points] total loss per batch: 1.283
Policy (actual, predicted): 0 0
Policy data: tensor([0.3448, 0.1150, 0.1150, 0.0548, 0.2934, 0.0273, 0.0498],
       device='cuda:0')
Policy pred: tensor([0.3541, 0.1186, 0.1268, 0.0534, 0.2784, 0.0265, 0.0422],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999991059303284
 
[Iteration 3] Process ID: 131596 [Epoch: 92,   256/ 591 points] total loss per batch: 1.309
Policy (actual, predicted): 0 0
Policy data: tensor([0.2507, 0.1776, 0.1360, 0.1360, 0.0780, 0.0858, 0.1360],
       device='cuda:0')
Policy pred: tensor([0.2688, 0.1584, 0.1416, 0.1296, 0.0776, 0.0909, 0.1331],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999598860740662
 
[Iteration 3] Process ID: 131596 [Epoch: 92,   384/ 591 points] total loss per batch: 1.172
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 0.0000e+00, 0.0000e+00, 1.1499e-11, 1.7638e-13, 1.1775e-08,
        4.1179e-07], device='cuda:0')
Policy pred: tensor([9.9890e-01, 2.8425e-08, 8.9071e-07, 9.1309e-04, 1.5277e-05, 7.0558e-05,
        1.0034e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999985098838806
 
[Iteration 3] Process ID: 131596 [Epoch: 92,   512/ 591 points] total loss per batch: 1.285
Policy (actual, predicted): 3 6
Policy data: tensor([0.1176, 0.1539, 0.0501, 0.2581, 0.1287, 0.0738, 0.2179],
       device='cuda:0')
Policy pred: tensor([0.1282, 0.1600, 0.0554, 0.1988, 0.1425, 0.0807, 0.2344],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 93,   128/ 591 points] total loss per batch: 1.255
Policy (actual, predicted): 0 0
Policy data: tensor([0.3448, 0.1150, 0.1150, 0.0548, 0.2934, 0.0273, 0.0498],
       device='cuda:0')
Policy pred: tensor([0.3318, 0.1209, 0.1253, 0.0573, 0.2894, 0.0243, 0.0510],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999977946281433
 
[Iteration 3] Process ID: 131596 [Epoch: 93,   256/ 591 points] total loss per batch: 1.293
Policy (actual, predicted): 2 2
Policy data: tensor([0.0217, 0.1514, 0.4414, 0.0057, 0.3471, 0.0156, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.0219, 0.1558, 0.5058, 0.0067, 0.2798, 0.0145, 0.0155],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.5397282838821411
 
[Iteration 3] Process ID: 131596 [Epoch: 93,   384/ 591 points] total loss per batch: 1.230
Policy (actual, predicted): 0 0
Policy data: tensor([0.8966, 0.0098, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.7631, 0.0332, 0.0357, 0.0307, 0.0576, 0.0340, 0.0457],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.059027720242738724
 
[Iteration 3] Process ID: 131596 [Epoch: 93,   512/ 591 points] total loss per batch: 1.244
Policy (actual, predicted): 4 0
Policy data: tensor([0.1348, 0.1442, 0.1430, 0.1442, 0.1489, 0.1395, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.8082, 0.0278, 0.0301, 0.0252, 0.0453, 0.0280, 0.0354],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.05607803165912628
 
[Iteration 3] Process ID: 131596 [Epoch: 94,   128/ 591 points] total loss per batch: 1.288
Policy (actual, predicted): 2 2
Policy data: tensor([5.5501e-20, 5.0910e-11, 1.0000e+00, 1.8023e-19, 1.3407e-18, 5.4200e-23,
        1.4601e-20], device='cuda:0')
Policy pred: tensor([5.1746e-05, 3.4915e-04, 9.9874e-01, 2.9868e-04, 3.1453e-04, 1.7415e-05,
        2.3049e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999493956565857
 
[Iteration 3] Process ID: 131596 [Epoch: 94,   256/ 591 points] total loss per batch: 1.329
Policy (actual, predicted): 3 3
Policy data: tensor([1.3693e-09, 8.0170e-06, 4.9153e-13, 9.9092e-01, 9.0683e-03, 2.2646e-17,
        1.2439e-06], device='cuda:0')
Policy pred: tensor([6.2035e-05, 2.0702e-03, 1.1389e-03, 9.7394e-01, 2.0329e-02, 1.3607e-04,
        2.3285e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999936819076538
 
[Iteration 3] Process ID: 131596 [Epoch: 94,   384/ 591 points] total loss per batch: 1.271
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0072, 0.0139, 0.0208, 0.0091, 0.0285, 0.0183, 0.9022],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.32402849197387695
 
[Iteration 3] Process ID: 131596 [Epoch: 94,   512/ 591 points] total loss per batch: 1.160
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000e+00, 0.0000e+00, 9.9718e-01, 1.1090e-08, 5.5423e-13, 2.8185e-03,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([1.5916e-09, 1.3390e-06, 9.9979e-01, 1.2023e-06, 1.8702e-07, 2.0993e-04,
        2.0564e-07], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999995231628418
 
[Iteration 3] Process ID: 131596 [Epoch: 95,   128/ 591 points] total loss per batch: 1.289
Policy (actual, predicted): 2 2
Policy data: tensor([0.0215, 0.0263, 0.8668, 0.0114, 0.0311, 0.0182, 0.0247],
       device='cuda:0')
Policy pred: tensor([0.0250, 0.0278, 0.8529, 0.0109, 0.0336, 0.0195, 0.0301],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.27490854263305664
 
[Iteration 3] Process ID: 131596 [Epoch: 95,   256/ 591 points] total loss per batch: 1.172
Policy (actual, predicted): 1 1
Policy data: tensor([0.1407, 0.1512, 0.1454, 0.1430, 0.1372, 0.1360, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1472, 0.1571, 0.1365, 0.1468, 0.1323, 0.1252, 0.1549],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999798536300659
 
[Iteration 3] Process ID: 131596 [Epoch: 95,   384/ 591 points] total loss per batch: 1.297
Policy (actual, predicted): 1 2
Policy data: tensor([0.1407, 0.1465, 0.1465, 0.1407, 0.1442, 0.1372, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1500, 0.1495, 0.1511, 0.1321, 0.1455, 0.1248, 0.1469],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999882578849792
 
[Iteration 3] Process ID: 131596 [Epoch: 95,   512/ 591 points] total loss per batch: 1.273
Policy (actual, predicted): 4 4
Policy data: tensor([4.3964e-19, 1.6074e-13, 2.6583e-21, 1.5329e-19, 1.0000e+00, 1.2717e-17,
        4.8339e-17], device='cuda:0')
Policy pred: tensor([1.8221e-06, 1.1511e-04, 3.3741e-05, 7.5617e-04, 9.9875e-01, 9.6585e-06,
        3.3343e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 96,   128/ 591 points] total loss per batch: 1.259
Policy (actual, predicted): 4 6
Policy data: tensor([0.1419, 0.1465, 0.1419, 0.1383, 0.1489, 0.1372, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1144, 0.1178, 0.1184, 0.1162, 0.1194, 0.1086, 0.3052],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.4587825536727905
 
[Iteration 3] Process ID: 131596 [Epoch: 96,   256/ 591 points] total loss per batch: 1.285
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1228, 0.4054, 0.1469, 0.0721, 0.0810, 0.0857, 0.0860],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.009244059212505817
 
[Iteration 3] Process ID: 131596 [Epoch: 96,   384/ 591 points] total loss per batch: 1.403
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 1.3434e-10, 0.0000e+00, 1.4982e-09, 1.0000e+00, 1.8493e-18,
        1.1451e-07], device='cuda:0')
Policy pred: tensor([3.6275e-07, 9.0541e-06, 5.3400e-06, 1.1887e-04, 9.9963e-01, 5.1320e-06,
        2.3518e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999986886978149
 
[Iteration 3] Process ID: 131596 [Epoch: 96,   512/ 591 points] total loss per batch: 1.107
Policy (actual, predicted): 1 1
Policy data: tensor([0.1479, 0.7307, 0.0150, 0.0030, 0.0042, 0.0772, 0.0221],
       device='cuda:0')
Policy pred: tensor([0.1468, 0.7243, 0.0150, 0.0041, 0.0066, 0.0777, 0.0254],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9995338916778564
 
[Iteration 3] Process ID: 131596 [Epoch: 97,   128/ 591 points] total loss per batch: 1.204
Policy (actual, predicted): 0 0
Policy data: tensor([0.2507, 0.1776, 0.1360, 0.1360, 0.0780, 0.0858, 0.1360],
       device='cuda:0')
Policy pred: tensor([0.2594, 0.1783, 0.1360, 0.1364, 0.0747, 0.0817, 0.1333],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999770522117615
 
[Iteration 3] Process ID: 131596 [Epoch: 97,   256/ 591 points] total loss per batch: 1.260
Policy (actual, predicted): 0 0
Policy data: tensor([0.3483, 0.0502, 0.0622, 0.1521, 0.0596, 0.0476, 0.2800],
       device='cuda:0')
Policy pred: tensor([0.3387, 0.0375, 0.0588, 0.1381, 0.0591, 0.0491, 0.3187],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9984307885169983
 
[Iteration 3] Process ID: 131596 [Epoch: 97,   384/ 591 points] total loss per batch: 1.333
Policy (actual, predicted): 6 6
Policy data: tensor([0.1583, 0.1887, 0.1448, 0.1210, 0.1210, 0.0422, 0.2242],
       device='cuda:0')
Policy pred: tensor([0.1608, 0.2068, 0.1404, 0.1147, 0.1185, 0.0439, 0.2149],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 97,   512/ 591 points] total loss per batch: 1.290
Policy (actual, predicted): 4 4
Policy data: tensor([0.1477, 0.1477, 0.1477, 0.1265, 0.1524, 0.1348, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1512, 0.1441, 0.1454, 0.1340, 0.1517, 0.1304, 0.1432],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.009725294075906277
 
[Iteration 3] Process ID: 131596 [Epoch: 98,   128/ 591 points] total loss per batch: 1.233
Policy (actual, predicted): 2 2
Policy data: tensor([0.1465, 0.1372, 0.1489, 0.1348, 0.1454, 0.1477, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1368, 0.1358, 0.1654, 0.1337, 0.1438, 0.1403, 0.1443],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999994695186615
 
[Iteration 3] Process ID: 131596 [Epoch: 98,   256/ 591 points] total loss per batch: 1.259
Policy (actual, predicted): 2 2
Policy data: tensor([0.1917, 0.1435, 0.5590, 0.0247, 0.0232, 0.0407, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.2240, 0.1483, 0.5199, 0.0224, 0.0248, 0.0393, 0.0212],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9927772283554077
 
[Iteration 3] Process ID: 131596 [Epoch: 98,   384/ 591 points] total loss per batch: 1.205
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000e+00, 5.4318e-05, 9.9995e-01, 1.2911e-19, 4.9078e-19, 4.9078e-19,
        4.9078e-19], device='cuda:0')
Policy pred: tensor([4.6319e-08, 1.1543e-04, 9.9969e-01, 5.4342e-07, 2.0910e-05, 1.6046e-06,
        1.7400e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999979734420776
 
[Iteration 3] Process ID: 131596 [Epoch: 98,   512/ 591 points] total loss per batch: 1.381
Policy (actual, predicted): 2 2
Policy data: tensor([0.0081, 0.0135, 0.9478, 0.0063, 0.0063, 0.0118, 0.0063],
       device='cuda:0')
Policy pred: tensor([0.0064, 0.0061, 0.9671, 0.0031, 0.0047, 0.0087, 0.0039],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998537302017212
 
[Iteration 3] Process ID: 131596 [Epoch: 99,   128/ 591 points] total loss per batch: 1.315
Policy (actual, predicted): 1 0
Policy data: tensor([0.1372, 0.1477, 0.1454, 0.1419, 0.1442, 0.1360, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.7187, 0.0394, 0.0447, 0.0377, 0.0674, 0.0397, 0.0525],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.040856219828128815
 
[Iteration 3] Process ID: 131596 [Epoch: 99,   256/ 591 points] total loss per batch: 1.218
Policy (actual, predicted): 6 6
Policy data: tensor([0.1583, 0.1887, 0.1448, 0.1210, 0.1210, 0.0422, 0.2242],
       device='cuda:0')
Policy pred: tensor([0.1564, 0.1844, 0.1410, 0.1199, 0.1223, 0.0436, 0.2324],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 99,   384/ 591 points] total loss per batch: 1.303
Policy (actual, predicted): 2 2
Policy data: tensor([1.3690e-21, 6.3954e-21, 1.0000e+00, 3.1212e-18, 2.3740e-23, 1.3690e-21,
        2.4310e-20], device='cuda:0')
Policy pred: tensor([3.0349e-04, 6.3581e-05, 9.9944e-01, 1.4600e-05, 9.2648e-05, 2.5422e-05,
        6.1290e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998987913131714
 
[Iteration 3] Process ID: 131596 [Epoch: 99,   512/ 591 points] total loss per batch: 1.186
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 1.4355e-15, 1.8845e-19, 7.8128e-12, 1.5633e-17,
        5.8033e-20], device='cuda:0')
Policy pred: tensor([3.3701e-07, 9.9953e-01, 3.2127e-05, 5.0854e-07, 3.0943e-04, 8.8208e-07,
        1.2624e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999887347221375
 
[Iteration 3] Process ID: 131596 [Epoch: 100,   128/ 591 points] total loss per batch: 1.312
Policy (actual, predicted): 4 4
Policy data: tensor([0.1453, 0.0850, 0.0643, 0.0156, 0.4241, 0.0776, 0.1880],
       device='cuda:0')
Policy pred: tensor([0.1780, 0.0835, 0.0615, 0.0170, 0.3875, 0.0722, 0.2004],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 100,   256/ 591 points] total loss per batch: 1.340
Policy (actual, predicted): 4 4
Policy data: tensor([0.0113, 0.0096, 0.0444, 0.0078, 0.8089, 0.0180, 0.1000],
       device='cuda:0')
Policy pred: tensor([0.0128, 0.0103, 0.0530, 0.0073, 0.8046, 0.0192, 0.0927],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.984916090965271
 
[Iteration 3] Process ID: 131596 [Epoch: 100,   384/ 591 points] total loss per batch: 1.115
Policy (actual, predicted): 3 3
Policy data: tensor([0.0665, 0.0665, 0.0879, 0.5117, 0.0963, 0.0452, 0.1260],
       device='cuda:0')
Policy pred: tensor([0.0686, 0.0662, 0.0969, 0.4805, 0.1057, 0.0501, 0.1320],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999915957450867
 
[Iteration 3] Process ID: 131596 [Epoch: 100,   512/ 591 points] total loss per batch: 1.303
Policy (actual, predicted): 4 4
Policy data: tensor([7.5326e-19, 5.8547e-17, 2.8361e-22, 3.0452e-23, 1.0000e+00, 2.9041e-19,
        1.6747e-17], device='cuda:0')
Policy pred: tensor([3.5726e-04, 2.5572e-04, 2.2570e-05, 2.5266e-05, 9.9926e-01, 6.3452e-06,
        6.8923e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 101,   128/ 591 points] total loss per batch: 1.249
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9998e-01, 0.0000e+00, 2.0791e-05,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([1.5580e-06, 3.6716e-07, 6.6482e-06, 9.9974e-01, 2.9030e-05, 1.7696e-04,
        4.5631e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999905228614807
 
[Iteration 3] Process ID: 131596 [Epoch: 101,   256/ 591 points] total loss per batch: 1.413
Policy (actual, predicted): 2 2
Policy data: tensor([0.0217, 0.1514, 0.4403, 0.0057, 0.3482, 0.0156, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.0193, 0.1488, 0.5118, 0.0058, 0.2852, 0.0122, 0.0169],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.5820902585983276
 
[Iteration 3] Process ID: 131596 [Epoch: 101,   384/ 591 points] total loss per batch: 1.313
Policy (actual, predicted): 1 1
Policy data: tensor([0.4448, 0.4653, 0.0267, 0.0109, 0.0142, 0.0190, 0.0190],
       device='cuda:0')
Policy pred: tensor([0.4012, 0.4955, 0.0299, 0.0139, 0.0169, 0.0225, 0.0201],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9911129474639893
 
[Iteration 3] Process ID: 131596 [Epoch: 101,   512/ 591 points] total loss per batch: 1.164
Policy (actual, predicted): 6 6
Policy data: tensor([0.1145, 0.0262, 0.0247, 0.0217, 0.2126, 0.0364, 0.5639],
       device='cuda:0')
Policy pred: tensor([0.1084, 0.0338, 0.0309, 0.0201, 0.2271, 0.0395, 0.5402],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9990420341491699
 
[Iteration 3] Process ID: 131596 [Epoch: 102,   128/ 591 points] total loss per batch: 1.258
Policy (actual, predicted): 2 2
Policy data: tensor([0.2505, 0.0671, 0.2990, 0.2149, 0.0902, 0.0433, 0.0351],
       device='cuda:0')
Policy pred: tensor([0.2675, 0.0655, 0.2867, 0.2058, 0.0929, 0.0440, 0.0377],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9952938556671143
 
[Iteration 3] Process ID: 131596 [Epoch: 102,   256/ 591 points] total loss per batch: 1.289
Policy (actual, predicted): 4 4
Policy data: tensor([0.0715, 0.0958, 0.0503, 0.0517, 0.3876, 0.2703, 0.0728],
       device='cuda:0')
Policy pred: tensor([0.1045, 0.1258, 0.0889, 0.1048, 0.2568, 0.2051, 0.1140],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 102,   384/ 591 points] total loss per batch: 1.294
Policy (actual, predicted): 1 1
Policy data: tensor([2.9877e-02, 7.9537e-01, 3.2440e-02, 3.2749e-03, 1.1032e-03, 4.6567e-07,
        1.3793e-01], device='cuda:0')
Policy pred: tensor([0.0222, 0.8201, 0.0307, 0.0043, 0.0034, 0.0009, 0.1185],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999995827674866
 
[Iteration 3] Process ID: 131596 [Epoch: 102,   512/ 591 points] total loss per batch: 1.219
Policy (actual, predicted): 2 1
Policy data: tensor([0.1489, 0.1477, 0.1524, 0.1325, 0.1489, 0.1301, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1147, 0.4252, 0.1398, 0.0700, 0.0783, 0.0845, 0.0875],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.000592268945183605
 
[Iteration 3] Process ID: 131596 [Epoch: 103,   128/ 591 points] total loss per batch: 1.328
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1150, 0.4175, 0.1450, 0.0712, 0.0803, 0.0838, 0.0871],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.0009066050406545401
 
[Iteration 3] Process ID: 131596 [Epoch: 103,   256/ 591 points] total loss per batch: 1.251
Policy (actual, predicted): 3 1
Policy data: tensor([0.1348, 0.1430, 0.1454, 0.1489, 0.1442, 0.1348, 0.1489],
       device='cuda:0')
Policy pred: tensor([0.1298, 0.1599, 0.1385, 0.1539, 0.1377, 0.1360, 0.1443],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9997725486755371
 
[Iteration 3] Process ID: 131596 [Epoch: 103,   384/ 591 points] total loss per batch: 1.320
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 4.8059e-10, 1.6616e-09, 2.7480e-17, 1.0000e+00, 5.0091e-09,
        4.7592e-06], device='cuda:0')
Policy pred: tensor([2.1456e-09, 8.5355e-04, 3.0910e-06, 4.4144e-07, 9.9884e-01, 5.1230e-08,
        2.9979e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 103,   512/ 591 points] total loss per batch: 1.158
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 1.4355e-15, 1.8845e-19, 7.8128e-12, 1.5633e-17,
        5.8033e-20], device='cuda:0')
Policy pred: tensor([1.1174e-08, 9.9998e-01, 1.2727e-06, 8.1281e-09, 7.6397e-06, 8.8872e-09,
        8.8977e-06], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999964237213135
 
[Iteration 3] Process ID: 131596 [Epoch: 104,   128/ 591 points] total loss per batch: 1.266
Policy (actual, predicted): 5 5
Policy data: tensor([0.0044, 0.0044, 0.0063, 0.0044, 0.0044, 0.9699, 0.0063],
       device='cuda:0')
Policy pred: tensor([0.0019, 0.0053, 0.0060, 0.0039, 0.0031, 0.9728, 0.0069],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9875451326370239
 
[Iteration 3] Process ID: 131596 [Epoch: 104,   256/ 591 points] total loss per batch: 1.293
Policy (actual, predicted): 1 5
Policy data: tensor([0.1501, 0.1512, 0.1466, 0.1277, 0.1454, 0.1372, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1449, 0.1494, 0.1460, 0.1272, 0.1412, 0.1512, 0.1401],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 104,   384/ 591 points] total loss per batch: 1.272
Policy (actual, predicted): 2 2
Policy data: tensor([2.6643e-15, 4.3561e-01, 5.6439e-01, 6.7388e-17, 1.1145e-14, 1.1412e-11,
        5.5903e-15], device='cuda:0')
Policy pred: tensor([5.3416e-04, 4.9230e-01, 5.0410e-01, 3.8685e-04, 1.3938e-03, 5.8665e-04,
        7.0465e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999990701675415
 
[Iteration 3] Process ID: 131596 [Epoch: 104,   512/ 591 points] total loss per batch: 1.150
Policy (actual, predicted): 1 1
Policy data: tensor([5.0161e-11, 1.0000e+00, 4.6716e-20, 9.7656e-24, 1.3555e-13, 7.1247e-11,
        2.7585e-15], device='cuda:0')
Policy pred: tensor([4.2446e-04, 9.9916e-01, 5.9807e-05, 2.4919e-05, 7.0664e-05, 1.0085e-05,
        2.5420e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999939799308777
 
[Iteration 3] Process ID: 131596 [Epoch: 105,   128/ 591 points] total loss per batch: 1.237
Policy (actual, predicted): 0 0
Policy data: tensor([0.6244, 0.1329, 0.1020, 0.0218, 0.0187, 0.0480, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.6180, 0.1335, 0.1087, 0.0223, 0.0191, 0.0494, 0.0490],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.003711005439981818
 
[Iteration 3] Process ID: 131596 [Epoch: 105,   256/ 591 points] total loss per batch: 1.324
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000e+00, 0.0000e+00, 6.1127e-09, 5.5685e-16, 3.7121e-08, 1.0000e+00,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([2.1132e-06, 1.2648e-06, 1.7069e-04, 7.0790e-06, 4.4150e-07, 9.9981e-01,
        1.1943e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999986290931702
 
[Iteration 3] Process ID: 131596 [Epoch: 105,   384/ 591 points] total loss per batch: 1.219
Policy (actual, predicted): 1 1
Policy data: tensor([0.0149, 0.3927, 0.1064, 0.1443, 0.0000, 0.0436, 0.2980],
       device='cuda:0')
Policy pred: tensor([1.4635e-02, 3.9853e-01, 1.0731e-01, 1.3196e-01, 1.0591e-05, 3.8528e-02,
        3.0903e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999996423721313
 
[Iteration 3] Process ID: 131596 [Epoch: 105,   512/ 591 points] total loss per batch: 1.248
Policy (actual, predicted): 3 3
Policy data: tensor([0.0665, 0.0665, 0.0879, 0.5117, 0.0963, 0.0452, 0.1260],
       device='cuda:0')
Policy pred: tensor([0.0653, 0.0668, 0.0859, 0.5196, 0.0976, 0.0473, 0.1174],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999994695186615
 
[Iteration 3] Process ID: 131596 [Epoch: 106,   128/ 591 points] total loss per batch: 1.275
Policy (actual, predicted): 0 0
Policy data: tensor([4.7743e-01, 0.0000e+00, 5.1976e-02, 1.9841e-05, 4.5169e-01, 5.4369e-05,
        1.8829e-02], device='cuda:0')
Policy pred: tensor([4.9796e-01, 5.5303e-06, 4.8931e-02, 6.9330e-04, 4.3600e-01, 5.6017e-04,
        1.5846e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 106,   256/ 591 points] total loss per batch: 1.260
Policy (actual, predicted): 1 2
Policy data: tensor([0.1454, 0.1500, 0.1430, 0.1348, 0.1477, 0.1336, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1461, 0.1547, 0.1607, 0.1344, 0.1433, 0.1243, 0.1365],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999980926513672
 
[Iteration 3] Process ID: 131596 [Epoch: 106,   384/ 591 points] total loss per batch: 1.259
Policy (actual, predicted): 0 0
Policy data: tensor([0.6245, 0.1342, 0.1020, 0.0218, 0.0187, 0.0466, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.6514, 0.1277, 0.0973, 0.0185, 0.0158, 0.0428, 0.0465],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.008769790641963482
 
[Iteration 3] Process ID: 131596 [Epoch: 106,   512/ 591 points] total loss per batch: 1.219
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 3.2557e-17, 0.0000e+00, 1.0000e+00, 1.2720e-08, 3.2557e-17,
        1.7303e-09], device='cuda:0')
Policy pred: tensor([6.5200e-06, 2.8373e-04, 7.5109e-06, 9.9893e-01, 5.1087e-04, 7.2406e-05,
        1.8970e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998210668563843
 
[Iteration 3] Process ID: 131596 [Epoch: 107,   128/ 591 points] total loss per batch: 1.309
Policy (actual, predicted): 1 1
Policy data: tensor([0.0650, 0.3209, 0.0485, 0.1777, 0.1139, 0.1247, 0.1491],
       device='cuda:0')
Policy pred: tensor([0.0624, 0.3235, 0.0489, 0.1749, 0.1101, 0.1262, 0.1539],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999819397926331
 
[Iteration 3] Process ID: 131596 [Epoch: 107,   256/ 591 points] total loss per batch: 1.361
Policy (actual, predicted): 1 1
Policy data: tensor([0.1419, 0.1465, 0.1419, 0.1430, 0.1454, 0.1360, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1451, 0.1483, 0.1353, 0.1443, 0.1470, 0.1331, 0.1470],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998769760131836
 
[Iteration 3] Process ID: 131596 [Epoch: 107,   384/ 591 points] total loss per batch: 1.113
Policy (actual, predicted): 4 0
Policy data: tensor([0.1454, 0.1465, 0.1477, 0.1325, 0.1501, 0.1336, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1518, 0.1405, 0.1449, 0.1271, 0.1516, 0.1358, 0.1483],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.01435732189565897
 
[Iteration 3] Process ID: 131596 [Epoch: 107,   512/ 591 points] total loss per batch: 1.209
Policy (actual, predicted): 4 4
Policy data: tensor([0.1533, 0.1533, 0.0975, 0.0550, 0.2571, 0.0668, 0.2170],
       device='cuda:0')
Policy pred: tensor([0.1455, 0.1471, 0.1133, 0.0633, 0.2471, 0.0700, 0.2138],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999973177909851
 
[Iteration 3] Process ID: 131596 [Epoch: 108,   128/ 591 points] total loss per batch: 1.250
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 1.6843e-07, 0.0000e+00, 2.6493e-09, 1.0000e+00, 1.3457e-18,
        2.4653e-07], device='cuda:0')
Policy pred: tensor([2.6155e-07, 3.2285e-06, 1.1855e-06, 2.0819e-04, 9.9964e-01, 2.9906e-06,
        1.4656e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999992847442627
 
[Iteration 3] Process ID: 131596 [Epoch: 108,   256/ 591 points] total loss per batch: 1.178
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0067, 0.0118, 0.0213, 0.0092, 0.0234, 0.0193, 0.9083],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.351186603307724
 
[Iteration 3] Process ID: 131596 [Epoch: 108,   384/ 591 points] total loss per batch: 1.306
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1177, 0.4384, 0.1390, 0.0658, 0.0763, 0.0817, 0.0811],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.003070892533287406
 
[Iteration 3] Process ID: 131596 [Epoch: 108,   512/ 591 points] total loss per batch: 1.226
Policy (actual, predicted): 2 2
Policy data: tensor([0.1977, 0.1435, 0.5532, 0.0247, 0.0232, 0.0407, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.2089, 0.1477, 0.5286, 0.0270, 0.0269, 0.0418, 0.0191],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9875398874282837
 
[Iteration 3] Process ID: 131596 [Epoch: 109,   128/ 591 points] total loss per batch: 1.219
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000e+00, 0.0000e+00, 9.4087e-02, 1.4492e-08, 7.1820e-06, 6.4460e-06,
        9.0590e-01], device='cuda:0')
Policy pred: tensor([4.3203e-07, 2.3531e-09, 5.7660e-02, 7.4530e-04, 4.2830e-05, 4.8932e-05,
        9.4150e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998891949653625
 
[Iteration 3] Process ID: 131596 [Epoch: 109,   256/ 591 points] total loss per batch: 1.204
Policy (actual, predicted): 4 1
Policy data: tensor([0.1466, 0.1466, 0.1489, 0.1336, 0.1524, 0.1325, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1172, 0.4215, 0.1422, 0.0691, 0.0775, 0.0825, 0.0899],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.0001451633870601654
 
[Iteration 3] Process ID: 131596 [Epoch: 109,   384/ 591 points] total loss per batch: 1.238
Policy (actual, predicted): 2 2
Policy data: tensor([0.0263, 0.0165, 0.8683, 0.0132, 0.0311, 0.0247, 0.0199],
       device='cuda:0')
Policy pred: tensor([0.0201, 0.0130, 0.9052, 0.0092, 0.0207, 0.0160, 0.0158],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9922403693199158
 
[Iteration 3] Process ID: 131596 [Epoch: 109,   512/ 591 points] total loss per batch: 1.400
Policy (actual, predicted): 5 5
Policy data: tensor([0.0764, 0.0659, 0.1061, 0.0524, 0.0855, 0.5439, 0.0698],
       device='cuda:0')
Policy pred: tensor([0.0755, 0.0654, 0.1176, 0.0489, 0.0936, 0.5316, 0.0675],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998874068260193
 
[Iteration 3] Process ID: 131596 [Epoch: 110,   128/ 591 points] total loss per batch: 1.269
Policy (actual, predicted): 3 3
Policy data: tensor([0.0738, 0.0904, 0.2061, 0.3947, 0.1288, 0.0712, 0.0351],
       device='cuda:0')
Policy pred: tensor([0.0787, 0.0822, 0.2047, 0.4062, 0.1213, 0.0697, 0.0372],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999916553497314
 
[Iteration 3] Process ID: 131596 [Epoch: 110,   256/ 591 points] total loss per batch: 1.150
Policy (actual, predicted): 3 3
Policy data: tensor([0.0729, 0.1156, 0.0729, 0.3226, 0.1056, 0.0368, 0.2738],
       device='cuda:0')
Policy pred: tensor([0.0750, 0.1086, 0.0733, 0.3533, 0.1010, 0.0353, 0.2535],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999992251396179
 
[Iteration 3] Process ID: 131596 [Epoch: 110,   384/ 591 points] total loss per batch: 1.311
Policy (actual, predicted): 4 4
Policy data: tensor([0.1043, 0.2590, 0.0589, 0.0541, 0.3447, 0.0000, 0.1791],
       device='cuda:0')
Policy pred: tensor([9.7924e-02, 2.5879e-01, 5.8261e-02, 5.2355e-02, 3.5712e-01, 2.7772e-04,
        1.7527e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999999463558197
 
[Iteration 3] Process ID: 131596 [Epoch: 110,   512/ 591 points] total loss per batch: 1.323
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 8.6767e-12, 3.5443e-14, 3.6294e-21, 2.0929e-19, 3.5443e-24,
        3.7165e-18], device='cuda:0')
Policy pred: tensor([9.9962e-01, 1.0883e-04, 6.3737e-07, 9.7248e-07, 4.5645e-06, 1.0631e-06,
        2.6282e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 111,   128/ 591 points] total loss per batch: 1.394
Policy (actual, predicted): 0 0
Policy data: tensor([0.6244, 0.1329, 0.1033, 0.0218, 0.0187, 0.0466, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.6002, 0.1303, 0.1039, 0.0275, 0.0226, 0.0559, 0.0596],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.0016256892122328281
 
[Iteration 3] Process ID: 131596 [Epoch: 111,   256/ 591 points] total loss per batch: 1.181
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000e+00, 0.0000e+00, 6.1127e-09, 5.5685e-16, 3.7121e-08, 1.0000e+00,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([6.0843e-06, 1.4642e-05, 6.6964e-04, 1.3771e-05, 7.5940e-06, 9.9915e-01,
        1.3479e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999980926513672
 
[Iteration 3] Process ID: 131596 [Epoch: 111,   384/ 591 points] total loss per batch: 1.262
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9961e-01, 0.0000e+00, 3.8781e-04,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([4.0340e-08, 2.6096e-08, 2.8598e-07, 9.9995e-01, 2.9136e-05, 2.8512e-06,
        1.3195e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999995231628418
 
[Iteration 3] Process ID: 131596 [Epoch: 111,   512/ 591 points] total loss per batch: 1.272
Policy (actual, predicted): 1 1
Policy data: tensor([0.0998, 0.6876, 0.0159, 0.0000, 0.0000, 0.1552, 0.0416],
       device='cuda:0')
Policy pred: tensor([1.1748e-01, 6.6016e-01, 1.2867e-02, 3.1825e-05, 5.5280e-06, 1.6671e-01,
        4.2741e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999404549598694
 
[Iteration 3] Process ID: 131596 [Epoch: 112,   128/ 591 points] total loss per batch: 1.287
Policy (actual, predicted): 0 0
Policy data: tensor([0.2726, 0.1151, 0.1639, 0.0448, 0.2509, 0.0269, 0.1258],
       device='cuda:0')
Policy pred: tensor([0.2661, 0.1072, 0.1677, 0.0451, 0.2642, 0.0278, 0.1220],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999993443489075
 
[Iteration 3] Process ID: 131596 [Epoch: 112,   256/ 591 points] total loss per batch: 1.231
Policy (actual, predicted): 1 1
Policy data: tensor([0.0246, 0.8483, 0.0131, 0.0114, 0.0551, 0.0214, 0.0262],
       device='cuda:0')
Policy pred: tensor([0.0210, 0.8563, 0.0125, 0.0129, 0.0531, 0.0201, 0.0241],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9995842576026917
 
[Iteration 3] Process ID: 131596 [Epoch: 112,   384/ 591 points] total loss per batch: 1.146
Policy (actual, predicted): 1 1
Policy data: tensor([1.2350e-18, 1.0000e+00, 3.4590e-21, 3.7141e-22, 2.0171e-07, 6.4565e-10,
        3.8945e-16], device='cuda:0')
Policy pred: tensor([9.8654e-05, 9.9954e-01, 2.0945e-05, 6.8153e-05, 2.6150e-05, 7.6274e-06,
        2.4047e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999991774559021
 
[Iteration 3] Process ID: 131596 [Epoch: 112,   512/ 591 points] total loss per batch: 1.293
Policy (actual, predicted): 2 2
Policy data: tensor([0.0146, 0.0715, 0.7924, 0.0112, 0.0162, 0.0730, 0.0211],
       device='cuda:0')
Policy pred: tensor([0.0160, 0.0776, 0.7822, 0.0129, 0.0206, 0.0695, 0.0213],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9993767738342285
 
[Iteration 3] Process ID: 131596 [Epoch: 113,   128/ 591 points] total loss per batch: 1.356
Policy (actual, predicted): 5 5
Policy data: tensor([0.0313, 0.1597, 0.0387, 0.0475, 0.0143, 0.6910, 0.0175],
       device='cuda:0')
Policy pred: tensor([0.0273, 0.1302, 0.0462, 0.0560, 0.0122, 0.7072, 0.0209],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999830722808838
 
[Iteration 3] Process ID: 131596 [Epoch: 113,   256/ 591 points] total loss per batch: 1.238
Policy (actual, predicted): 1 1
Policy data: tensor([0.2195, 0.2730, 0.0182, 0.1893, 0.0000, 0.1107, 0.1893],
       device='cuda:0')
Policy pred: tensor([2.5577e-01, 2.6147e-01, 1.5219e-02, 1.7603e-01, 2.8016e-06, 1.0466e-01,
        1.8685e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 113,   384/ 591 points] total loss per batch: 1.185
Policy (actual, predicted): 4 4
Policy data: tensor([0.0183, 0.0281, 0.0150, 0.0080, 0.8907, 0.0150, 0.0249],
       device='cuda:0')
Policy pred: tensor([0.0189, 0.0324, 0.0186, 0.0080, 0.8846, 0.0133, 0.0244],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.988484263420105
 
[Iteration 3] Process ID: 131596 [Epoch: 113,   512/ 591 points] total loss per batch: 1.344
Policy (actual, predicted): 1 5
Policy data: tensor([0.1395, 0.1489, 0.1419, 0.1465, 0.1407, 0.1395, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1343, 0.1393, 0.1389, 0.1344, 0.1342, 0.1677, 0.1512],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999972581863403
 
[Iteration 3] Process ID: 131596 [Epoch: 114,   128/ 591 points] total loss per batch: 1.344
Policy (actual, predicted): 6 6
Policy data: tensor([0.1579, 0.1354, 0.2204, 0.0586, 0.0990, 0.0599, 0.2688],
       device='cuda:0')
Policy pred: tensor([0.1828, 0.1309, 0.2059, 0.0599, 0.1050, 0.0573, 0.2582],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9901261925697327
 
[Iteration 3] Process ID: 131596 [Epoch: 114,   256/ 591 points] total loss per batch: 1.137
Policy (actual, predicted): 5 5
Policy data: tensor([0.0044, 0.0044, 0.0044, 0.0118, 0.0063, 0.9644, 0.0044],
       device='cuda:0')
Policy pred: tensor([0.0062, 0.0053, 0.0062, 0.0158, 0.0078, 0.9538, 0.0049],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999918341636658
 
[Iteration 3] Process ID: 131596 [Epoch: 114,   384/ 591 points] total loss per batch: 1.215
Policy (actual, predicted): 0 0
Policy data: tensor([0.4056, 0.0692, 0.3775, 0.0274, 0.0154, 0.0745, 0.0304],
       device='cuda:0')
Policy pred: tensor([0.4436, 0.0609, 0.3619, 0.0155, 0.0155, 0.0717, 0.0308],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.22482968866825104
 
[Iteration 3] Process ID: 131596 [Epoch: 114,   512/ 591 points] total loss per batch: 1.341
Policy (actual, predicted): 1 1
Policy data: tensor([0.1430, 0.1500, 0.1348, 0.1442, 0.1419, 0.1395, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1501, 0.1519, 0.1256, 0.1486, 0.1309, 0.1444, 0.1484],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999997615814209
 
[Iteration 3] Process ID: 131596 [Epoch: 115,   128/ 591 points] total loss per batch: 1.324
Policy (actual, predicted): 0 0
Policy data: tensor([0.4028, 0.3378, 0.0385, 0.0000, 0.0000, 0.0592, 0.1617],
       device='cuda:0')
Policy pred: tensor([4.0891e-01, 3.2970e-01, 3.7349e-02, 5.9824e-05, 3.7416e-05, 6.4886e-02,
        1.5906e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 115,   256/ 591 points] total loss per batch: 1.360
Policy (actual, predicted): 4 0
Policy data: tensor([0.1348, 0.1442, 0.1430, 0.1442, 0.1489, 0.1395, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.7455, 0.0383, 0.0432, 0.0309, 0.0581, 0.0370, 0.0469],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.07443880289793015
 
[Iteration 3] Process ID: 131596 [Epoch: 115,   384/ 591 points] total loss per batch: 1.221
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0054, 0.0115, 0.0195, 0.0076, 0.0261, 0.0189, 0.9111],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.3681643009185791
 
[Iteration 3] Process ID: 131596 [Epoch: 115,   512/ 591 points] total loss per batch: 1.156
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 8.2964e-01, 2.5501e-13, 1.8525e-15, 1.0572e-05, 4.6371e-09,
        1.7035e-01], device='cuda:0')
Policy pred: tensor([1.0445e-07, 8.1191e-01, 3.0335e-04, 3.0030e-04, 7.6781e-04, 4.1607e-04,
        1.8630e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999184608459473
 
[Iteration 3] Process ID: 131596 [Epoch: 116,   128/ 591 points] total loss per batch: 1.294
Policy (actual, predicted): 2 2
Policy data: tensor([1.6581e-13, 2.0790e-14, 1.0000e+00, 3.0322e-14, 4.0053e-12, 2.9689e-16,
        6.1009e-15], device='cuda:0')
Policy pred: tensor([2.7106e-07, 2.6516e-06, 9.9999e-01, 5.5215e-08, 7.0352e-06, 1.5997e-06,
        9.4949e-07], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999966025352478
 
[Iteration 3] Process ID: 131596 [Epoch: 116,   256/ 591 points] total loss per batch: 1.247
Policy (actual, predicted): 0 4
Policy data: tensor([0.1512, 0.1454, 0.1466, 0.1360, 0.1512, 0.1277, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1348, 0.1402, 0.1429, 0.1476, 0.1573, 0.1295, 0.1478],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999995231628418
 
[Iteration 3] Process ID: 131596 [Epoch: 116,   384/ 591 points] total loss per batch: 1.232
Policy (actual, predicted): 4 4
Policy data: tensor([0.0245, 0.0652, 0.0245, 0.0096, 0.8400, 0.0213, 0.0148],
       device='cuda:0')
Policy pred: tensor([0.0213, 0.0503, 0.0205, 0.0086, 0.8689, 0.0178, 0.0126],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9995314478874207
 
[Iteration 3] Process ID: 131596 [Epoch: 116,   512/ 591 points] total loss per batch: 1.334
Policy (actual, predicted): 1 5
Policy data: tensor([0.1395, 0.1489, 0.1419, 0.1465, 0.1407, 0.1395, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1425, 0.1303, 0.1437, 0.1396, 0.1489, 0.1543, 0.1407],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999994695186615
 
[Iteration 3] Process ID: 131596 [Epoch: 117,   128/ 591 points] total loss per batch: 1.260
Policy (actual, predicted): 4 4
Policy data: tensor([0.1578, 0.1214, 0.1578, 0.0264, 0.2835, 0.0125, 0.2406],
       device='cuda:0')
Policy pred: tensor([0.1542, 0.1265, 0.1707, 0.0305, 0.2652, 0.0151, 0.2377],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999832510948181
 
[Iteration 3] Process ID: 131596 [Epoch: 117,   256/ 591 points] total loss per batch: 1.325
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000, 0.1285, 0.2486, 0.0208, 0.4932, 0.0456, 0.0634],
       device='cuda:0')
Policy pred: tensor([1.2738e-05, 1.3768e-01, 2.4795e-01, 2.3948e-02, 4.7072e-01, 4.6990e-02,
        7.2702e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 117,   384/ 591 points] total loss per batch: 1.206
Policy (actual, predicted): 5 5
Policy data: tensor([5.1689e-19, 5.5501e-20, 1.4951e-17, 2.9807e-17, 5.1689e-19, 1.0000e+00,
        1.7601e-12], device='cuda:0')
Policy pred: tensor([1.7269e-04, 1.5368e-04, 1.3656e-04, 2.9309e-04, 2.3561e-04, 9.9897e-01,
        4.1648e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998652935028076
 
[Iteration 3] Process ID: 131596 [Epoch: 117,   512/ 591 points] total loss per batch: 1.228
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 8.6020e-15, 3.1931e-17, 1.7560e-21, 1.0000e+00, 8.2035e-21,
        2.8361e-22], device='cuda:0')
Policy pred: tensor([1.9983e-10, 2.3887e-04, 6.4995e-06, 1.2872e-06, 9.9973e-01, 2.4513e-07,
        2.1084e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999988675117493
 
[Iteration 3] Process ID: 131596 [Epoch: 118,   128/ 591 points] total loss per batch: 1.245
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 2.3553e-09, 7.0268e-20, 2.6710e-19, 1.4689e-13,
        1.5992e-09], device='cuda:0')
Policy pred: tensor([5.0384e-08, 9.9984e-01, 9.1524e-05, 7.4873e-07, 2.7288e-05, 1.3771e-06,
        3.9988e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999973773956299
 
[Iteration 3] Process ID: 131596 [Epoch: 118,   256/ 591 points] total loss per batch: 1.262
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1198, 0.4247, 0.1394, 0.0678, 0.0770, 0.0862, 0.0851],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.0036275675520300865
 
[Iteration 3] Process ID: 131596 [Epoch: 118,   384/ 591 points] total loss per batch: 1.285
Policy (actual, predicted): 4 4
Policy data: tensor([7.5326e-19, 5.8547e-17, 2.8361e-22, 3.0452e-23, 1.0000e+00, 2.9041e-19,
        1.6747e-17], device='cuda:0')
Policy pred: tensor([1.2969e-04, 1.5298e-04, 3.3589e-05, 2.1315e-05, 9.9959e-01, 9.1982e-06,
        5.8480e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 118,   512/ 591 points] total loss per batch: 1.259
Policy (actual, predicted): 2 2
Policy data: tensor([0.0184, 0.0217, 0.8952, 0.0080, 0.0200, 0.0167, 0.0200],
       device='cuda:0')
Policy pred: tensor([0.0193, 0.0237, 0.8828, 0.0090, 0.0241, 0.0175, 0.0236],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9947745203971863
 
[Iteration 3] Process ID: 131596 [Epoch: 119,   128/ 591 points] total loss per batch: 1.109
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000, 0.2224, 0.2090, 0.0355, 0.4307, 0.1025, 0.0000],
       device='cuda:0')
Policy pred: tensor([3.7692e-05, 2.3850e-01, 1.8205e-01, 3.9803e-02, 4.3138e-01, 1.0806e-01,
        1.7719e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 119,   256/ 591 points] total loss per batch: 1.322
Policy (actual, predicted): 4 4
Policy data: tensor([0.2044, 0.1439, 0.1573, 0.0419, 0.2227, 0.1097, 0.1202],
       device='cuda:0')
Policy pred: tensor([0.2203, 0.1342, 0.1703, 0.0264, 0.2398, 0.0980, 0.1111],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 119,   384/ 591 points] total loss per batch: 1.259
Policy (actual, predicted): 4 0
Policy data: tensor([0.1477, 0.1489, 0.1442, 0.1301, 0.1570, 0.1313, 0.1407],
       device='cuda:0')
Policy pred: tensor([0.1684, 0.1446, 0.1605, 0.1294, 0.1498, 0.1292, 0.1181],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.027066849172115326
 
[Iteration 3] Process ID: 131596 [Epoch: 119,   512/ 591 points] total loss per batch: 1.356
Policy (actual, predicted): 0 0
Policy data: tensor([0.8949, 0.0116, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.6435, 0.0531, 0.0550, 0.0467, 0.0802, 0.0524, 0.0691],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.07176285982131958
 
[Iteration 3] Process ID: 131596 [Epoch: 120,   128/ 591 points] total loss per batch: 1.272
Policy (actual, predicted): 4 4
Policy data: tensor([0.1477, 0.1477, 0.1477, 0.1265, 0.1524, 0.1348, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1479, 0.1331, 0.1417, 0.1284, 0.1602, 0.1365, 0.1521],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.011955570429563522
 
[Iteration 3] Process ID: 131596 [Epoch: 120,   256/ 591 points] total loss per batch: 1.280
Policy (actual, predicted): 0 0
Policy data: tensor([4.7694e-01, 4.4778e-01, 4.6184e-03, 4.5488e-05, 6.8740e-02, 2.6008e-04,
        1.6104e-03], device='cuda:0')
Policy pred: tensor([0.4808, 0.4387, 0.0052, 0.0010, 0.0697, 0.0014, 0.0032],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998533129692078
 
[Iteration 3] Process ID: 131596 [Epoch: 120,   384/ 591 points] total loss per batch: 1.211
Policy (actual, predicted): 0 0
Policy data: tensor([0.5066, 0.0453, 0.1017, 0.0668, 0.0548, 0.1130, 0.1118],
       device='cuda:0')
Policy pred: tensor([0.4981, 0.0510, 0.0962, 0.0707, 0.0594, 0.1118, 0.1127],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999878406524658
 
[Iteration 3] Process ID: 131596 [Epoch: 120,   512/ 591 points] total loss per batch: 1.243
Policy (actual, predicted): 1 1
Policy data: tensor([0.1140, 0.2503, 0.1140, 0.1492, 0.0864, 0.0359, 0.2503],
       device='cuda:0')
Policy pred: tensor([0.1015, 0.2584, 0.1346, 0.1414, 0.0921, 0.0432, 0.2288],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 121,   128/ 591 points] total loss per batch: 1.259
Policy (actual, predicted): 2 2
Policy data: tensor([0.1489, 0.1430, 0.1512, 0.1336, 0.1501, 0.1336, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1419, 0.1430, 0.1601, 0.1266, 0.1531, 0.1370, 0.1383],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999229907989502
 
[Iteration 3] Process ID: 131596 [Epoch: 121,   256/ 591 points] total loss per batch: 1.300
Policy (actual, predicted): 1 1
Policy data: tensor([2.0347e-20, 1.0000e+00, 0.0000e+00, 1.3130e-10, 4.4600e-18, 2.0347e-20,
        5.9039e-14], device='cuda:0')
Policy pred: tensor([5.6913e-04, 9.9923e-01, 3.4591e-05, 1.0742e-04, 4.1301e-05, 1.4838e-05,
        6.6994e-06], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999792575836182
 
[Iteration 3] Process ID: 131596 [Epoch: 121,   384/ 591 points] total loss per batch: 1.285
Policy (actual, predicted): 0 0
Policy data: tensor([0.4737, 0.0441, 0.4088, 0.0092, 0.0125, 0.0250, 0.0265],
       device='cuda:0')
Policy pred: tensor([0.4933, 0.0381, 0.4051, 0.0087, 0.0119, 0.0222, 0.0208],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9993343949317932
 
[Iteration 3] Process ID: 131596 [Epoch: 121,   512/ 591 points] total loss per batch: 1.238
Policy (actual, predicted): 2 2
Policy data: tensor([5.5501e-20, 5.0910e-11, 1.0000e+00, 1.8023e-19, 1.3407e-18, 5.4200e-23,
        1.4601e-20], device='cuda:0')
Policy pred: tensor([1.6518e-05, 1.7865e-04, 9.9943e-01, 7.6991e-05, 1.9830e-04, 1.0274e-05,
        9.0261e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999994695186615
 
[Iteration 3] Process ID: 131596 [Epoch: 122,   128/ 591 points] total loss per batch: 1.344
Policy (actual, predicted): 4 4
Policy data: tensor([0.1501, 0.1466, 0.1442, 0.1254, 0.1512, 0.1348, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1448, 0.1403, 0.1517, 0.1260, 0.1575, 0.1284, 0.1513],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999992251396179
 
[Iteration 3] Process ID: 131596 [Epoch: 122,   256/ 591 points] total loss per batch: 1.318
Policy (actual, predicted): 0 0
Policy data: tensor([0.7043, 0.0000, 0.0655, 0.0000, 0.0000, 0.0414, 0.1888],
       device='cuda:0')
Policy pred: tensor([6.9406e-01, 2.0489e-05, 6.3581e-02, 3.0851e-05, 5.2303e-05, 4.2033e-02,
        2.0022e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 122,   384/ 591 points] total loss per batch: 1.290
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 1.3434e-10, 0.0000e+00, 1.4982e-09, 1.0000e+00, 1.8493e-18,
        1.1451e-07], device='cuda:0')
Policy pred: tensor([1.4295e-07, 7.3557e-06, 2.8598e-06, 1.0264e-04, 9.9980e-01, 1.2104e-06,
        8.4449e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999986290931702
 
[Iteration 3] Process ID: 131596 [Epoch: 122,   512/ 591 points] total loss per batch: 1.127
Policy (actual, predicted): 0 0
Policy data: tensor([0.6244, 0.1329, 0.1033, 0.0218, 0.0187, 0.0466, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.6265, 0.1419, 0.1011, 0.0207, 0.0173, 0.0438, 0.0488],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.005135077517479658
 
[Iteration 3] Process ID: 131596 [Epoch: 123,   128/ 591 points] total loss per batch: 1.232
Policy (actual, predicted): 2 2
Policy data: tensor([0.1917, 0.1435, 0.5590, 0.0247, 0.0232, 0.0407, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.1916, 0.1419, 0.5553, 0.0263, 0.0246, 0.0418, 0.0185],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9933519959449768
 
[Iteration 3] Process ID: 131596 [Epoch: 123,   256/ 591 points] total loss per batch: 1.244
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 8.2964e-01, 2.5501e-13, 1.8525e-15, 1.0572e-05, 4.6371e-09,
        1.7035e-01], device='cuda:0')
Policy pred: tensor([7.1530e-08, 8.3829e-01, 6.5552e-04, 3.6920e-04, 7.3064e-04, 4.7965e-04,
        1.5947e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998207092285156
 
[Iteration 3] Process ID: 131596 [Epoch: 123,   384/ 591 points] total loss per batch: 1.286
Policy (actual, predicted): 4 4
Policy data: tensor([0.1017, 0.1906, 0.0697, 0.0768, 0.2265, 0.1600, 0.1747],
       device='cuda:0')
Policy pred: tensor([0.1039, 0.1825, 0.0709, 0.0765, 0.2247, 0.1509, 0.1907],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998067617416382
 
[Iteration 3] Process ID: 131596 [Epoch: 123,   512/ 591 points] total loss per batch: 1.227
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 4.6844e-06, 0.0000e+00, 1.8136e-02,
        9.8186e-01], device='cuda:0')
Policy pred: tensor([3.1734e-06, 7.5961e-07, 8.1944e-07, 2.3561e-04, 2.6172e-09, 2.6100e-02,
        9.7366e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 124,   128/ 591 points] total loss per batch: 1.281
Policy (actual, predicted): 0 0
Policy data: tensor([0.8765, 0.0149, 0.0079, 0.0097, 0.0199, 0.0526, 0.0183],
       device='cuda:0')
Policy pred: tensor([0.8713, 0.0150, 0.0090, 0.0107, 0.0206, 0.0527, 0.0207],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9962653517723083
 
[Iteration 3] Process ID: 131596 [Epoch: 124,   256/ 591 points] total loss per batch: 1.178
Policy (actual, predicted): 4 4
Policy data: tensor([0.1501, 0.1477, 0.1466, 0.1265, 0.1524, 0.1372, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1537, 0.1428, 0.1380, 0.1362, 0.1561, 0.1349, 0.1382],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999984502792358
 
[Iteration 3] Process ID: 131596 [Epoch: 124,   384/ 591 points] total loss per batch: 1.247
Policy (actual, predicted): 4 4
Policy data: tensor([0.0734, 0.2334, 0.0410, 0.0551, 0.3249, 0.1063, 0.1658],
       device='cuda:0')
Policy pred: tensor([0.0724, 0.2367, 0.0385, 0.0538, 0.3355, 0.1048, 0.1582],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999972581863403
 
[Iteration 3] Process ID: 131596 [Epoch: 124,   512/ 591 points] total loss per batch: 1.336
Policy (actual, predicted): 4 4
Policy data: tensor([0.0183, 0.0297, 0.0133, 0.0150, 0.8920, 0.0167, 0.0150],
       device='cuda:0')
Policy pred: tensor([0.0162, 0.0296, 0.0103, 0.0140, 0.8995, 0.0173, 0.0132],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9983408451080322
 
[Iteration 3] Process ID: 131596 [Epoch: 125,   128/ 591 points] total loss per batch: 1.261
Policy (actual, predicted): 2 2
Policy data: tensor([0.1156, 0.0361, 0.7432, 0.0128, 0.0160, 0.0240, 0.0523],
       device='cuda:0')
Policy pred: tensor([0.1180, 0.0369, 0.7476, 0.0115, 0.0134, 0.0260, 0.0467],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.99684739112854
 
[Iteration 3] Process ID: 131596 [Epoch: 125,   256/ 591 points] total loss per batch: 1.244
Policy (actual, predicted): 3 0
Policy data: tensor([0.1430, 0.1465, 0.1372, 0.1500, 0.1407, 0.1348, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1521, 0.1433, 0.1350, 0.1494, 0.1435, 0.1295, 0.1472],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9951803088188171
 
[Iteration 3] Process ID: 131596 [Epoch: 125,   384/ 591 points] total loss per batch: 1.288
Policy (actual, predicted): 1 1
Policy data: tensor([0.1501, 0.1536, 0.1419, 0.1301, 0.1501, 0.1325, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1492, 0.1594, 0.1378, 0.1265, 0.1531, 0.1395, 0.1344],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 125,   512/ 591 points] total loss per batch: 1.264
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 8.6020e-15, 3.1931e-17, 1.7560e-21, 1.0000e+00, 8.2035e-21,
        2.8361e-22], device='cuda:0')
Policy pred: tensor([4.8042e-11, 3.0658e-04, 3.1754e-05, 1.2761e-06, 9.9964e-01, 5.5258e-07,
        1.5108e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999988079071045
 
[Iteration 3] Process ID: 131596 [Epoch: 126,   128/ 591 points] total loss per batch: 1.265
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0052, 0.0094, 0.0182, 0.0064, 0.0229, 0.0141, 0.9237],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.29312339425086975
 
[Iteration 3] Process ID: 131596 [Epoch: 126,   256/ 591 points] total loss per batch: 1.221
Policy (actual, predicted): 5 5
Policy data: tensor([0.0764, 0.0659, 0.1061, 0.0524, 0.0855, 0.5439, 0.0698],
       device='cuda:0')
Policy pred: tensor([0.0748, 0.0708, 0.1037, 0.0522, 0.0904, 0.5385, 0.0697],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9995232820510864
 
[Iteration 3] Process ID: 131596 [Epoch: 126,   384/ 591 points] total loss per batch: 1.237
Policy (actual, predicted): 4 4
Policy data: tensor([0.1331, 0.0605, 0.0905, 0.0148, 0.4779, 0.0000, 0.2231],
       device='cuda:0')
Policy pred: tensor([1.3005e-01, 5.6669e-02, 9.9955e-02, 1.3853e-02, 4.5158e-01, 1.8725e-04,
        2.4770e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 126,   512/ 591 points] total loss per batch: 1.295
Policy (actual, predicted): 3 3
Policy data: tensor([0.0955, 0.1501, 0.0539, 0.2739, 0.0955, 0.0792, 0.2519],
       device='cuda:0')
Policy pred: tensor([0.0891, 0.1508, 0.0612, 0.2815, 0.0955, 0.0751, 0.2468],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999995231628418
 
[Iteration 3] Process ID: 131596 [Epoch: 127,   128/ 591 points] total loss per batch: 1.282
Policy (actual, predicted): 4 4
Policy data: tensor([0.0755, 0.0567, 0.1094, 0.2027, 0.3624, 0.0624, 0.1310],
       device='cuda:0')
Policy pred: tensor([0.0773, 0.0641, 0.1064, 0.2031, 0.3539, 0.0605, 0.1347],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999749660491943
 
[Iteration 3] Process ID: 131596 [Epoch: 127,   256/ 591 points] total loss per batch: 1.262
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1228, 0.3996, 0.1473, 0.0720, 0.0814, 0.0859, 0.0911],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.0033977236598730087
 
[Iteration 3] Process ID: 131596 [Epoch: 127,   384/ 591 points] total loss per batch: 1.280
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 2.5064e-16, 0.0000e+00, 5.6356e-18, 4.0880e-19, 3.8986e-15,
        2.5064e-16], device='cuda:0')
Policy pred: tensor([9.9845e-01, 6.4152e-04, 5.2235e-05, 7.5213e-04, 2.9933e-05, 2.1142e-05,
        4.8859e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999528527259827
 
[Iteration 3] Process ID: 131596 [Epoch: 127,   512/ 591 points] total loss per batch: 1.302
Policy (actual, predicted): 2 2
Policy data: tensor([0.0374, 0.0476, 0.7277, 0.0159, 0.0175, 0.0576, 0.0962],
       device='cuda:0')
Policy pred: tensor([0.1013, 0.1018, 0.3768, 0.0835, 0.0972, 0.1100, 0.1294],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9996293783187866
 
[Iteration 3] Process ID: 131596 [Epoch: 128,   128/ 591 points] total loss per batch: 1.231
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000e+00, 5.4318e-05, 9.9995e-01, 1.2911e-19, 4.9078e-19, 4.9078e-19,
        4.9078e-19], device='cuda:0')
Policy pred: tensor([1.5650e-09, 4.1155e-05, 9.9992e-01, 6.9970e-08, 4.2461e-06, 1.8673e-07,
        3.1959e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999998807907104
 
[Iteration 3] Process ID: 131596 [Epoch: 128,   256/ 591 points] total loss per batch: 1.256
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 0.0000e+00, 7.5759e-14, 0.0000e+00, 2.9489e-20,
        1.5837e-17], device='cuda:0')
Policy pred: tensor([3.1439e-05, 9.9983e-01, 1.0808e-05, 2.0155e-06, 4.7692e-07, 3.1088e-06,
        1.2614e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 128,   384/ 591 points] total loss per batch: 1.274
Policy (actual, predicted): 4 4
Policy data: tensor([0.1547, 0.1360, 0.1454, 0.1289, 0.1594, 0.1337, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1491, 0.1383, 0.1386, 0.1298, 0.1680, 0.1358, 0.1405],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999696612358093
 
[Iteration 3] Process ID: 131596 [Epoch: 128,   512/ 591 points] total loss per batch: 1.248
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000, 0.0000, 0.2546, 0.1031, 0.3662, 0.0512, 0.2248],
       device='cuda:0')
Policy pred: tensor([2.7148e-05, 4.6684e-05, 2.5835e-01, 1.0290e-01, 3.7629e-01, 4.5666e-02,
        2.1672e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999988675117493
 
[Iteration 3] Process ID: 131596 [Epoch: 129,   128/ 591 points] total loss per batch: 1.196
Policy (actual, predicted): 1 6
Policy data: tensor([0.1580, 0.2054, 0.0568, 0.0758, 0.1884, 0.1102, 0.2054],
       device='cuda:0')
Policy pred: tensor([0.1582, 0.2122, 0.0532, 0.0732, 0.1823, 0.1009, 0.2200],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999849200248718
 
[Iteration 3] Process ID: 131596 [Epoch: 129,   256/ 591 points] total loss per batch: 1.271
Policy (actual, predicted): 1 1
Policy data: tensor([0.0330, 0.6655, 0.0208, 0.0094, 0.0192, 0.2410, 0.0111],
       device='cuda:0')
Policy pred: tensor([0.0268, 0.7286, 0.0146, 0.0059, 0.0122, 0.2055, 0.0064],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9993038773536682
 
[Iteration 3] Process ID: 131596 [Epoch: 129,   384/ 591 points] total loss per batch: 1.339
Policy (actual, predicted): 0 0
Policy data: tensor([0.4056, 0.0692, 0.3775, 0.0274, 0.0154, 0.0745, 0.0304],
       device='cuda:0')
Policy pred: tensor([0.4423, 0.0672, 0.3564, 0.0154, 0.0151, 0.0762, 0.0275],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.18929854035377502
 
[Iteration 3] Process ID: 131596 [Epoch: 129,   512/ 591 points] total loss per batch: 1.295
Policy (actual, predicted): 4 4
Policy data: tensor([0.0113, 0.0096, 0.0444, 0.0078, 0.8089, 0.0180, 0.1000],
       device='cuda:0')
Policy pred: tensor([0.0117, 0.0105, 0.0503, 0.0079, 0.7977, 0.0190, 0.1031],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.988329291343689
 
[Iteration 3] Process ID: 131596 [Epoch: 130,   128/ 591 points] total loss per batch: 1.258
Policy (actual, predicted): 4 4
Policy data: tensor([0.0263, 0.0263, 0.0263, 0.0079, 0.8608, 0.0294, 0.0231],
       device='cuda:0')
Policy pred: tensor([0.0221, 0.0234, 0.0254, 0.0069, 0.8766, 0.0263, 0.0193],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9913908839225769
 
[Iteration 3] Process ID: 131596 [Epoch: 130,   256/ 591 points] total loss per batch: 1.341
Policy (actual, predicted): 0 0
Policy data: tensor([9.9639e-01, 5.9633e-10, 1.2765e-11, 7.0197e-16, 3.6057e-03, 2.2856e-14,
        1.3496e-09], device='cuda:0')
Policy pred: tensor([9.9861e-01, 7.4445e-07, 3.6193e-07, 5.0646e-08, 1.3813e-03, 2.0138e-07,
        5.9679e-06], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999984502792358
 
[Iteration 3] Process ID: 131596 [Epoch: 130,   384/ 591 points] total loss per batch: 1.297
Policy (actual, predicted): 0 0
Policy data: tensor([0.4737, 0.0441, 0.4088, 0.0092, 0.0125, 0.0250, 0.0265],
       device='cuda:0')
Policy pred: tensor([0.4571, 0.0518, 0.4115, 0.0105, 0.0133, 0.0250, 0.0308],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9995012283325195
 
[Iteration 3] Process ID: 131596 [Epoch: 130,   512/ 591 points] total loss per batch: 1.129
Policy (actual, predicted): 1 1
Policy data: tensor([0.0934, 0.6501, 0.1337, 0.0188, 0.0173, 0.0412, 0.0455],
       device='cuda:0')
Policy pred: tensor([0.1194, 0.4075, 0.1403, 0.0706, 0.0805, 0.0839, 0.0978],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -8.942931890487671e-05
 
[Iteration 3] Process ID: 131596 [Epoch: 131,   128/ 591 points] total loss per batch: 1.215
Policy (actual, predicted): 3 3
Policy data: tensor([0.1407, 0.1430, 0.1395, 0.1559, 0.1372, 0.1407, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1338, 0.1413, 0.1376, 0.1608, 0.1411, 0.1411, 0.1444],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999979138374329
 
[Iteration 3] Process ID: 131596 [Epoch: 131,   256/ 591 points] total loss per batch: 1.137
Policy (actual, predicted): 2 2
Policy data: tensor([0.2128, 0.2892, 0.4030, 0.0122, 0.0122, 0.0583, 0.0122],
       device='cuda:0')
Policy pred: tensor([0.2087, 0.2970, 0.3953, 0.0121, 0.0153, 0.0575, 0.0141],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9920803308486938
 
[Iteration 3] Process ID: 131596 [Epoch: 131,   384/ 591 points] total loss per batch: 1.300
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0048, 0.0000, 0.9952, 0.0000],
       device='cuda:0')
Policy pred: tensor([9.6885e-05, 3.3446e-05, 2.1567e-05, 3.3384e-03, 5.1864e-06, 9.9632e-01,
        1.8204e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999994039535522
 
[Iteration 3] Process ID: 131596 [Epoch: 131,   512/ 591 points] total loss per batch: 1.318
Policy (actual, predicted): 1 1
Policy data: tensor([0.1395, 0.1501, 0.1407, 0.1477, 0.1454, 0.1301, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1353, 0.1502, 0.1492, 0.1418, 0.1460, 0.1336, 0.1439],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9995378255844116
 
[Iteration 3] Process ID: 131596 [Epoch: 132,   128/ 591 points] total loss per batch: 1.303
Policy (actual, predicted): 0 0
Policy data: tensor([0.4016, 0.0000, 0.0630, 0.0000, 0.0000, 0.3145, 0.2210],
       device='cuda:0')
Policy pred: tensor([4.5640e-01, 9.7087e-06, 5.2727e-02, 6.4255e-05, 6.8862e-06, 2.7199e-01,
        2.1879e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998128414154053
 
[Iteration 3] Process ID: 131596 [Epoch: 132,   256/ 591 points] total loss per batch: 1.272
Policy (actual, predicted): 4 4
Policy data: tensor([0.1501, 0.1430, 0.1442, 0.1313, 0.1559, 0.1360, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1426, 0.1424, 0.1495, 0.1359, 0.1554, 0.1360, 0.1382],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9911887645721436
 
[Iteration 3] Process ID: 131596 [Epoch: 132,   384/ 591 points] total loss per batch: 1.276
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 4.8059e-10, 1.6616e-09, 2.7480e-17, 1.0000e+00, 5.0091e-09,
        4.7592e-06], device='cuda:0')
Policy pred: tensor([8.0320e-08, 8.6132e-04, 1.7642e-05, 1.1397e-06, 9.9845e-01, 3.7777e-07,
        6.6825e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 132,   512/ 591 points] total loss per batch: 1.215
Policy (actual, predicted): 3 3
Policy data: tensor([5.2778e-18, 0.0000e+00, 0.0000e+00, 9.0147e-01, 5.0333e-24, 5.1541e-21,
        9.8525e-02], device='cuda:0')
Policy pred: tensor([3.4133e-04, 8.3333e-08, 1.7510e-05, 9.1069e-01, 1.1387e-04, 4.9741e-04,
        8.8338e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999766945838928
 
[Iteration 3] Process ID: 131596 [Epoch: 133,   128/ 591 points] total loss per batch: 1.203
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7696e-15, 1.3965e-13, 1.0000e+00,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([2.9952e-05, 1.9078e-06, 1.1714e-06, 9.0427e-05, 4.9022e-06, 9.9985e-01,
        1.8258e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999958872795105
 
[Iteration 3] Process ID: 131596 [Epoch: 133,   256/ 591 points] total loss per batch: 1.334
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1289, 0.3695, 0.1454, 0.0770, 0.0870, 0.0901, 0.1020],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.0001244768500328064
 
[Iteration 3] Process ID: 131596 [Epoch: 133,   384/ 591 points] total loss per batch: 1.284
Policy (actual, predicted): 0 0
Policy data: tensor([0.6245, 0.1342, 0.1020, 0.0218, 0.0187, 0.0466, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.5905, 0.1468, 0.1086, 0.0245, 0.0215, 0.0510, 0.0570],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.0037367914337664843
 
[Iteration 3] Process ID: 131596 [Epoch: 133,   512/ 591 points] total loss per batch: 1.227
Policy (actual, predicted): 2 1
Policy data: tensor([0.1489, 0.1477, 0.1524, 0.1325, 0.1489, 0.1301, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1192, 0.4370, 0.1390, 0.0630, 0.0784, 0.0756, 0.0878],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.005798989906907082
 
[Iteration 3] Process ID: 131596 [Epoch: 134,   128/ 591 points] total loss per batch: 1.163
Policy (actual, predicted): 4 4
Policy data: tensor([0.2260, 0.1036, 0.0947, 0.0365, 0.4309, 0.0365, 0.0718],
       device='cuda:0')
Policy pred: tensor([0.2274, 0.0961, 0.0945, 0.0401, 0.4354, 0.0348, 0.0718],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999980330467224
 
[Iteration 3] Process ID: 131596 [Epoch: 134,   256/ 591 points] total loss per batch: 1.304
Policy (actual, predicted): 3 3
Policy data: tensor([1.6772e-04, 0.0000e+00, 5.7598e-04, 9.9391e-01, 6.8750e-04, 1.3587e-05,
        4.6453e-03], device='cuda:0')
Policy pred: tensor([1.6055e-04, 1.4309e-05, 2.3931e-04, 9.9410e-01, 7.3405e-04, 4.8404e-05,
        4.7042e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999935626983643
 
[Iteration 3] Process ID: 131596 [Epoch: 134,   384/ 591 points] total loss per batch: 1.296
Policy (actual, predicted): 0 0
Policy data: tensor([0.3297, 0.0255, 0.0255, 0.0241, 0.0938, 0.2501, 0.2513],
       device='cuda:0')
Policy pred: tensor([0.3046, 0.0328, 0.0258, 0.0240, 0.0902, 0.2603, 0.2624],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9959797263145447
 
[Iteration 3] Process ID: 131596 [Epoch: 134,   512/ 591 points] total loss per batch: 1.190
Policy (actual, predicted): 6 6
Policy data: tensor([0.1441, 0.0000, 0.1638, 0.2384, 0.0000, 0.0213, 0.4324],
       device='cuda:0')
Policy pred: tensor([1.4958e-01, 7.3864e-05, 1.5808e-01, 2.3967e-01, 1.8880e-04, 2.2651e-02,
        4.2976e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 135,   128/ 591 points] total loss per batch: 1.122
Policy (actual, predicted): 6 6
Policy data: tensor([0.2005, 0.0000, 0.0790, 0.0339, 0.2685, 0.0857, 0.3324],
       device='cuda:0')
Policy pred: tensor([2.1984e-01, 1.1232e-04, 8.0823e-02, 3.6388e-02, 2.6873e-01, 8.4795e-02,
        3.0932e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 135,   256/ 591 points] total loss per batch: 1.255
Policy (actual, predicted): 0 0
Policy data: tensor([0.3856, 0.2287, 0.0586, 0.0211, 0.1959, 0.0437, 0.0665],
       device='cuda:0')
Policy pred: tensor([0.3956, 0.2288, 0.0618, 0.0206, 0.1850, 0.0443, 0.0638],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9997943043708801
 
[Iteration 3] Process ID: 131596 [Epoch: 135,   384/ 591 points] total loss per batch: 1.412
Policy (actual, predicted): 1 1
Policy data: tensor([0.2073, 0.2587, 0.0449, 0.0955, 0.0000, 0.1531, 0.2404],
       device='cuda:0')
Policy pred: tensor([1.6287e-01, 2.7332e-01, 5.6183e-02, 9.4645e-02, 5.3869e-06, 1.8510e-01,
        2.2787e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 135,   512/ 591 points] total loss per batch: 1.202
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9906e-01, 0.0000e+00, 9.3862e-04,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([1.1717e-05, 3.4996e-06, 8.4927e-06, 9.9815e-01, 6.3368e-06, 1.8069e-03,
        1.6445e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999605417251587
 
[Iteration 3] Process ID: 131596 [Epoch: 136,   128/ 591 points] total loss per batch: 1.372
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 1.7251e-07, 8.4322e-16, 2.3063e-21, 2.4183e-15, 6.9951e-14,
        2.4764e-22], device='cuda:0')
Policy pred: tensor([9.9999e-01, 5.0468e-06, 1.5307e-07, 6.1335e-08, 5.7355e-06, 2.6177e-08,
        1.6633e-06], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 136,   256/ 591 points] total loss per batch: 1.170
Policy (actual, predicted): 2 2
Policy data: tensor([1.1480e-21, 0.0000e+00, 1.0000e+00, 5.3631e-21, 1.1211e-24, 5.3631e-21,
        2.0386e-20], device='cuda:0')
Policy pred: tensor([1.9864e-07, 1.3733e-07, 9.9985e-01, 1.3451e-05, 6.4790e-05, 7.7463e-06,
        6.3041e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999909996986389
 
[Iteration 3] Process ID: 131596 [Epoch: 136,   384/ 591 points] total loss per batch: 1.271
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000, 0.1610, 0.3009, 0.0376, 0.3395, 0.1610, 0.0000],
       device='cuda:0')
Policy pred: tensor([9.4469e-05, 1.5181e-01, 3.0628e-01, 4.1505e-02, 3.4058e-01, 1.5962e-01,
        1.1174e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 136,   512/ 591 points] total loss per batch: 1.203
Policy (actual, predicted): 3 3
Policy data: tensor([0.1383, 0.1454, 0.1442, 0.1477, 0.1419, 0.1383, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1384, 0.1487, 0.1394, 0.1509, 0.1400, 0.1331, 0.1494],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998862743377686
 
[Iteration 3] Process ID: 131596 [Epoch: 137,   128/ 591 points] total loss per batch: 1.271
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0023, 0.0000, 0.9977, 0.0000],
       device='cuda:0')
Policy pred: tensor([2.6483e-05, 4.1995e-07, 1.9080e-05, 3.1910e-03, 3.4889e-07, 9.9676e-01,
        3.8323e-06], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999956488609314
 
[Iteration 3] Process ID: 131596 [Epoch: 137,   256/ 591 points] total loss per batch: 1.242
Policy (actual, predicted): 0 0
Policy data: tensor([0.3134, 0.1113, 0.1456, 0.0474, 0.1893, 0.0474, 0.1456],
       device='cuda:0')
Policy pred: tensor([0.3160, 0.1127, 0.1512, 0.0492, 0.1840, 0.0480, 0.1389],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 137,   384/ 591 points] total loss per batch: 1.264
Policy (actual, predicted): 4 4
Policy data: tensor([0.1477, 0.1407, 0.1501, 0.1289, 0.1605, 0.1266, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1546, 0.1448, 0.1555, 0.1321, 0.1575, 0.1178, 0.1377],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 137,   512/ 591 points] total loss per batch: 1.259
Policy (actual, predicted): 4 4
Policy data: tensor([0.1489, 0.1466, 0.1477, 0.1336, 0.1536, 0.1301, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1442, 0.1426, 0.1355, 0.1359, 0.1642, 0.1317, 0.1460],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.02039722166955471
 
[Iteration 3] Process ID: 131596 [Epoch: 138,   128/ 591 points] total loss per batch: 1.315
Policy (actual, predicted): 0 0
Policy data: tensor([0.3128, 0.0000, 0.2017, 0.1191, 0.0000, 0.1891, 0.1773],
       device='cuda:0')
Policy pred: tensor([3.1195e-01, 2.6616e-05, 2.1702e-01, 1.1565e-01, 6.5259e-05, 1.9343e-01,
        1.6186e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 138,   256/ 591 points] total loss per batch: 1.353
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 4.6359e-12, 0.0000e+00, 1.6805e-17, 0.0000e+00, 3.7417e-17,
        6.7988e-07], device='cuda:0')
Policy pred: tensor([9.9995e-01, 3.5986e-05, 3.3174e-08, 2.5836e-06, 1.0896e-08, 2.4490e-07,
        1.3419e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999998211860657
 
[Iteration 3] Process ID: 131596 [Epoch: 138,   384/ 591 points] total loss per batch: 1.252
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9906e-01, 0.0000e+00, 9.3862e-04,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([8.8892e-07, 4.3580e-07, 1.5952e-06, 9.9978e-01, 3.8823e-07, 2.1824e-04,
        1.8683e-06], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999825358390808
 
[Iteration 3] Process ID: 131596 [Epoch: 138,   512/ 591 points] total loss per batch: 1.161
Policy (actual, predicted): 2 2
Policy data: tensor([0.1156, 0.0361, 0.7432, 0.0128, 0.0160, 0.0240, 0.0523],
       device='cuda:0')
Policy pred: tensor([0.1131, 0.0331, 0.7357, 0.0150, 0.0217, 0.0264, 0.0549],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9932982921600342
 
[Iteration 3] Process ID: 131596 [Epoch: 139,   128/ 591 points] total loss per batch: 1.285
Policy (actual, predicted): 1 0
Policy data: tensor([0.1442, 0.1477, 0.1336, 0.1419, 0.1395, 0.1454, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1529, 0.1518, 0.1356, 0.1462, 0.1230, 0.1406, 0.1500],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999846816062927
 
[Iteration 3] Process ID: 131596 [Epoch: 139,   256/ 591 points] total loss per batch: 1.283
Policy (actual, predicted): 2 2
Policy data: tensor([1.1003e-06, 2.9133e-06, 9.9991e-01, 3.8581e-19, 7.9434e-05, 1.0247e-15,
        1.9120e-06], device='cuda:0')
Policy pred: tensor([3.6128e-06, 1.3361e-04, 9.9969e-01, 1.8914e-06, 1.2922e-04, 1.5888e-05,
        2.7632e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999991059303284
 
[Iteration 3] Process ID: 131596 [Epoch: 139,   384/ 591 points] total loss per batch: 1.227
Policy (actual, predicted): 6 6
Policy data: tensor([0.1407, 0.1442, 0.1419, 0.1336, 0.1407, 0.1384, 0.1605],
       device='cuda:0')
Policy pred: tensor([0.1453, 0.1505, 0.1372, 0.1223, 0.1502, 0.1323, 0.1623],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999983310699463
 
[Iteration 3] Process ID: 131596 [Epoch: 139,   512/ 591 points] total loss per batch: 1.210
Policy (actual, predicted): 6 6
Policy data: tensor([0.1407, 0.1383, 0.1383, 0.1489, 0.1383, 0.1430, 0.1524],
       device='cuda:0')
Policy pred: tensor([0.1390, 0.1446, 0.1347, 0.1511, 0.1339, 0.1417, 0.1550],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999974370002747
 
[Iteration 3] Process ID: 131596 [Epoch: 140,   128/ 591 points] total loss per batch: 1.231
Policy (actual, predicted): 0 0
Policy data: tensor([9.7685e-01, 0.0000e+00, 2.5326e-03, 5.5271e-05, 5.5271e-05, 1.6543e-05,
        2.0495e-02], device='cuda:0')
Policy pred: tensor([9.7428e-01, 3.8266e-06, 2.4124e-03, 1.0708e-03, 2.8070e-04, 5.4218e-04,
        2.1414e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999993443489075
 
[Iteration 3] Process ID: 131596 [Epoch: 140,   256/ 591 points] total loss per batch: 1.269
Policy (actual, predicted): 0 2
Policy data: tensor([0.1501, 0.1407, 0.1477, 0.1325, 0.1501, 0.1348, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1560, 0.1439, 0.1622, 0.1367, 0.1370, 0.1205, 0.1437],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 140,   384/ 591 points] total loss per batch: 1.300
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.1798, 0.0882, 0.2429, 0.2255, 0.1318, 0.1318],
       device='cuda:0')
Policy pred: tensor([7.0821e-05, 1.6868e-01, 9.8990e-02, 2.3847e-01, 2.3003e-01, 1.3965e-01,
        1.2411e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999984502792358
 
[Iteration 3] Process ID: 131596 [Epoch: 140,   512/ 591 points] total loss per batch: 1.268
Policy (actual, predicted): 0 1
Policy data: tensor([0.1536, 0.1466, 0.1442, 0.1289, 0.1477, 0.1313, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1543, 0.1607, 0.1436, 0.1278, 0.1441, 0.1230, 0.1465],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9818806648254395
 
[Iteration 3] Process ID: 131596 [Epoch: 141,   128/ 591 points] total loss per batch: 1.280
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 7.3256e-14, 2.3293e-15, 4.0286e-18, 1.4744e-06, 6.6626e-16,
        3.2178e-08], device='cuda:0')
Policy pred: tensor([9.9842e-01, 3.8267e-05, 2.9370e-06, 2.9892e-06, 1.2337e-03, 6.8378e-07,
        2.9737e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 141,   256/ 591 points] total loss per batch: 1.424
Policy (actual, predicted): 0 0
Policy data: tensor([0.7867, 0.0395, 0.0146, 0.0041, 0.0868, 0.0425, 0.0258],
       device='cuda:0')
Policy pred: tensor([0.8041, 0.0355, 0.0144, 0.0047, 0.0789, 0.0379, 0.0243],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9992660880088806
 
[Iteration 3] Process ID: 131596 [Epoch: 141,   384/ 591 points] total loss per batch: 1.221
Policy (actual, predicted): 1 1
Policy data: tensor([8.6393e-18, 1.0000e+00, 8.6393e-18, 1.7223e-17, 2.9168e-22, 3.2070e-20,
        3.2070e-20], device='cuda:0')
Policy pred: tensor([6.3073e-05, 9.9948e-01, 3.8600e-04, 4.8968e-05, 1.4999e-05, 3.4631e-06,
        1.2042e-06], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9997269511222839
 
[Iteration 3] Process ID: 131596 [Epoch: 141,   512/ 591 points] total loss per batch: 1.206
Policy (actual, predicted): 2 2
Policy data: tensor([3.2442e-01, 2.0857e-02, 5.1191e-01, 1.1861e-05, 1.4092e-01, 3.1682e-04,
        1.5668e-03], device='cuda:0')
Policy pred: tensor([0.3093, 0.0241, 0.5359, 0.0006, 0.1261, 0.0008, 0.0031],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998788833618164
 
[Iteration 3] Process ID: 131596 [Epoch: 142,   128/ 591 points] total loss per batch: 1.287
Policy (actual, predicted): 6 6
Policy data: tensor([0.1561, 0.1427, 0.1304, 0.1427, 0.1427, 0.0989, 0.1864],
       device='cuda:0')
Policy pred: tensor([0.1623, 0.1506, 0.1347, 0.1310, 0.1282, 0.0897, 0.2036],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999966025352478
 
[Iteration 3] Process ID: 131596 [Epoch: 142,   256/ 591 points] total loss per batch: 1.241
Policy (actual, predicted): 6 6
Policy data: tensor([0.1441, 0.0000, 0.1638, 0.2384, 0.0000, 0.0213, 0.4324],
       device='cuda:0')
Policy pred: tensor([1.6566e-01, 2.3076e-05, 1.5399e-01, 2.5545e-01, 1.2365e-04, 2.4523e-02,
        4.0023e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 142,   384/ 591 points] total loss per batch: 1.241
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 9.9834e-01, 0.0000e+00, 1.6033e-17, 9.9269e-17, 1.0165e-13,
        1.6558e-03], device='cuda:0')
Policy pred: tensor([7.3339e-06, 9.9804e-01, 1.1843e-07, 1.4553e-06, 8.1417e-06, 1.4509e-06,
        1.9394e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 142,   512/ 591 points] total loss per batch: 1.266
Policy (actual, predicted): 1 1
Policy data: tensor([0.0998, 0.6876, 0.0159, 0.0000, 0.0000, 0.1552, 0.0416],
       device='cuda:0')
Policy pred: tensor([1.0294e-01, 6.5871e-01, 1.9928e-02, 4.5542e-05, 2.0650e-05, 1.7005e-01,
        4.8308e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999158978462219
 
[Iteration 3] Process ID: 131596 [Epoch: 143,   128/ 591 points] total loss per batch: 1.243
Policy (actual, predicted): 1 1
Policy data: tensor([8.6393e-18, 1.0000e+00, 8.6393e-18, 1.7223e-17, 2.9168e-22, 3.2070e-20,
        3.2070e-20], device='cuda:0')
Policy pred: tensor([6.3566e-05, 9.9898e-01, 8.3479e-04, 1.0068e-04, 1.8313e-05, 4.0927e-06,
        1.4332e-06], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999738931655884
 
[Iteration 3] Process ID: 131596 [Epoch: 143,   256/ 591 points] total loss per batch: 1.224
Policy (actual, predicted): 4 2
Policy data: tensor([0.1477, 0.1477, 0.1477, 0.1265, 0.1524, 0.1348, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1450, 0.1504, 0.1539, 0.1239, 0.1476, 0.1394, 0.1398],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.0045078834518790245
 
[Iteration 3] Process ID: 131596 [Epoch: 143,   384/ 591 points] total loss per batch: 1.314
Policy (actual, predicted): 0 4
Policy data: tensor([0.1512, 0.1430, 0.1489, 0.1289, 0.1430, 0.1372, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1478, 0.1480, 0.1457, 0.1217, 0.1538, 0.1376, 0.1455],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 143,   512/ 591 points] total loss per batch: 1.259
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 2.3479e-07, 6.9915e-09, 3.0720e-19, 3.1457e-16, 1.7056e-09,
        1.1677e-18], device='cuda:0')
Policy pred: tensor([9.9931e-01, 4.0594e-04, 6.0231e-06, 1.5995e-05, 2.0097e-04, 1.2890e-05,
        5.1612e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999994039535522
 
[Iteration 3] Process ID: 131596 [Epoch: 144,   128/ 591 points] total loss per batch: 1.206
Policy (actual, predicted): 4 4
Policy data: tensor([2.1681e-01, 1.4692e-01, 7.6242e-02, 1.0391e-04, 4.5245e-01, 1.0673e-03,
        1.0641e-01], device='cuda:0')
Policy pred: tensor([0.1850, 0.1624, 0.0818, 0.0005, 0.4735, 0.0014, 0.0954],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 144,   256/ 591 points] total loss per batch: 1.324
Policy (actual, predicted): 2 2
Policy data: tensor([0.0146, 0.0715, 0.7924, 0.0112, 0.0162, 0.0730, 0.0211],
       device='cuda:0')
Policy pred: tensor([0.0133, 0.0620, 0.8136, 0.0108, 0.0161, 0.0650, 0.0193],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998762607574463
 
[Iteration 3] Process ID: 131596 [Epoch: 144,   384/ 591 points] total loss per batch: 1.259
Policy (actual, predicted): 2 2
Policy data: tensor([1.6581e-13, 2.0790e-14, 1.0000e+00, 3.0322e-14, 4.0053e-12, 2.9689e-16,
        6.1009e-15], device='cuda:0')
Policy pred: tensor([2.4970e-06, 1.3521e-05, 9.9995e-01, 2.7451e-07, 2.8161e-05, 3.7058e-06,
        6.5232e-06], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999927282333374
 
[Iteration 3] Process ID: 131596 [Epoch: 144,   512/ 591 points] total loss per batch: 1.160
Policy (actual, predicted): 1 0
Policy data: tensor([0.1372, 0.1477, 0.1454, 0.1419, 0.1442, 0.1360, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.7863, 0.0308, 0.0338, 0.0264, 0.0513, 0.0310, 0.0405],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.066448874771595
 
[Iteration 3] Process ID: 131596 [Epoch: 145,   128/ 591 points] total loss per batch: 1.160
Policy (actual, predicted): 6 6
Policy data: tensor([0.1454, 0.1325, 0.1395, 0.1477, 0.1383, 0.1419, 0.1547],
       device='cuda:0')
Policy pred: tensor([0.1497, 0.1358, 0.1457, 0.1302, 0.1343, 0.1467, 0.1577],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999734163284302
 
[Iteration 3] Process ID: 131596 [Epoch: 145,   256/ 591 points] total loss per batch: 1.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0048, 0.0000, 0.9952, 0.0000],
       device='cuda:0')
Policy pred: tensor([4.5970e-05, 5.6478e-06, 1.0824e-05, 3.9674e-03, 6.2671e-07, 9.9592e-01,
        4.7630e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999995231628418
 
[Iteration 3] Process ID: 131596 [Epoch: 145,   384/ 591 points] total loss per batch: 1.255
Policy (actual, predicted): 5 5
Policy data: tensor([0.0941, 0.0812, 0.0903, 0.0508, 0.0928, 0.5187, 0.0721],
       device='cuda:0')
Policy pred: tensor([0.0912, 0.0677, 0.1003, 0.0490, 0.0892, 0.5297, 0.0729],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9993395805358887
 
[Iteration 3] Process ID: 131596 [Epoch: 145,   512/ 591 points] total loss per batch: 1.259
Policy (actual, predicted): 2 2
Policy data: tensor([0.0217, 0.1514, 0.4426, 0.0057, 0.3459, 0.0156, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.0214, 0.1419, 0.5582, 0.0056, 0.2440, 0.0141, 0.0148],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.46921390295028687
 
[Iteration 3] Process ID: 131596 [Epoch: 146,   128/ 591 points] total loss per batch: 1.295
Policy (actual, predicted): 4 4
Policy data: tensor([0.1578, 0.1214, 0.1578, 0.0264, 0.2835, 0.0125, 0.2406],
       device='cuda:0')
Policy pred: tensor([0.1603, 0.1169, 0.1658, 0.0324, 0.2855, 0.0190, 0.2201],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999808073043823
 
[Iteration 3] Process ID: 131596 [Epoch: 146,   256/ 591 points] total loss per batch: 1.257
Policy (actual, predicted): 2 2
Policy data: tensor([2.1615e-01, 3.1014e-02, 6.5737e-01, 1.8202e-04, 8.5820e-02, 3.4818e-03,
        5.9787e-03], device='cuda:0')
Policy pred: tensor([0.2040, 0.0342, 0.6840, 0.0008, 0.0669, 0.0034, 0.0066],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999813437461853
 
[Iteration 3] Process ID: 131596 [Epoch: 146,   384/ 591 points] total loss per batch: 1.256
Policy (actual, predicted): 4 4
Policy data: tensor([0.0734, 0.2334, 0.0410, 0.0551, 0.3249, 0.1063, 0.1658],
       device='cuda:0')
Policy pred: tensor([0.0729, 0.2238, 0.0370, 0.0556, 0.3634, 0.0930, 0.1544],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999915361404419
 
[Iteration 3] Process ID: 131596 [Epoch: 146,   512/ 591 points] total loss per batch: 1.187
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 1.4009e-03, 6.8765e-01, 3.2255e-06, 2.6170e-05,
        3.1092e-01], device='cuda:0')
Policy pred: tensor([5.5000e-06, 2.2458e-06, 2.3887e-03, 7.0417e-01, 7.2815e-04, 3.2882e-04,
        2.9238e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9986624717712402
 
[Iteration 3] Process ID: 131596 [Epoch: 147,   128/ 591 points] total loss per batch: 1.232
Policy (actual, predicted): 0 0
Policy data: tensor([0.3128, 0.0000, 0.2017, 0.1191, 0.0000, 0.1891, 0.1773],
       device='cuda:0')
Policy pred: tensor([2.9859e-01, 3.7199e-05, 1.9539e-01, 1.3592e-01, 4.6067e-05, 1.7619e-01,
        1.9382e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 147,   256/ 591 points] total loss per batch: 1.142
Policy (actual, predicted): 1 1
Policy data: tensor([0.1790, 0.2977, 0.1257, 0.0539, 0.1641, 0.0539, 0.1257],
       device='cuda:0')
Policy pred: tensor([0.1908, 0.2821, 0.1288, 0.0559, 0.1647, 0.0514, 0.1262],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 1.0
 
[Iteration 3] Process ID: 131596 [Epoch: 147,   384/ 591 points] total loss per batch: 1.395
Policy (actual, predicted): 4 4
Policy data: tensor([0.1512, 0.1442, 0.1407, 0.1301, 0.1547, 0.1277, 0.1512],
       device='cuda:0')
Policy pred: tensor([0.1473, 0.1423, 0.1378, 0.1310, 0.1539, 0.1426, 0.1451],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 147,   512/ 591 points] total loss per batch: 1.255
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 0.0000e+00, 9.5367e-07, 8.0046e-15, 1.0000e+00, 2.6952e-12,
        4.4319e-07], device='cuda:0')
Policy pred: tensor([6.1612e-07, 1.3324e-08, 9.3901e-06, 3.8716e-07, 9.9992e-01, 2.0601e-06,
        6.3810e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999959468841553
 
[Iteration 3] Process ID: 131596 [Epoch: 148,   128/ 591 points] total loss per batch: 1.385
Policy (actual, predicted): 4 1
Policy data: tensor([0.1466, 0.1466, 0.1477, 0.1325, 0.1524, 0.1325, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1068, 0.4628, 0.1361, 0.0616, 0.0709, 0.0750, 0.0869],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.0032176151871681213
 
[Iteration 3] Process ID: 131596 [Epoch: 148,   256/ 591 points] total loss per batch: 1.204
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.0000, 0.2082, 0.1717, 0.1410, 0.1154, 0.3636],
       device='cuda:0')
Policy pred: tensor([3.3884e-05, 3.7981e-05, 1.9102e-01, 1.9310e-01, 1.4052e-01, 1.4882e-01,
        3.2646e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 148,   384/ 591 points] total loss per batch: 1.202
Policy (actual, predicted): 6 6
Policy data: tensor([0.1932, 0.1356, 0.0778, 0.1239, 0.1621, 0.0778, 0.2296],
       device='cuda:0')
Policy pred: tensor([0.2077, 0.1311, 0.0820, 0.1195, 0.1567, 0.0798, 0.2231],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999852776527405
 
[Iteration 3] Process ID: 131596 [Epoch: 148,   512/ 591 points] total loss per batch: 1.251
Policy (actual, predicted): 0 0
Policy data: tensor([0.3483, 0.0502, 0.0622, 0.1521, 0.0596, 0.0476, 0.2800],
       device='cuda:0')
Policy pred: tensor([0.3671, 0.0517, 0.0539, 0.1520, 0.0429, 0.0435, 0.2889],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.996577262878418
 
[Iteration 3] Process ID: 131596 [Epoch: 149,   128/ 591 points] total loss per batch: 1.314
Policy (actual, predicted): 1 1
Policy data: tensor([0.1395, 0.1489, 0.1442, 0.1465, 0.1372, 0.1383, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1415, 0.1681, 0.1376, 0.1557, 0.1251, 0.1201, 0.1519],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 131596 [Epoch: 149,   256/ 591 points] total loss per batch: 1.171
Policy (actual, predicted): 1 1
Policy data: tensor([1.0917e-12, 1.0000e+00, 1.0766e-17, 4.9118e-20, 2.1140e-09, 5.0296e-17,
        3.5801e-14], device='cuda:0')
Policy pred: tensor([1.1893e-03, 9.9623e-01, 8.5617e-04, 1.5381e-04, 2.9724e-04, 2.7180e-04,
        1.0054e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9997702836990356
 
[Iteration 3] Process ID: 131596 [Epoch: 149,   384/ 591 points] total loss per batch: 1.269
Policy (actual, predicted): 6 2
Policy data: tensor([0.1395, 0.1465, 0.1407, 0.1430, 0.1383, 0.1419, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1274, 0.1385, 0.1602, 0.1361, 0.1512, 0.1292, 0.1574],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9995371103286743
 
[Iteration 3] Process ID: 131596 [Epoch: 149,   512/ 591 points] total loss per batch: 1.254
Policy (actual, predicted): 4 2
Policy data: tensor([0.1454, 0.1442, 0.1465, 0.1360, 0.1477, 0.1360, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1452, 0.1325, 0.1562, 0.1421, 0.1471, 0.1352, 0.1416],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9986510872840881
 
[Iteration 3] Process ID: 131596 [Epoch: 150,   128/ 591 points] total loss per batch: 1.282
Policy (actual, predicted): 1 1
Policy data: tensor([0.0998, 0.6876, 0.0159, 0.0000, 0.0000, 0.1552, 0.0416],
       device='cuda:0')
Policy pred: tensor([9.1190e-02, 6.9942e-01, 1.3648e-02, 5.0036e-05, 1.5443e-05, 1.5118e-01,
        4.4491e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999616146087646
 
[Iteration 3] Process ID: 131596 [Epoch: 150,   256/ 591 points] total loss per batch: 1.154
Policy (actual, predicted): 6 6
Policy data: tensor([0.1141, 0.2796, 0.0190, 0.1674, 0.0000, 0.0975, 0.3223],
       device='cuda:0')
Policy pred: tensor([1.1020e-01, 2.8654e-01, 1.8472e-02, 1.7934e-01, 5.7130e-05, 9.5817e-02,
        3.0956e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999628663063049
 
[Iteration 3] Process ID: 131596 [Epoch: 150,   384/ 591 points] total loss per batch: 1.210
Policy (actual, predicted): 6 6
Policy data: tensor([0.0698, 0.0591, 0.0945, 0.0524, 0.1285, 0.0631, 0.5326],
       device='cuda:0')
Policy pred: tensor([0.0649, 0.0563, 0.0865, 0.0544, 0.1159, 0.0601, 0.5619],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999933838844299
 
[Iteration 3] Process ID: 131596 [Epoch: 150,   512/ 591 points] total loss per batch: 1.333
Policy (actual, predicted): 2 2
Policy data: tensor([0.0762, 0.1530, 0.5734, 0.0074, 0.0775, 0.0894, 0.0231],
       device='cuda:0')
Policy pred: tensor([0.0689, 0.1556, 0.5767, 0.0068, 0.0799, 0.0870, 0.0251],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9984448552131653
 
