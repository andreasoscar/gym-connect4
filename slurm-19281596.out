04/07/2022 12:34:47 AM [INFO]: Preparing model for multi-process MCTS...
04/07/2022 12:34:48 AM [INFO]: Loaded cc4_current_net__iter3.pth.tar model.
04/07/2022 12:34:48 AM [INFO]: Spawning 32 processes...
START TIME:  00:34:43
04/07/2022 12:34:58 AM [INFO]: [CPU: 5]: Starting MCTS self-play...
04/07/2022 12:34:58 AM [INFO]: [CPU: 0]: Starting MCTS self-play...
04/07/2022 12:34:58 AM [INFO]: [CPU: 6]: Starting MCTS self-play...
04/07/2022 12:34:58 AM [INFO]: [CPU: 3]: Starting MCTS self-play...
04/07/2022 12:34:58 AM [INFO]: [CPU: 8]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:34:58 AM [INFO]: [CPU: 5]: Game 0
04/07/2022 12:34:58 AM [INFO]: [CPU: 6]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:34:58 AM [INFO]: [CPU: 8]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:34:58 AM [INFO]: [CPU: 3]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:34:58 AM [INFO]: [CPU: 0]: Game 0
04/07/2022 12:34:58 AM [INFO]: [CPU: 2]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:34:58 AM [INFO]: [CPU: 2]: Game 0
04/07/2022 12:34:58 AM [INFO]: [CPU: 4]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:34:58 AM [INFO]: [CPU: 4]: Game 0
04/07/2022 12:34:58 AM [INFO]: [CPU: 7]: Starting MCTS self-play...
04/07/2022 12:34:58 AM [INFO]: [CPU: 1]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:34:58 AM [INFO]: [CPU: 7]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:34:58 AM [INFO]: [CPU: 1]: Game 0
04/07/2022 12:34:58 AM [INFO]: [CPU: 10]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:34:58 AM [INFO]: [CPU: 10]: Game 0
04/07/2022 12:34:58 AM [INFO]: [CPU: 11]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:34:58 AM [INFO]: [CPU: 11]: Game 0
04/07/2022 12:34:58 AM [INFO]: [CPU: 9]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:34:58 AM [INFO]: [CPU: 9]: Game 0
04/07/2022 12:34:59 AM [INFO]: [CPU: 12]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:34:59 AM [INFO]: [CPU: 12]: Game 0
04/07/2022 12:35:00 AM [INFO]: [CPU: 13]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 13]: Game 0
04/07/2022 12:35:00 AM [INFO]: [CPU: 15]: Starting MCTS self-play...
04/07/2022 12:35:00 AM [INFO]: [CPU: 17]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 15]: Game 0
04/07/2022 12:35:00 AM [INFO]: [CPU: 14]: Starting MCTS self-play...
04/07/2022 12:35:00 AM [INFO]: [CPU: 16]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 17]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 16]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 14]: Game 0
04/07/2022 12:35:00 AM [INFO]: [CPU: 21]: Starting MCTS self-play...
04/07/2022 12:35:00 AM [INFO]: [CPU: 18]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 21]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 18]: Game 0
04/07/2022 12:35:00 AM [INFO]: [CPU: 24]: Starting MCTS self-play...
04/07/2022 12:35:00 AM [INFO]: [CPU: 19]: Starting MCTS self-play...
04/07/2022 12:35:00 AM [INFO]: [CPU: 22]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 24]: Game 0
04/07/2022 12:35:00 AM [INFO]: [CPU: 20]: Starting MCTS self-play...
04/07/2022 12:35:00 AM [INFO]: [CPU: 26]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 22]: Game 0
04/07/2022 12:35:00 AM [INFO]: [CPU: 27]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 26]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 20]: Game 0
04/07/2022 12:35:00 AM [INFO]: [CPU: 29]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 19]: Game 0
04/07/2022 12:35:00 AM [INFO]: [CPU: 30]: Starting MCTS self-play...
04/07/2022 12:35:00 AM [INFO]: [CPU: 23]: Starting MCTS self-play...
04/07/2022 12:35:00 AM [INFO]: [CPU: 25]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 27]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 30]: Game 0
04/07/2022 12:35:00 AM [INFO]: [CPU: 28]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 23]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 29]: Game 0
04/07/2022 12:35:00 AM [INFO]: [CPU: 31]: Starting MCTS self-play...
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 25]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 28]: Game 0
  0%|          | 0/10 [00:00<?, ?it/s]04/07/2022 12:35:00 AM [INFO]: [CPU: 31]: Game 0
  0%|          | 0/10 [00:01<?, ?it/s]  0%|          | 0/10 [00:01<?, ?it/s]
  0%|          | 0/10 [00:01<?, ?it/s]  0%|          | 0/10 [00:01<?, ?it/s]
  0%|          | 0/10 [00:01<?, ?it/s]
  0%|          | 0/10 [00:01<?, ?it/s]

Process Process-28:

Process Process-24:
Process Process-27:
Process Process-16:
Process Process-17:
Process Process-26:
  0%|          | 0/10 [00:03<?, ?it/s]  0%|          | 0/10 [00:03<?, ?it/s]

Process Process-4:
Traceback (most recent call last):
Process Process-10:
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  0%|          | 0/10 [00:03<?, ?it/s]Traceback (most recent call last):

RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
Traceback (most recent call last):
  0%|          | 0/10 [00:03<?, ?it/s]  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)

  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Process Process-3:
Process Process-8:
  0%|          | 0/10 [00:03<?, ?it/s]
  0%|          | 0/10 [00:03<?, ?it/s]
Process Process-7:
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
Traceback (most recent call last):
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Process Process-2:
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  0%|          | 0/10 [00:03<?, ?it/s]
Process Process-9:
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  0%|          | 0/10 [00:03<?, ?it/s]
Process Process-5:
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  0%|          | 0/10 [00:03<?, ?it/s]  0%|          | 0/10 [00:01<?, ?it/s]

Process Process-32:
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
Process Process-12:
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  0%|          | 0/10 [00:02<?, ?it/s]
Process Process-13:
  0%|          | 0/10 [00:01<?, ?it/s]
Process Process-18:
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  0%|          | 0/10 [00:01<?, ?it/s]
Process Process-23:
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  0%|          | 0/10 [00:01<?, ?it/s]
Process Process-21:
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  0%|          | 0/10 [00:01<?, ?it/s]
Process Process-15:
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  0%|          | 0/10 [00:01<?, ?it/s]
Process Process-25:
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  0%|          | 0/10 [00:01<?, ?it/s]
Process Process-29:
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  0%|          | 0/10 [00:01<?, ?it/s]
Process Process-22:
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  0%|          | 0/10 [00:01<?, ?it/s]
Process Process-19:
  0%|          | 0/10 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
Process Process-20:
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  0%|          | 0/10 [00:03<?, ?it/s]
  0%|          | 0/10 [00:03<?, ?it/s]
Process Process-1:
Process Process-11:
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  0%|          | 0/10 [00:01<?, ?it/s]
Process Process-31:
  0%|          | 0/10 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
Process Process-30:
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  0%|          | 0/10 [00:03<?, ?it/s]
Process Process-6:
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  0%|          | 0/10 [00:02<?, ?it/s]
Process Process-14:
Traceback (most recent call last):
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/x_aolss/.conda/envs/myownenv/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 187, in MCTS_self_play
    root = UCT_search(current_board,777,connectnet,t)
  File "/home/x_aolss/gym_connect4/gym-connect4/MCTS_c4.py", line 136, in UCT_search
    encoded_s = torch.from_numpy(encoded_s).cuda().float()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
04/07/2022 12:35:07 AM [INFO]: Finished multi-process MCTS!
04/07/2022 12:35:07 AM [INFO]: Loading training data...
/home/x_aolss/gym_connect4/gym-connect4/train_c4.py:141: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  datasets = np.array(datasets)
04/07/2022 12:35:07 AM [INFO]: Loaded data from ./datasets/iter_3/.
04/07/2022 12:35:08 AM [INFO]: Loaded checkpoint model cc4_current_net__iter3.pth.tar.
04/07/2022 12:35:08 AM [INFO]: Starting training process...
04/07/2022 12:36:29 AM [INFO]: Finished Training!
FINISHED SELF PLAY:  00:35:07
Update step size: 4
[Iteration 3] Process ID: 188609 [Epoch: 1,   128/ 591 points] total loss per batch: 2.525
Policy (actual, predicted): 5 3
Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0048, 0.0000, 0.9952, 0.0000],
       device='cuda:0')
Policy pred: tensor([0.1040, 0.0175, 0.1006, 0.5897, 0.0297, 0.0837, 0.0748],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.998031497001648
 
[Iteration 3] Process ID: 188609 [Epoch: 1,   256/ 591 points] total loss per batch: 2.283
Policy (actual, predicted): 6 2
Policy data: tensor([0.1360, 0.1442, 0.1419, 0.1419, 0.1454, 0.1383, 0.1524],
       device='cuda:0')
Policy pred: tensor([0.1951, 0.1591, 0.1999, 0.0787, 0.1287, 0.1088, 0.1297],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.5478475093841553
 
[Iteration 3] Process ID: 188609 [Epoch: 1,   384/ 591 points] total loss per batch: 1.937
Policy (actual, predicted): 0 2
Policy data: tensor([0.6687, 0.0664, 0.1092, 0.0295, 0.0412, 0.0526, 0.0324],
       device='cuda:0')
Policy pred: tensor([0.1836, 0.1536, 0.2475, 0.0771, 0.0862, 0.1171, 0.1348],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9305500388145447
 
[Iteration 3] Process ID: 188609 [Epoch: 1,   512/ 591 points] total loss per batch: 1.986
Policy (actual, predicted): 6 6
Policy data: tensor([0.0935, 0.1762, 0.1476, 0.1350, 0.1613, 0.0579, 0.2285],
       device='cuda:0')
Policy pred: tensor([0.0737, 0.2075, 0.1351, 0.1839, 0.1110, 0.0410, 0.2476],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.8285431265830994
 
[Iteration 3] Process ID: 188609 [Epoch: 2,   128/ 591 points] total loss per batch: 1.661
Policy (actual, predicted): 4 0
Policy data: tensor([0.1419, 0.1454, 0.1407, 0.1407, 0.1489, 0.1360, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.2409, 0.1222, 0.2069, 0.0724, 0.1025, 0.0941, 0.1611],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.158927321434021
 
[Iteration 3] Process ID: 188609 [Epoch: 2,   256/ 591 points] total loss per batch: 1.974
Policy (actual, predicted): 2 2
Policy data: tensor([0.2808, 0.0266, 0.5874, 0.0173, 0.0220, 0.0471, 0.0189],
       device='cuda:0')
Policy pred: tensor([0.1983, 0.0853, 0.3916, 0.0368, 0.1258, 0.0747, 0.0875],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.7276781797409058
 
[Iteration 3] Process ID: 188609 [Epoch: 2,   384/ 591 points] total loss per batch: 1.796
Policy (actual, predicted): 1 1
Policy data: tensor([4.3506e-03, 9.6874e-01, 3.9572e-03, 1.5618e-04, 1.6343e-02, 1.5618e-04,
        6.3011e-03], device='cuda:0')
Policy pred: tensor([0.1167, 0.3492, 0.0648, 0.1302, 0.0929, 0.1158, 0.1304],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999971330165863
 
[Iteration 3] Process ID: 188609 [Epoch: 2,   512/ 591 points] total loss per batch: 1.807
Policy (actual, predicted): 6 0
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.2807, 0.0895, 0.2228, 0.0333, 0.1082, 0.0703, 0.1952],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.3441694974899292
 
[Iteration 3] Process ID: 188609 [Epoch: 3,   128/ 591 points] total loss per batch: 1.737
Policy (actual, predicted): 1 4
Policy data: tensor([0.0650, 0.3209, 0.0485, 0.1777, 0.1139, 0.1247, 0.1491],
       device='cuda:0')
Policy pred: tensor([0.0523, 0.1603, 0.0722, 0.1895, 0.2259, 0.1516, 0.1482],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999953508377075
 
[Iteration 3] Process ID: 188609 [Epoch: 3,   256/ 591 points] total loss per batch: 1.719
Policy (actual, predicted): 0 0
Policy data: tensor([0.2785, 0.1397, 0.1668, 0.0548, 0.1527, 0.0548, 0.1527],
       device='cuda:0')
Policy pred: tensor([0.2044, 0.1483, 0.1469, 0.1415, 0.1319, 0.1011, 0.1259],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999927878379822
 
[Iteration 3] Process ID: 188609 [Epoch: 3,   384/ 591 points] total loss per batch: 1.701
Policy (actual, predicted): 3 6
Policy data: tensor([0.0729, 0.1156, 0.0729, 0.3226, 0.1056, 0.0368, 0.2738],
       device='cuda:0')
Policy pred: tensor([0.1410, 0.1556, 0.0732, 0.1961, 0.1211, 0.0417, 0.2714],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999837279319763
 
[Iteration 3] Process ID: 188609 [Epoch: 3,   512/ 591 points] total loss per batch: 1.747
Policy (actual, predicted): 4 2
Policy data: tensor([0.1477, 0.1466, 0.1477, 0.1277, 0.1524, 0.1325, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1302, 0.1411, 0.2837, 0.0940, 0.1027, 0.1192, 0.1291],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9987937211990356
 
[Iteration 3] Process ID: 188609 [Epoch: 4,   128/ 591 points] total loss per batch: 1.544
Policy (actual, predicted): 5 0
Policy data: tensor([0.1119, 0.1398, 0.1302, 0.0668, 0.0871, 0.3796, 0.0846],
       device='cuda:0')
Policy pred: tensor([0.2090, 0.1241, 0.1356, 0.0776, 0.1026, 0.1778, 0.1733],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9901352524757385
 
[Iteration 3] Process ID: 188609 [Epoch: 4,   256/ 591 points] total loss per batch: 1.668
Policy (actual, predicted): 0 2
Policy data: tensor([0.4173, 0.0708, 0.3789, 0.0123, 0.0154, 0.0748, 0.0305],
       device='cuda:0')
Policy pred: tensor([0.1884, 0.1124, 0.3594, 0.0304, 0.0383, 0.0712, 0.1999],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.13301676511764526
 
[Iteration 3] Process ID: 188609 [Epoch: 4,   384/ 591 points] total loss per batch: 1.633
Policy (actual, predicted): 2 4
Policy data: tensor([0.0767, 0.2093, 0.5781, 0.0074, 0.0686, 0.0336, 0.0263],
       device='cuda:0')
Policy pred: tensor([0.1014, 0.2003, 0.1040, 0.0698, 0.3185, 0.1027, 0.1033],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.978481113910675
 
[Iteration 3] Process ID: 188609 [Epoch: 4,   512/ 591 points] total loss per batch: 1.616
Policy (actual, predicted): 1 3
Policy data: tensor([0.0000, 0.3818, 0.0441, 0.3392, 0.0833, 0.1516, 0.0000],
       device='cuda:0')
Policy pred: tensor([5.6846e-05, 1.1783e-01, 3.4031e-02, 6.2958e-01, 4.7326e-02, 1.7021e-01,
        9.6960e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999957084655762
 
[Iteration 3] Process ID: 188609 [Epoch: 5,   128/ 591 points] total loss per batch: 1.566
Policy (actual, predicted): 3 6
Policy data: tensor([0.1339, 0.0000, 0.1733, 0.3619, 0.0000, 0.0283, 0.3026],
       device='cuda:0')
Policy pred: tensor([2.2923e-01, 4.9690e-04, 1.9605e-01, 9.4664e-02, 3.2171e-06, 1.2472e-02,
        4.6708e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999892115592957
 
[Iteration 3] Process ID: 188609 [Epoch: 5,   256/ 591 points] total loss per batch: 1.657
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 1.4355e-15, 1.8845e-19, 7.8128e-12, 1.5633e-17,
        5.8033e-20], device='cuda:0')
Policy pred: tensor([8.0662e-05, 8.3081e-01, 2.8930e-02, 1.6204e-03, 1.3008e-01, 2.7374e-03,
        5.7389e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998385310173035
 
[Iteration 3] Process ID: 188609 [Epoch: 5,   384/ 591 points] total loss per batch: 1.529
Policy (actual, predicted): 1 1
Policy data: tensor([0.0934, 0.6501, 0.1337, 0.0188, 0.0173, 0.0412, 0.0455],
       device='cuda:0')
Policy pred: tensor([0.1589, 0.2855, 0.1912, 0.0777, 0.0741, 0.0902, 0.1224],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.0038336848374456167
 
[Iteration 3] Process ID: 188609 [Epoch: 5,   512/ 591 points] total loss per batch: 1.575
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 4.8059e-10, 1.6616e-09, 2.7480e-17, 1.0000e+00, 5.0091e-09,
        4.7592e-06], device='cuda:0')
Policy pred: tensor([0.0163, 0.1335, 0.0719, 0.0178, 0.4626, 0.0563, 0.2416],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999925494194031
 
[Iteration 3] Process ID: 188609 [Epoch: 6,   128/ 591 points] total loss per batch: 1.569
Policy (actual, predicted): 0 0
Policy data: tensor([0.8966, 0.0098, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.7583, 0.0271, 0.0425, 0.0278, 0.0498, 0.0331, 0.0615],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.16746196150779724
 
[Iteration 3] Process ID: 188609 [Epoch: 6,   256/ 591 points] total loss per batch: 1.466
Policy (actual, predicted): 6 0
Policy data: tensor([2.9859e-01, 9.0344e-02, 3.8911e-03, 1.2060e-04, 1.5514e-01, 2.5303e-04,
        4.5166e-01], device='cuda:0')
Policy pred: tensor([0.2907, 0.1403, 0.0394, 0.0242, 0.2710, 0.0201, 0.2144],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999985933303833
 
[Iteration 3] Process ID: 188609 [Epoch: 6,   384/ 591 points] total loss per batch: 1.512
Policy (actual, predicted): 6 6
Policy data: tensor([0.0480, 0.0177, 0.0193, 0.0111, 0.0509, 0.1023, 0.7508],
       device='cuda:0')
Policy pred: tensor([0.1249, 0.0592, 0.0654, 0.0342, 0.1401, 0.0589, 0.5173],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9885281920433044
 
[Iteration 3] Process ID: 188609 [Epoch: 6,   512/ 591 points] total loss per batch: 1.571
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1550, 0.3120, 0.1757, 0.0768, 0.0621, 0.0943, 0.1242],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.029359707608819008
 
[Iteration 3] Process ID: 188609 [Epoch: 7,   128/ 591 points] total loss per batch: 1.533
Policy (actual, predicted): 4 1
Policy data: tensor([0.1466, 0.1454, 0.1512, 0.1277, 0.1536, 0.1336, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1417, 0.4258, 0.1412, 0.0586, 0.0585, 0.0651, 0.1091],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.008462763391435146
 
[Iteration 3] Process ID: 188609 [Epoch: 7,   256/ 591 points] total loss per batch: 1.433
Policy (actual, predicted): 1 0
Policy data: tensor([0.1140, 0.2503, 0.1140, 0.1492, 0.0864, 0.0359, 0.2503],
       device='cuda:0')
Policy pred: tensor([0.2228, 0.1855, 0.1485, 0.1296, 0.0703, 0.1120, 0.1313],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9996262192726135
 
[Iteration 3] Process ID: 188609 [Epoch: 7,   384/ 591 points] total loss per batch: 1.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0081, 0.0135, 0.9478, 0.0063, 0.0063, 0.0118, 0.0063],
       device='cuda:0')
Policy pred: tensor([0.1396, 0.0670, 0.4511, 0.0584, 0.0732, 0.1170, 0.0937],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9990417957305908
 
[Iteration 3] Process ID: 188609 [Epoch: 7,   512/ 591 points] total loss per batch: 1.445
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.0000, 0.2066, 0.1309, 0.0933, 0.2496, 0.3196],
       device='cuda:0')
Policy pred: tensor([8.9532e-06, 1.5438e-04, 1.5665e-01, 1.2486e-01, 1.2658e-01, 1.0359e-01,
        4.8815e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999992251396179
 
[Iteration 3] Process ID: 188609 [Epoch: 8,   128/ 591 points] total loss per batch: 1.523
Policy (actual, predicted): 0 0
Policy data: tensor([0.6231, 0.1329, 0.1020, 0.0218, 0.0187, 0.0494, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.4343, 0.1621, 0.1957, 0.0286, 0.0395, 0.0438, 0.0961],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.3733278512954712
 
[Iteration 3] Process ID: 188609 [Epoch: 8,   256/ 591 points] total loss per batch: 1.454
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 0.0000e+00, 4.3964e-19, 8.8631e-17, 2.7601e-13,
        6.2062e-15], device='cuda:0')
Policy pred: tensor([0.0015, 0.7818, 0.0015, 0.1248, 0.0686, 0.0020, 0.0199],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9996287822723389
 
[Iteration 3] Process ID: 188609 [Epoch: 8,   384/ 591 points] total loss per batch: 1.403
Policy (actual, predicted): 6 6
Policy data: tensor([6.0353e-19, 0.0000e+00, 0.0000e+00, 1.4753e-15, 2.2723e-22, 1.4070e-21,
        1.0000e+00], device='cuda:0')
Policy pred: tensor([2.3418e-03, 6.1252e-06, 9.1178e-04, 3.9899e-02, 7.3710e-03, 9.8144e-04,
        9.4849e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9852539300918579
 
[Iteration 3] Process ID: 188609 [Epoch: 8,   512/ 591 points] total loss per batch: 1.451
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000e+00, 0.0000e+00, 1.0000e+00, 5.1967e-16, 3.1903e-16, 4.3982e-15,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([2.7021e-06, 8.6494e-05, 9.8971e-01, 1.6074e-03, 3.7242e-03, 1.3548e-03,
        3.5130e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999144077301025
 
[Iteration 3] Process ID: 188609 [Epoch: 9,   128/ 591 points] total loss per batch: 1.375
Policy (actual, predicted): 3 2
Policy data: tensor([0.1339, 0.0000, 0.1733, 0.3619, 0.0000, 0.0283, 0.3026],
       device='cuda:0')
Policy pred: tensor([7.1933e-02, 3.3470e-04, 3.5638e-01, 2.5544e-01, 1.1463e-04, 5.6894e-02,
        2.5891e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998791217803955
 
[Iteration 3] Process ID: 188609 [Epoch: 9,   256/ 591 points] total loss per batch: 1.413
Policy (actual, predicted): 1 3
Policy data: tensor([0.1442, 0.1477, 0.1336, 0.1419, 0.1395, 0.1454, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1610, 0.1467, 0.1114, 0.2004, 0.1144, 0.1188, 0.1473],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999266266822815
 
[Iteration 3] Process ID: 188609 [Epoch: 9,   384/ 591 points] total loss per batch: 1.428
Policy (actual, predicted): 3 6
Policy data: tensor([0.0000e+00, 0.0000e+00, 1.4009e-03, 6.8765e-01, 3.2255e-06, 2.6170e-05,
        3.1092e-01], device='cuda:0')
Policy pred: tensor([6.0774e-05, 3.3717e-04, 9.5892e-03, 2.7806e-01, 1.0686e-02, 1.5240e-03,
        6.9974e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9997137188911438
 
[Iteration 3] Process ID: 188609 [Epoch: 9,   512/ 591 points] total loss per batch: 1.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.1536, 0.1419, 0.1454, 0.1289, 0.1501, 0.1372, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.2864, 0.2113, 0.1602, 0.0954, 0.0825, 0.0632, 0.1010],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.8797835111618042
 
[Iteration 3] Process ID: 188609 [Epoch: 10,   128/ 591 points] total loss per batch: 1.436
Policy (actual, predicted): 1 6
Policy data: tensor([0.1383, 0.1477, 0.1372, 0.1442, 0.1442, 0.1419, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.0995, 0.0682, 0.0702, 0.1163, 0.2008, 0.1031, 0.3420],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.014174778945744038
 
[Iteration 3] Process ID: 188609 [Epoch: 10,   256/ 591 points] total loss per batch: 1.445
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000e+00, 0.0000e+00, 6.5395e-09, 2.8452e-17, 2.0306e-13, 1.0000e+00,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([2.1079e-04, 2.9403e-05, 3.3370e-02, 8.2739e-03, 1.8433e-02, 9.3673e-01,
        2.9498e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.99961918592453
 
[Iteration 3] Process ID: 188609 [Epoch: 10,   384/ 591 points] total loss per batch: 1.375
Policy (actual, predicted): 2 2
Policy data: tensor([0.2808, 0.0266, 0.5874, 0.0173, 0.0220, 0.0471, 0.0189],
       device='cuda:0')
Policy pred: tensor([0.1759, 0.0620, 0.5209, 0.0271, 0.0828, 0.0440, 0.0873],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.966563880443573
 
[Iteration 3] Process ID: 188609 [Epoch: 10,   512/ 591 points] total loss per batch: 1.403
Policy (actual, predicted): 6 5
Policy data: tensor([0.1407, 0.1419, 0.1442, 0.1372, 0.1454, 0.1325, 0.1582],
       device='cuda:0')
Policy pred: tensor([0.1297, 0.0637, 0.1005, 0.1365, 0.2052, 0.2130, 0.1513],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9841036796569824
 
[Iteration 3] Process ID: 188609 [Epoch: 11,   128/ 591 points] total loss per batch: 1.286
Policy (actual, predicted): 4 1
Policy data: tensor([0.1466, 0.1466, 0.1477, 0.1325, 0.1524, 0.1325, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1469, 0.3972, 0.1324, 0.0655, 0.0896, 0.0818, 0.0867],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.023146405816078186
 
[Iteration 3] Process ID: 188609 [Epoch: 11,   256/ 591 points] total loss per batch: 1.369
Policy (actual, predicted): 6 1
Policy data: tensor([0.1360, 0.1512, 0.1442, 0.1419, 0.1430, 0.1313, 0.1524],
       device='cuda:0')
Policy pred: tensor([0.1221, 0.1886, 0.1616, 0.1451, 0.1193, 0.1183, 0.1450],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9621990323066711
 
[Iteration 3] Process ID: 188609 [Epoch: 11,   384/ 591 points] total loss per batch: 1.384
Policy (actual, predicted): 6 2
Policy data: tensor([0.1383, 0.1454, 0.1395, 0.1407, 0.1465, 0.1407, 0.1489],
       device='cuda:0')
Policy pred: tensor([0.1587, 0.1633, 0.2426, 0.0754, 0.1340, 0.0677, 0.1581],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.7958871126174927
 
[Iteration 3] Process ID: 188609 [Epoch: 11,   512/ 591 points] total loss per batch: 1.437
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1096, 0.3731, 0.1726, 0.0736, 0.0890, 0.0754, 0.1067],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.0035086055286228657
 
[Iteration 3] Process ID: 188609 [Epoch: 12,   128/ 591 points] total loss per batch: 1.321
Policy (actual, predicted): 4 1
Policy data: tensor([0.1454, 0.1489, 0.1477, 0.1301, 0.1559, 0.1325, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1348, 0.3616, 0.1528, 0.0647, 0.0907, 0.0894, 0.1060],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.009913469664752483
 
[Iteration 3] Process ID: 188609 [Epoch: 12,   256/ 591 points] total loss per batch: 1.378
Policy (actual, predicted): 2 2
Policy data: tensor([0.1977, 0.1435, 0.5532, 0.0247, 0.0232, 0.0407, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.2086, 0.2268, 0.4753, 0.0078, 0.0172, 0.0449, 0.0195],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9495953917503357
 
[Iteration 3] Process ID: 188609 [Epoch: 12,   384/ 591 points] total loss per batch: 1.457
Policy (actual, predicted): 6 3
Policy data: tensor([0.1430, 0.1466, 0.1384, 0.1454, 0.1360, 0.1313, 0.1594],
       device='cuda:0')
Policy pred: tensor([0.1723, 0.0410, 0.1203, 0.2123, 0.1478, 0.1751, 0.1312],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999978542327881
 
[Iteration 3] Process ID: 188609 [Epoch: 12,   512/ 591 points] total loss per batch: 1.490
Policy (actual, predicted): 4 6
Policy data: tensor([0.1419, 0.1454, 0.1407, 0.1407, 0.1489, 0.1360, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1160, 0.1716, 0.0545, 0.0808, 0.0884, 0.0859, 0.4028],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.5365077257156372
 
[Iteration 3] Process ID: 188609 [Epoch: 13,   128/ 591 points] total loss per batch: 1.290
Policy (actual, predicted): 1 1
Policy data: tensor([2.9505e-19, 9.9981e-01, 6.4039e-15, 1.9154e-04, 6.0232e-16, 1.0200e-20,
        6.4039e-15], device='cuda:0')
Policy pred: tensor([0.0178, 0.6890, 0.0664, 0.1146, 0.0021, 0.0757, 0.0344],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998599290847778
 
[Iteration 3] Process ID: 188609 [Epoch: 13,   256/ 591 points] total loss per batch: 1.385
Policy (actual, predicted): 0 0
Policy data: tensor([0.2442, 0.1105, 0.2058, 0.0569, 0.1584, 0.0918, 0.1325],
       device='cuda:0')
Policy pred: tensor([0.2348, 0.2023, 0.1541, 0.0574, 0.1122, 0.0679, 0.1713],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999885559082031
 
[Iteration 3] Process ID: 188609 [Epoch: 13,   384/ 591 points] total loss per batch: 1.399
Policy (actual, predicted): 1 1
Policy data: tensor([0.0934, 0.6501, 0.1337, 0.0188, 0.0173, 0.0412, 0.0455],
       device='cuda:0')
Policy pred: tensor([0.1122, 0.4154, 0.1292, 0.0695, 0.0807, 0.0953, 0.0977],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.035147517919540405
 
[Iteration 3] Process ID: 188609 [Epoch: 13,   512/ 591 points] total loss per batch: 1.360
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 1.7811e-07, 3.3510e-18, 3.6495e-20, 7.6289e-24, 7.6289e-24,
        3.6495e-20], device='cuda:0')
Policy pred: tensor([0.7424, 0.1489, 0.0070, 0.0081, 0.0303, 0.0181, 0.0453],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9930205941200256
 
[Iteration 3] Process ID: 188609 [Epoch: 14,   128/ 591 points] total loss per batch: 1.353
Policy (actual, predicted): 4 4
Policy data: tensor([0.1442, 0.1383, 0.1442, 0.1325, 0.1559, 0.1372, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1457, 0.1726, 0.1079, 0.1203, 0.1805, 0.1363, 0.1368],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.99971604347229
 
[Iteration 3] Process ID: 188609 [Epoch: 14,   256/ 591 points] total loss per batch: 1.324
Policy (actual, predicted): 6 6
Policy data: tensor([0.1145, 0.0262, 0.0247, 0.0217, 0.2126, 0.0364, 0.5639],
       device='cuda:0')
Policy pred: tensor([0.0718, 0.1810, 0.0579, 0.0201, 0.0825, 0.1018, 0.4849],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9749608635902405
 
[Iteration 3] Process ID: 188609 [Epoch: 14,   384/ 591 points] total loss per batch: 1.414
Policy (actual, predicted): 4 4
Policy data: tensor([0.1533, 0.1533, 0.0975, 0.0550, 0.2571, 0.0668, 0.2170],
       device='cuda:0')
Policy pred: tensor([0.0891, 0.1371, 0.0744, 0.0553, 0.3398, 0.0686, 0.2358],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999905228614807
 
[Iteration 3] Process ID: 188609 [Epoch: 14,   512/ 591 points] total loss per batch: 1.422
Policy (actual, predicted): 2 2
Policy data: tensor([0.1415, 0.0489, 0.5765, 0.0170, 0.0419, 0.0760, 0.0983],
       device='cuda:0')
Policy pred: tensor([0.2394, 0.0220, 0.5001, 0.0153, 0.1048, 0.0441, 0.0741],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9901259541511536
 
[Iteration 3] Process ID: 188609 [Epoch: 15,   128/ 591 points] total loss per batch: 1.385
Policy (actual, predicted): 4 4
Policy data: tensor([0.2080, 0.2265, 0.1344, 0.0433, 0.2464, 0.0775, 0.0640],
       device='cuda:0')
Policy pred: tensor([0.1810, 0.2145, 0.0887, 0.0785, 0.2740, 0.0826, 0.0807],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999954342842102
 
[Iteration 3] Process ID: 188609 [Epoch: 15,   256/ 591 points] total loss per batch: 1.162
Policy (actual, predicted): 2 2
Policy data: tensor([0.0146, 0.0715, 0.7924, 0.0112, 0.0162, 0.0730, 0.0211],
       device='cuda:0')
Policy pred: tensor([0.0342, 0.0152, 0.9215, 0.0018, 0.0042, 0.0145, 0.0087],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9921631217002869
 
[Iteration 3] Process ID: 188609 [Epoch: 15,   384/ 591 points] total loss per batch: 1.456
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.3345, 0.2337, 0.0341, 0.2172, 0.0621, 0.1185],
       device='cuda:0')
Policy pred: tensor([2.6320e-04, 4.0883e-01, 1.6298e-01, 3.4735e-02, 1.6728e-01, 8.1060e-02,
        1.4485e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999922513961792
 
[Iteration 3] Process ID: 188609 [Epoch: 15,   512/ 591 points] total loss per batch: 1.345
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 4.6359e-12, 0.0000e+00, 1.6805e-17, 0.0000e+00, 3.7417e-17,
        6.7988e-07], device='cuda:0')
Policy pred: tensor([9.7873e-01, 9.5290e-03, 3.7235e-05, 2.0984e-03, 8.0769e-06, 1.9106e-04,
        9.4102e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999961256980896
 
[Iteration 3] Process ID: 188609 [Epoch: 16,   128/ 591 points] total loss per batch: 1.410
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0109, 0.0209, 0.0218, 0.0051, 0.0261, 0.0392, 0.8761],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.2949925363063812
 
[Iteration 3] Process ID: 188609 [Epoch: 16,   256/ 591 points] total loss per batch: 1.217
Policy (actual, predicted): 2 2
Policy data: tensor([0.0774, 0.0375, 0.7471, 0.0285, 0.0330, 0.0375, 0.0390],
       device='cuda:0')
Policy pred: tensor([0.1464, 0.0310, 0.6506, 0.0280, 0.0231, 0.0471, 0.0737],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9897111058235168
 
[Iteration 3] Process ID: 188609 [Epoch: 16,   384/ 591 points] total loss per batch: 1.329
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.1822, 0.4010, 0.0431, 0.2273, 0.0556, 0.0909],
       device='cuda:0')
Policy pred: tensor([1.5877e-04, 2.3954e-01, 2.1097e-01, 8.1607e-02, 2.7999e-01, 6.5663e-02,
        1.2207e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999990463256836
 
[Iteration 3] Process ID: 188609 [Epoch: 16,   512/ 591 points] total loss per batch: 1.418
Policy (actual, predicted): 2 2
Policy data: tensor([0.1917, 0.1435, 0.5590, 0.0247, 0.0232, 0.0407, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.1960, 0.1311, 0.4908, 0.0285, 0.0200, 0.0843, 0.0493],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9938530921936035
 
[Iteration 3] Process ID: 188609 [Epoch: 17,   128/ 591 points] total loss per batch: 1.244
Policy (actual, predicted): 1 1
Policy data: tensor([4.2423e-17, 1.0000e+00, 2.3890e-18, 2.3656e-16, 1.1161e-17, 3.8584e-19,
        1.0008e-18], device='cuda:0')
Policy pred: tensor([0.0031, 0.9776, 0.0071, 0.0042, 0.0037, 0.0033, 0.0011],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9604439735412598
 
[Iteration 3] Process ID: 188609 [Epoch: 17,   256/ 591 points] total loss per batch: 1.225
Policy (actual, predicted): 6 6
Policy data: tensor([0.2673, 0.0000, 0.1837, 0.0324, 0.0000, 0.1958, 0.3208],
       device='cuda:0')
Policy pred: tensor([2.1833e-01, 1.9924e-05, 1.4018e-01, 1.0979e-01, 1.6808e-06, 2.0698e-01,
        3.2471e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999993443489075
 
[Iteration 3] Process ID: 188609 [Epoch: 17,   384/ 591 points] total loss per batch: 1.366
Policy (actual, predicted): 6 0
Policy data: tensor([0.3659, 0.0000, 0.0484, 0.0321, 0.0000, 0.0000, 0.5535],
       device='cuda:0')
Policy pred: tensor([5.4560e-01, 3.6352e-05, 7.9143e-02, 2.8123e-02, 7.7144e-07, 3.0861e-04,
        3.4678e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999477863311768
 
[Iteration 3] Process ID: 188609 [Epoch: 17,   512/ 591 points] total loss per batch: 1.433
Policy (actual, predicted): 4 4
Policy data: tensor([0.1017, 0.1906, 0.0697, 0.0768, 0.2265, 0.1600, 0.1747],
       device='cuda:0')
Policy pred: tensor([0.0454, 0.1010, 0.0601, 0.0839, 0.2443, 0.2253, 0.2400],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999204277992249
 
[Iteration 3] Process ID: 188609 [Epoch: 18,   128/ 591 points] total loss per batch: 1.281
Policy (actual, predicted): 1 6
Policy data: tensor([8.3551e-02, 5.2405e-01, 3.2238e-04, 4.7825e-02, 1.4135e-02, 6.8652e-07,
        3.3012e-01], device='cuda:0')
Policy pred: tensor([0.0968, 0.3621, 0.0048, 0.0322, 0.0222, 0.0014, 0.4805],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999886155128479
 
[Iteration 3] Process ID: 188609 [Epoch: 18,   256/ 591 points] total loss per batch: 1.304
Policy (actual, predicted): 2 2
Policy data: tensor([3.2442e-01, 2.0857e-02, 5.1191e-01, 1.1861e-05, 1.4092e-01, 3.1682e-04,
        1.5668e-03], device='cuda:0')
Policy pred: tensor([0.1718, 0.0894, 0.5835, 0.0191, 0.1058, 0.0124, 0.0180],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9990519285202026
 
[Iteration 3] Process ID: 188609 [Epoch: 18,   384/ 591 points] total loss per batch: 1.360
Policy (actual, predicted): 0 0
Policy data: tensor([0.6244, 0.1329, 0.1033, 0.0218, 0.0187, 0.0466, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.6383, 0.1426, 0.1069, 0.0170, 0.0150, 0.0364, 0.0439],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.04657856374979019
 
[Iteration 3] Process ID: 188609 [Epoch: 18,   512/ 591 points] total loss per batch: 1.439
Policy (actual, predicted): 2 2
Policy data: tensor([1.9169e-19, 2.4568e-10, 1.0000e+00, 5.5242e-12, 6.3201e-07, 6.6271e-21,
        1.8024e-14], device='cuda:0')
Policy pred: tensor([0.0123, 0.1756, 0.7380, 0.0590, 0.0045, 0.0031, 0.0075],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999649524688721
 
[Iteration 3] Process ID: 188609 [Epoch: 19,   128/ 591 points] total loss per batch: 1.296
Policy (actual, predicted): 0 0
Policy data: tensor([0.4737, 0.0441, 0.4088, 0.0092, 0.0125, 0.0250, 0.0265],
       device='cuda:0')
Policy pred: tensor([0.7602, 0.0198, 0.2019, 0.0028, 0.0031, 0.0061, 0.0063],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9990965127944946
 
[Iteration 3] Process ID: 188609 [Epoch: 19,   256/ 591 points] total loss per batch: 1.327
Policy (actual, predicted): 4 6
Policy data: tensor([0.1477, 0.1477, 0.1442, 0.1313, 0.1535, 0.1336, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1056, 0.0967, 0.1705, 0.1398, 0.1717, 0.0947, 0.2210],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9994144439697266
 
[Iteration 3] Process ID: 188609 [Epoch: 19,   384/ 591 points] total loss per batch: 1.306
Policy (actual, predicted): 2 2
Policy data: tensor([1.1003e-06, 2.9133e-06, 9.9991e-01, 3.8581e-19, 7.9434e-05, 1.0247e-15,
        1.9120e-06], device='cuda:0')
Policy pred: tensor([0.0018, 0.0102, 0.9491, 0.0046, 0.0290, 0.0023, 0.0029],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998753070831299
 
[Iteration 3] Process ID: 188609 [Epoch: 19,   512/ 591 points] total loss per batch: 1.309
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 4.8059e-10, 1.6616e-09, 2.7480e-17, 1.0000e+00, 5.0091e-09,
        4.7592e-06], device='cuda:0')
Policy pred: tensor([3.1873e-05, 1.9286e-02, 5.2735e-03, 1.1055e-03, 9.3200e-01, 3.2338e-03,
        3.9071e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999996423721313
 
[Iteration 3] Process ID: 188609 [Epoch: 20,   128/ 591 points] total loss per batch: 1.262
Policy (actual, predicted): 4 4
Policy data: tensor([1.4790e-15, 5.9791e-20, 0.0000e+00, 1.6494e-14, 1.0000e+00, 1.9882e-16,
        4.5641e-14], device='cuda:0')
Policy pred: tensor([1.1954e-02, 2.5537e-03, 5.7036e-04, 1.0043e-03, 9.8238e-01, 8.7374e-04,
        6.6489e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999963104724884
 
[Iteration 3] Process ID: 188609 [Epoch: 20,   256/ 591 points] total loss per batch: 1.334
Policy (actual, predicted): 4 4
Policy data: tensor([0.1659, 0.0505, 0.1521, 0.0340, 0.3792, 0.0376, 0.1807],
       device='cuda:0')
Policy pred: tensor([0.1753, 0.0573, 0.1402, 0.0605, 0.4076, 0.0380, 0.1211],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998211860657
 
[Iteration 3] Process ID: 188609 [Epoch: 20,   384/ 591 points] total loss per batch: 1.165
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 0.0000e+00, 4.7631e-08, 0.0000e+00, 2.2985e-07,
        4.4360e-07], device='cuda:0')
Policy pred: tensor([4.4464e-06, 9.7106e-01, 2.6906e-04, 2.7882e-04, 1.9125e-06, 9.1075e-04,
        2.7471e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999974370002747
 
[Iteration 3] Process ID: 188609 [Epoch: 20,   512/ 591 points] total loss per batch: 1.331
Policy (actual, predicted): 2 1
Policy data: tensor([0.1465, 0.1360, 0.1489, 0.1348, 0.1477, 0.1372, 0.1489],
       device='cuda:0')
Policy pred: tensor([0.1162, 0.1876, 0.1365, 0.1192, 0.1401, 0.1414, 0.1590],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9993550777435303
 
[Iteration 3] Process ID: 188609 [Epoch: 21,   128/ 591 points] total loss per batch: 1.333
Policy (actual, predicted): 0 0
Policy data: tensor([0.4774, 0.0218, 0.3718, 0.0156, 0.0743, 0.0188, 0.0203],
       device='cuda:0')
Policy pred: tensor([0.5431, 0.0515, 0.2821, 0.0094, 0.0744, 0.0227, 0.0169],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.998756468296051
 
[Iteration 3] Process ID: 188609 [Epoch: 21,   256/ 591 points] total loss per batch: 1.366
Policy (actual, predicted): 4 4
Policy data: tensor([0.1998, 0.1998, 0.1681, 0.0306, 0.2573, 0.0459, 0.0984],
       device='cuda:0')
Policy pred: tensor([0.2065, 0.2304, 0.1521, 0.0244, 0.2384, 0.0382, 0.1099],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999904632568359
 
[Iteration 3] Process ID: 188609 [Epoch: 21,   384/ 591 points] total loss per batch: 1.181
Policy (actual, predicted): 0 0
Policy data: tensor([9.7685e-01, 0.0000e+00, 2.5326e-03, 5.5271e-05, 5.5271e-05, 1.6543e-05,
        2.0495e-02], device='cuda:0')
Policy pred: tensor([9.1847e-01, 1.8777e-05, 2.6919e-03, 6.2730e-03, 1.2966e-03, 8.2222e-03,
        6.3028e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999971389770508
 
[Iteration 3] Process ID: 188609 [Epoch: 21,   512/ 591 points] total loss per batch: 1.352
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0086, 0.0105, 0.0154, 0.0055, 0.0159, 0.0314, 0.9127],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.2728936970233917
 
[Iteration 3] Process ID: 188609 [Epoch: 22,   128/ 591 points] total loss per batch: 1.214
Policy (actual, predicted): 1 1
Policy data: tensor([0.0934, 0.6501, 0.1337, 0.0188, 0.0173, 0.0412, 0.0455],
       device='cuda:0')
Policy pred: tensor([0.0956, 0.4301, 0.1322, 0.0713, 0.0791, 0.0881, 0.1035],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.0050699724815785885
 
[Iteration 3] Process ID: 188609 [Epoch: 22,   256/ 591 points] total loss per batch: 1.225
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 0.0000e+00, 4.4611e-19, 8.9935e-17, 3.6315e-13,
        6.2976e-15], device='cuda:0')
Policy pred: tensor([2.3812e-04, 9.7635e-01, 9.1128e-05, 8.0044e-03, 1.4631e-02, 1.6727e-04,
        5.1967e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9988271594047546
 
[Iteration 3] Process ID: 188609 [Epoch: 22,   384/ 591 points] total loss per batch: 1.349
Policy (actual, predicted): 0 0
Policy data: tensor([0.1501, 0.1407, 0.1477, 0.1325, 0.1501, 0.1348, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.2575, 0.1480, 0.0929, 0.0822, 0.1293, 0.0990, 0.1910],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999791979789734
 
[Iteration 3] Process ID: 188609 [Epoch: 22,   512/ 591 points] total loss per batch: 1.401
Policy (actual, predicted): 1 1
Policy data: tensor([0.0934, 0.6501, 0.1337, 0.0188, 0.0173, 0.0412, 0.0455],
       device='cuda:0')
Policy pred: tensor([0.1202, 0.3849, 0.1398, 0.0722, 0.0841, 0.0954, 0.1036],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.07234357297420502
 
[Iteration 3] Process ID: 188609 [Epoch: 23,   128/ 591 points] total loss per batch: 1.165
Policy (actual, predicted): 2 2
Policy data: tensor([0.0217, 0.1514, 0.4403, 0.0057, 0.3482, 0.0156, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.0284, 0.1639, 0.5391, 0.0146, 0.2024, 0.0139, 0.0377],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.5524982213973999
 
[Iteration 3] Process ID: 188609 [Epoch: 23,   256/ 591 points] total loss per batch: 1.351
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1257, 0.3669, 0.1382, 0.0727, 0.1048, 0.1008, 0.0908],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.04226061701774597
 
[Iteration 3] Process ID: 188609 [Epoch: 23,   384/ 591 points] total loss per batch: 1.325
Policy (actual, predicted): 0 6
Policy data: tensor([0.1559, 0.1512, 0.1466, 0.1289, 0.1454, 0.1325, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1171, 0.1652, 0.1001, 0.1154, 0.1539, 0.1664, 0.1819],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999981701374054
 
[Iteration 3] Process ID: 188609 [Epoch: 23,   512/ 591 points] total loss per batch: 1.307
Policy (actual, predicted): 2 2
Policy data: tensor([6.7748e-02, 2.6026e-01, 2.8020e-01, 6.6160e-05, 2.8020e-01, 2.7363e-04,
        1.1125e-01], device='cuda:0')
Policy pred: tensor([0.0642, 0.2453, 0.3182, 0.0118, 0.2387, 0.0103, 0.1115],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999989867210388
 
[Iteration 3] Process ID: 188609 [Epoch: 24,   128/ 591 points] total loss per batch: 1.291
Policy (actual, predicted): 6 0
Policy data: tensor([0.1407, 0.1442, 0.1442, 0.1407, 0.1465, 0.1360, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.6361, 0.0614, 0.0494, 0.0462, 0.0716, 0.0487, 0.0866],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.05784985423088074
 
[Iteration 3] Process ID: 188609 [Epoch: 24,   256/ 591 points] total loss per batch: 1.299
Policy (actual, predicted): 3 0
Policy data: tensor([0.1372, 0.1454, 0.1419, 0.1477, 0.1442, 0.1372, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1785, 0.1361, 0.1744, 0.1568, 0.1242, 0.1179, 0.1122],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.998779296875
 
[Iteration 3] Process ID: 188609 [Epoch: 24,   384/ 591 points] total loss per batch: 1.251
Policy (actual, predicted): 0 6
Policy data: tensor([0.1501, 0.1430, 0.1454, 0.1372, 0.1501, 0.1313, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.0959, 0.2063, 0.1110, 0.0729, 0.0706, 0.1194, 0.3239],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.09951899200677872
 
[Iteration 3] Process ID: 188609 [Epoch: 24,   512/ 591 points] total loss per batch: 1.367
Policy (actual, predicted): 5 5
Policy data: tensor([0.0890, 0.1051, 0.1601, 0.1001, 0.1112, 0.3270, 0.1075],
       device='cuda:0')
Policy pred: tensor([0.1275, 0.0754, 0.1139, 0.1029, 0.1316, 0.3723, 0.0764],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999009966850281
 
[Iteration 3] Process ID: 188609 [Epoch: 25,   128/ 591 points] total loss per batch: 1.319
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9906e-01, 0.0000e+00, 9.3862e-04,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([9.9249e-05, 3.9452e-05, 1.0752e-04, 9.9539e-01, 2.0466e-06, 3.6854e-03,
        6.8060e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.995171070098877
 
[Iteration 3] Process ID: 188609 [Epoch: 25,   256/ 591 points] total loss per batch: 1.297
Policy (actual, predicted): 4 1
Policy data: tensor([0.1512, 0.1466, 0.1501, 0.1277, 0.1536, 0.1301, 0.1407],
       device='cuda:0')
Policy pred: tensor([0.1189, 0.3850, 0.1443, 0.0762, 0.0772, 0.0926, 0.1059],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.017269108444452286
 
[Iteration 3] Process ID: 188609 [Epoch: 25,   384/ 591 points] total loss per batch: 1.359
Policy (actual, predicted): 4 5
Policy data: tensor([0.1372, 0.1465, 0.1442, 0.1313, 0.1535, 0.1419, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1516, 0.1268, 0.1147, 0.1198, 0.1691, 0.1842, 0.1337],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999997615814209
 
[Iteration 3] Process ID: 188609 [Epoch: 25,   512/ 591 points] total loss per batch: 1.229
Policy (actual, predicted): 4 6
Policy data: tensor([0.1533, 0.1533, 0.0975, 0.0550, 0.2571, 0.0668, 0.2170],
       device='cuda:0')
Policy pred: tensor([0.1564, 0.1465, 0.0961, 0.0677, 0.2018, 0.0773, 0.2542],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9993780851364136
 
[Iteration 3] Process ID: 188609 [Epoch: 26,   128/ 591 points] total loss per batch: 1.169
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000e+00, 0.0000e+00, 9.9718e-01, 1.1090e-08, 5.5423e-13, 2.8185e-03,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([3.8097e-06, 1.0216e-04, 9.9254e-01, 1.0262e-03, 4.5000e-04, 5.6284e-03,
        2.4486e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9996357560157776
 
[Iteration 3] Process ID: 188609 [Epoch: 26,   256/ 591 points] total loss per batch: 1.340
Policy (actual, predicted): 6 6
Policy data: tensor([0.0935, 0.1762, 0.1476, 0.1350, 0.1613, 0.0579, 0.2285],
       device='cuda:0')
Policy pred: tensor([0.0844, 0.1440, 0.1999, 0.1228, 0.1840, 0.0538, 0.2110],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9989098906517029
 
[Iteration 3] Process ID: 188609 [Epoch: 26,   384/ 591 points] total loss per batch: 1.415
Policy (actual, predicted): 4 3
Policy data: tensor([0.1279, 0.1400, 0.1532, 0.1532, 0.2175, 0.0803, 0.1279],
       device='cuda:0')
Policy pred: tensor([0.1688, 0.1208, 0.1321, 0.1764, 0.1511, 0.0912, 0.1596],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999479651451111
 
[Iteration 3] Process ID: 188609 [Epoch: 26,   512/ 591 points] total loss per batch: 1.221
Policy (actual, predicted): 4 2
Policy data: tensor([0.0000, 0.1705, 0.2630, 0.0242, 0.4474, 0.0948, 0.0000],
       device='cuda:0')
Policy pred: tensor([2.3477e-04, 1.5667e-01, 3.5197e-01, 3.4669e-02, 3.3101e-01, 1.2512e-01,
        3.3575e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999957084655762
 
[Iteration 3] Process ID: 188609 [Epoch: 27,   128/ 591 points] total loss per batch: 1.300
Policy (actual, predicted): 0 0
Policy data: tensor([0.8949, 0.0116, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.6603, 0.0485, 0.0532, 0.0403, 0.0779, 0.0522, 0.0678],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.04202964901924133
 
[Iteration 3] Process ID: 188609 [Epoch: 27,   256/ 591 points] total loss per batch: 1.241
Policy (actual, predicted): 0 0
Policy data: tensor([0.6244, 0.1329, 0.1033, 0.0218, 0.0187, 0.0466, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.6478, 0.1026, 0.1061, 0.0203, 0.0219, 0.0507, 0.0505],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.024086160585284233
 
[Iteration 3] Process ID: 188609 [Epoch: 27,   384/ 591 points] total loss per batch: 1.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0926, 0.1207, 0.1029, 0.0514, 0.0332, 0.0374, 0.5617],
       device='cuda:0')
Policy pred: tensor([0.0609, 0.0756, 0.0807, 0.0321, 0.0541, 0.0370, 0.6596],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9996790289878845
 
[Iteration 3] Process ID: 188609 [Epoch: 27,   512/ 591 points] total loss per batch: 1.275
Policy (actual, predicted): 6 6
Policy data: tensor([0.1991, 0.0808, 0.1826, 0.0451, 0.1673, 0.0888, 0.2362],
       device='cuda:0')
Policy pred: tensor([0.1933, 0.0679, 0.1742, 0.0942, 0.1595, 0.0684, 0.2425],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999641180038452
 
[Iteration 3] Process ID: 188609 [Epoch: 28,   128/ 591 points] total loss per batch: 1.306
Policy (actual, predicted): 4 1
Policy data: tensor([0.1477, 0.1430, 0.1407, 0.1336, 0.1594, 0.1336, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1062, 0.1737, 0.1528, 0.1212, 0.1470, 0.1460, 0.1532],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999992847442627
 
[Iteration 3] Process ID: 188609 [Epoch: 28,   256/ 591 points] total loss per batch: 1.438
Policy (actual, predicted): 4 0
Policy data: tensor([0.1442, 0.1383, 0.1442, 0.1325, 0.1559, 0.1372, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1779, 0.1467, 0.1444, 0.1137, 0.1383, 0.1217, 0.1574],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9995636343955994
 
[Iteration 3] Process ID: 188609 [Epoch: 28,   384/ 591 points] total loss per batch: 1.159
Policy (actual, predicted): 1 1
Policy data: tensor([0.0330, 0.6655, 0.0208, 0.0094, 0.0192, 0.2410, 0.0111],
       device='cuda:0')
Policy pred: tensor([0.0241, 0.5719, 0.0122, 0.0026, 0.0147, 0.3588, 0.0157],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9960479736328125
 
[Iteration 3] Process ID: 188609 [Epoch: 28,   512/ 591 points] total loss per batch: 1.262
Policy (actual, predicted): 2 3
Policy data: tensor([0.1465, 0.1372, 0.1489, 0.1348, 0.1454, 0.1477, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1231, 0.1288, 0.1647, 0.1897, 0.1080, 0.1530, 0.1328],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998085498809814
 
[Iteration 3] Process ID: 188609 [Epoch: 29,   128/ 591 points] total loss per batch: 1.319
Policy (actual, predicted): 0 0
Policy data: tensor([9.9979e-01, 2.0900e-04, 0.0000e+00, 1.5909e-15, 3.7529e-17, 2.4134e-14,
        9.0839e-12], device='cuda:0')
Policy pred: tensor([9.8713e-01, 3.1795e-03, 4.3492e-04, 4.1030e-03, 7.1873e-04, 8.1421e-04,
        3.6152e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9984311461448669
 
[Iteration 3] Process ID: 188609 [Epoch: 29,   256/ 591 points] total loss per batch: 1.296
Policy (actual, predicted): 6 6
Policy data: tensor([0.2005, 0.0000, 0.0790, 0.0339, 0.2685, 0.0857, 0.3324],
       device='cuda:0')
Policy pred: tensor([2.1744e-01, 8.3553e-05, 1.0357e-01, 7.6502e-02, 2.2801e-01, 8.0454e-02,
        2.9393e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999911189079285
 
[Iteration 3] Process ID: 188609 [Epoch: 29,   384/ 591 points] total loss per batch: 1.276
Policy (actual, predicted): 6 1
Policy data: tensor([0.1407, 0.1442, 0.1419, 0.1336, 0.1407, 0.1384, 0.1605],
       device='cuda:0')
Policy pred: tensor([0.1600, 0.1848, 0.1652, 0.0986, 0.1167, 0.1221, 0.1526],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999933838844299
 
[Iteration 3] Process ID: 188609 [Epoch: 29,   512/ 591 points] total loss per batch: 1.249
Policy (actual, predicted): 0 0
Policy data: tensor([0.2785, 0.1397, 0.1668, 0.0548, 0.1527, 0.0548, 0.1527],
       device='cuda:0')
Policy pred: tensor([0.2204, 0.1375, 0.1537, 0.0795, 0.1931, 0.0796, 0.1362],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999787211418152
 
[Iteration 3] Process ID: 188609 [Epoch: 30,   128/ 591 points] total loss per batch: 1.339
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2306, 0.1787, 0.2956, 0.0646, 0.2306, 0.0000],
       device='cuda:0')
Policy pred: tensor([9.0902e-05, 2.0379e-01, 1.2876e-01, 3.7917e-01, 1.2281e-01, 1.6445e-01,
        9.3488e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999806880950928
 
[Iteration 3] Process ID: 188609 [Epoch: 30,   256/ 591 points] total loss per batch: 1.280
Policy (actual, predicted): 1 1
Policy data: tensor([0.0609, 0.3484, 0.1650, 0.0609, 0.0609, 0.0306, 0.2732],
       device='cuda:0')
Policy pred: tensor([0.0632, 0.3794, 0.1739, 0.0495, 0.0495, 0.0272, 0.2573],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999887347221375
 
[Iteration 3] Process ID: 188609 [Epoch: 30,   384/ 591 points] total loss per batch: 1.353
Policy (actual, predicted): 1 1
Policy data: tensor([5.4135e-02, 8.1486e-01, 7.9576e-04, 2.0640e-03, 1.8411e-03, 1.1031e-01,
        1.6000e-02], device='cuda:0')
Policy pred: tensor([0.0461, 0.8850, 0.0037, 0.0022, 0.0029, 0.0528, 0.0073],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998695254325867
 
[Iteration 3] Process ID: 188609 [Epoch: 30,   512/ 591 points] total loss per batch: 1.255
Policy (actual, predicted): 6 6
Policy data: tensor([0.1407, 0.1477, 0.1465, 0.1383, 0.1430, 0.1336, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1528, 0.1424, 0.1440, 0.1475, 0.1465, 0.1053, 0.1615],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.8710388541221619
 
[Iteration 3] Process ID: 188609 [Epoch: 31,   128/ 591 points] total loss per batch: 1.279
Policy (actual, predicted): 4 0
Policy data: tensor([0.1348, 0.1442, 0.1430, 0.1442, 0.1489, 0.1395, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.7083, 0.0394, 0.0459, 0.0390, 0.0619, 0.0412, 0.0644],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.030179541558027267
 
[Iteration 3] Process ID: 188609 [Epoch: 31,   256/ 591 points] total loss per batch: 1.444
Policy (actual, predicted): 6 4
Policy data: tensor([0.1430, 0.1442, 0.1348, 0.1454, 0.1407, 0.1454, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1348, 0.1055, 0.1067, 0.1319, 0.2430, 0.1259, 0.1521],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999749660491943
 
[Iteration 3] Process ID: 188609 [Epoch: 31,   384/ 591 points] total loss per batch: 1.273
Policy (actual, predicted): 1 1
Policy data: tensor([0.0943, 0.2116, 0.1628, 0.1940, 0.0710, 0.1035, 0.1628],
       device='cuda:0')
Policy pred: tensor([0.0929, 0.2357, 0.1450, 0.2292, 0.0519, 0.0940, 0.1513],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999771118164062
 
[Iteration 3] Process ID: 188609 [Epoch: 31,   512/ 591 points] total loss per batch: 1.194
Policy (actual, predicted): 6 6
Policy data: tensor([0.1419, 0.1442, 0.1372, 0.1465, 0.1372, 0.1395, 0.1535],
       device='cuda:0')
Policy pred: tensor([0.1014, 0.1099, 0.1057, 0.1170, 0.1213, 0.1121, 0.3326],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.18605826795101166
 
[Iteration 3] Process ID: 188609 [Epoch: 32,   128/ 591 points] total loss per batch: 1.244
Policy (actual, predicted): 0 0
Policy data: tensor([0.4737, 0.0441, 0.4088, 0.0092, 0.0125, 0.0250, 0.0265],
       device='cuda:0')
Policy pred: tensor([0.6347, 0.0462, 0.2622, 0.0066, 0.0143, 0.0134, 0.0226],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9993540048599243
 
[Iteration 3] Process ID: 188609 [Epoch: 32,   256/ 591 points] total loss per batch: 1.191
Policy (actual, predicted): 6 6
Policy data: tensor([0.1583, 0.1887, 0.1448, 0.1210, 0.1210, 0.0422, 0.2242],
       device='cuda:0')
Policy pred: tensor([0.1494, 0.1948, 0.1370, 0.1475, 0.1073, 0.0556, 0.2085],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999931454658508
 
[Iteration 3] Process ID: 188609 [Epoch: 32,   384/ 591 points] total loss per batch: 1.426
Policy (actual, predicted): 1 0
Policy data: tensor([0.1372, 0.1477, 0.1454, 0.1419, 0.1442, 0.1360, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.8395, 0.0250, 0.0248, 0.0187, 0.0344, 0.0226, 0.0351],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.06034112721681595
 
[Iteration 3] Process ID: 188609 [Epoch: 32,   512/ 591 points] total loss per batch: 1.338
Policy (actual, predicted): 1 1
Policy data: tensor([8.6393e-18, 1.0000e+00, 8.6393e-18, 1.7223e-17, 2.9168e-22, 3.2070e-20,
        3.2070e-20], device='cuda:0')
Policy pred: tensor([1.3195e-03, 9.9047e-01, 3.0599e-03, 4.0222e-03, 3.9474e-04, 6.9070e-04,
        4.1713e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9687290787696838
 
[Iteration 3] Process ID: 188609 [Epoch: 33,   128/ 591 points] total loss per batch: 1.096
Policy (actual, predicted): 4 4
Policy data: tensor([0.0734, 0.2334, 0.0410, 0.0551, 0.3249, 0.1063, 0.1658],
       device='cuda:0')
Policy pred: tensor([0.0678, 0.1637, 0.0409, 0.0468, 0.3854, 0.0854, 0.2100],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998765587806702
 
[Iteration 3] Process ID: 188609 [Epoch: 33,   256/ 591 points] total loss per batch: 1.226
Policy (actual, predicted): 0 0
Policy data: tensor([7.1246e-01, 3.4085e-05, 1.2956e-04, 1.2066e-13, 2.8738e-01, 5.8889e-11,
        2.5576e-12], device='cuda:0')
Policy pred: tensor([6.8005e-01, 6.2846e-03, 6.7723e-04, 4.1647e-04, 3.0118e-01, 5.5261e-04,
        1.0838e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999873042106628
 
[Iteration 3] Process ID: 188609 [Epoch: 33,   384/ 591 points] total loss per batch: 1.365
Policy (actual, predicted): 2 2
Policy data: tensor([0.1829, 0.1103, 0.5624, 0.0123, 0.0123, 0.0421, 0.0776],
       device='cuda:0')
Policy pred: tensor([0.1683, 0.1760, 0.5150, 0.0109, 0.0212, 0.0556, 0.0530],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9621321558952332
 
[Iteration 3] Process ID: 188609 [Epoch: 33,   512/ 591 points] total loss per batch: 1.396
Policy (actual, predicted): 0 0
Policy data: tensor([0.4255, 0.0000, 0.2600, 0.0674, 0.0000, 0.0000, 0.2471],
       device='cuda:0')
Policy pred: tensor([3.6011e-01, 2.1016e-05, 2.5112e-01, 5.3857e-02, 2.0432e-06, 6.8066e-04,
        3.3421e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999849796295166
 
[Iteration 3] Process ID: 188609 [Epoch: 34,   128/ 591 points] total loss per batch: 1.296
Policy (actual, predicted): 4 4
Policy data: tensor([0.1904, 0.0932, 0.1600, 0.0432, 0.3153, 0.0639, 0.1341],
       device='cuda:0')
Policy pred: tensor([0.1338, 0.0809, 0.2368, 0.0389, 0.3150, 0.0610, 0.1335],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998918175697327
 
[Iteration 3] Process ID: 188609 [Epoch: 34,   256/ 591 points] total loss per batch: 1.421
Policy (actual, predicted): 4 0
Policy data: tensor([0.1501, 0.1430, 0.1442, 0.1313, 0.1559, 0.1360, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1765, 0.1273, 0.1412, 0.1166, 0.1395, 0.1600, 0.1389],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.920921802520752
 
[Iteration 3] Process ID: 188609 [Epoch: 34,   384/ 591 points] total loss per batch: 1.192
Policy (actual, predicted): 1 3
Policy data: tensor([0.2195, 0.2730, 0.0182, 0.1893, 0.0000, 0.1107, 0.1893],
       device='cuda:0')
Policy pred: tensor([1.9352e-01, 1.7269e-01, 2.8688e-02, 2.6415e-01, 1.6779e-05, 9.5009e-02,
        2.4593e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999772310256958
 
[Iteration 3] Process ID: 188609 [Epoch: 34,   512/ 591 points] total loss per batch: 1.216
Policy (actual, predicted): 2 2
Policy data: tensor([0.1917, 0.1435, 0.5590, 0.0247, 0.0232, 0.0407, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.1778, 0.1176, 0.5877, 0.0243, 0.0295, 0.0423, 0.0208],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.974297046661377
 
[Iteration 3] Process ID: 188609 [Epoch: 35,   128/ 591 points] total loss per batch: 1.338
Policy (actual, predicted): 4 4
Policy data: tensor([0.0476, 0.2453, 0.0525, 0.1117, 0.2667, 0.1020, 0.1743],
       device='cuda:0')
Policy pred: tensor([0.0465, 0.2623, 0.0374, 0.0956, 0.2825, 0.0794, 0.1961],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999948143959045
 
[Iteration 3] Process ID: 188609 [Epoch: 35,   256/ 591 points] total loss per batch: 1.266
Policy (actual, predicted): 6 6
Policy data: tensor([0.1145, 0.0262, 0.0247, 0.0217, 0.2126, 0.0364, 0.5639],
       device='cuda:0')
Policy pred: tensor([0.1569, 0.0536, 0.0370, 0.0172, 0.2141, 0.0298, 0.4913],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9858778715133667
 
[Iteration 3] Process ID: 188609 [Epoch: 35,   384/ 591 points] total loss per batch: 1.226
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 1.7811e-07, 3.3510e-18, 3.6495e-20, 7.6289e-24, 7.6289e-24,
        3.6495e-20], device='cuda:0')
Policy pred: tensor([0.9638, 0.0196, 0.0020, 0.0011, 0.0023, 0.0038, 0.0074],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9984818696975708
 
[Iteration 3] Process ID: 188609 [Epoch: 35,   512/ 591 points] total loss per batch: 1.307
Policy (actual, predicted): 6 1
Policy data: tensor([0.1561, 0.1427, 0.1304, 0.1427, 0.1427, 0.0989, 0.1864],
       device='cuda:0')
Policy pred: tensor([0.1622, 0.1856, 0.0903, 0.1547, 0.1592, 0.1079, 0.1400],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999801516532898
 
[Iteration 3] Process ID: 188609 [Epoch: 36,   128/ 591 points] total loss per batch: 1.262
Policy (actual, predicted): 2 2
Policy data: tensor([1.1003e-06, 2.9133e-06, 9.9991e-01, 3.8581e-19, 7.9434e-05, 1.0247e-15,
        1.9120e-06], device='cuda:0')
Policy pred: tensor([1.2744e-04, 2.5241e-03, 9.8867e-01, 9.1406e-04, 6.4309e-03, 3.7128e-04,
        9.6483e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999036192893982
 
[Iteration 3] Process ID: 188609 [Epoch: 36,   256/ 591 points] total loss per batch: 1.324
Policy (actual, predicted): 2 2
Policy data: tensor([0.0217, 0.1514, 0.4403, 0.0057, 0.3482, 0.0156, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.0217, 0.1567, 0.4046, 0.0139, 0.3510, 0.0236, 0.0285],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.6879414319992065
 
[Iteration 3] Process ID: 188609 [Epoch: 36,   384/ 591 points] total loss per batch: 1.321
Policy (actual, predicted): 6 6
Policy data: tensor([0.1395, 0.1465, 0.1348, 0.1489, 0.1407, 0.1383, 0.1512],
       device='cuda:0')
Policy pred: tensor([0.1482, 0.1273, 0.1484, 0.1371, 0.1322, 0.1454, 0.1613],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.998786985874176
 
[Iteration 3] Process ID: 188609 [Epoch: 36,   512/ 591 points] total loss per batch: 1.173
Policy (actual, predicted): 2 2
Policy data: tensor([0.0215, 0.0263, 0.8668, 0.0114, 0.0311, 0.0182, 0.0247],
       device='cuda:0')
Policy pred: tensor([0.0264, 0.0235, 0.8802, 0.0051, 0.0275, 0.0137, 0.0235],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.25757500529289246
 
[Iteration 3] Process ID: 188609 [Epoch: 37,   128/ 591 points] total loss per batch: 1.383
Policy (actual, predicted): 3 3
Policy data: tensor([0.0955, 0.1501, 0.0539, 0.2739, 0.0955, 0.0792, 0.2519],
       device='cuda:0')
Policy pred: tensor([0.0867, 0.1468, 0.0776, 0.3236, 0.0832, 0.0737, 0.2085],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999526739120483
 
[Iteration 3] Process ID: 188609 [Epoch: 37,   256/ 591 points] total loss per batch: 1.214
Policy (actual, predicted): 2 2
Policy data: tensor([2.7122e-01, 2.4647e-02, 4.8035e-01, 1.9650e-04, 1.8649e-01, 1.5233e-03,
        3.5576e-02], device='cuda:0')
Policy pred: tensor([0.2457, 0.0181, 0.4806, 0.0104, 0.2087, 0.0157, 0.0207],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999606013298035
 
[Iteration 3] Process ID: 188609 [Epoch: 37,   384/ 591 points] total loss per batch: 1.226
Policy (actual, predicted): 2 2
Policy data: tensor([0.1155, 0.0361, 0.7418, 0.0128, 0.0160, 0.0255, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.1314, 0.0282, 0.7356, 0.0118, 0.0166, 0.0276, 0.0487],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9936791062355042
 
[Iteration 3] Process ID: 188609 [Epoch: 37,   512/ 591 points] total loss per batch: 1.232
Policy (actual, predicted): 4 6
Policy data: tensor([0.1466, 0.1419, 0.1489, 0.1313, 0.1524, 0.1360, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1407, 0.1571, 0.1166, 0.1495, 0.1474, 0.1296, 0.1590],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9902639985084534
 
[Iteration 3] Process ID: 188609 [Epoch: 38,   128/ 591 points] total loss per batch: 1.353
Policy (actual, predicted): 0 1
Policy data: tensor([0.1512, 0.1512, 0.1384, 0.1277, 0.1501, 0.1336, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1584, 0.1940, 0.1394, 0.1197, 0.1147, 0.1456, 0.1282],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999880850315094
 
[Iteration 3] Process ID: 188609 [Epoch: 38,   256/ 591 points] total loss per batch: 1.265
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.1798, 0.0882, 0.2429, 0.2255, 0.1318, 0.1318],
       device='cuda:0')
Policy pred: tensor([5.1482e-05, 2.1168e-01, 1.0848e-01, 2.4465e-01, 1.8178e-01, 1.3947e-01,
        1.1389e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999992311000824
 
[Iteration 3] Process ID: 188609 [Epoch: 38,   384/ 591 points] total loss per batch: 1.291
Policy (actual, predicted): 6 6
Policy data: tensor([0.1579, 0.1354, 0.2204, 0.0586, 0.0990, 0.0599, 0.2688],
       device='cuda:0')
Policy pred: tensor([0.1765, 0.1194, 0.2033, 0.0516, 0.0970, 0.0764, 0.2758],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9654492735862732
 
[Iteration 3] Process ID: 188609 [Epoch: 38,   512/ 591 points] total loss per batch: 1.213
Policy (actual, predicted): 0 0
Policy data: tensor([7.4563e-01, 4.7480e-04, 2.5335e-01, 7.2741e-10, 6.9555e-05, 3.4502e-07,
        4.7480e-04], device='cuda:0')
Policy pred: tensor([0.6855, 0.0119, 0.2811, 0.0025, 0.0120, 0.0028, 0.0043],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999975562095642
 
[Iteration 3] Process ID: 188609 [Epoch: 39,   128/ 591 points] total loss per batch: 1.278
Policy (actual, predicted): 3 3
Policy data: tensor([0.1010, 0.0631, 0.0695, 0.3116, 0.2641, 0.0695, 0.1211],
       device='cuda:0')
Policy pred: tensor([0.1092, 0.0636, 0.0733, 0.3195, 0.2435, 0.0737, 0.1172],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999954104423523
 
[Iteration 3] Process ID: 188609 [Epoch: 39,   256/ 591 points] total loss per batch: 1.325
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 2.3553e-09, 7.0268e-20, 2.6710e-19, 1.4689e-13,
        1.5992e-09], device='cuda:0')
Policy pred: tensor([2.9491e-06, 9.8950e-01, 6.3956e-03, 8.9874e-05, 1.0266e-03, 1.5419e-04,
        2.8344e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9995282888412476
 
[Iteration 3] Process ID: 188609 [Epoch: 39,   384/ 591 points] total loss per batch: 1.334
Policy (actual, predicted): 6 4
Policy data: tensor([0.1454, 0.1372, 0.1360, 0.1360, 0.1489, 0.1430, 0.1535],
       device='cuda:0')
Policy pred: tensor([0.1464, 0.1473, 0.1548, 0.1211, 0.1582, 0.1346, 0.1375],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999956488609314
 
[Iteration 3] Process ID: 188609 [Epoch: 39,   512/ 591 points] total loss per batch: 1.200
Policy (actual, predicted): 4 4
Policy data: tensor([0.1501, 0.1454, 0.1501, 0.1242, 0.1524, 0.1337, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1373, 0.1413, 0.1370, 0.1381, 0.1850, 0.1161, 0.1451],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9956380128860474
 
[Iteration 3] Process ID: 188609 [Epoch: 40,   128/ 591 points] total loss per batch: 1.298
Policy (actual, predicted): 4 4
Policy data: tensor([0.1017, 0.1906, 0.0697, 0.0768, 0.2265, 0.1600, 0.1747],
       device='cuda:0')
Policy pred: tensor([0.0996, 0.1759, 0.0691, 0.0830, 0.2436, 0.1324, 0.1965],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9996873736381531
 
[Iteration 3] Process ID: 188609 [Epoch: 40,   256/ 591 points] total loss per batch: 1.182
Policy (actual, predicted): 6 2
Policy data: tensor([0.1524, 0.1431, 0.1477, 0.1313, 0.1442, 0.1266, 0.1547],
       device='cuda:0')
Policy pred: tensor([0.1467, 0.1442, 0.1722, 0.1071, 0.1370, 0.1209, 0.1719],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9997164607048035
 
[Iteration 3] Process ID: 188609 [Epoch: 40,   384/ 591 points] total loss per batch: 1.392
Policy (actual, predicted): 0 0
Policy data: tensor([9.9660e-01, 0.0000e+00, 9.8604e-05, 1.8227e-05, 1.8227e-05, 3.9518e-05,
        3.2289e-03], device='cuda:0')
Policy pred: tensor([9.8824e-01, 2.2614e-05, 2.4989e-03, 4.2799e-03, 4.1618e-04, 1.9723e-03,
        2.5660e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999970197677612
 
[Iteration 3] Process ID: 188609 [Epoch: 40,   512/ 591 points] total loss per batch: 1.336
Policy (actual, predicted): 1 1
Policy data: tensor([0.0650, 0.3209, 0.0485, 0.1777, 0.1139, 0.1247, 0.1491],
       device='cuda:0')
Policy pred: tensor([0.0866, 0.2981, 0.0377, 0.1995, 0.1277, 0.1428, 0.1076],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999744892120361
 
[Iteration 3] Process ID: 188609 [Epoch: 41,   128/ 591 points] total loss per batch: 1.293
Policy (actual, predicted): 6 6
Policy data: tensor([0.1579, 0.1354, 0.2204, 0.0586, 0.0990, 0.0599, 0.2688],
       device='cuda:0')
Policy pred: tensor([0.1315, 0.0995, 0.2001, 0.0735, 0.1232, 0.0814, 0.2908],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9519806504249573
 
[Iteration 3] Process ID: 188609 [Epoch: 41,   256/ 591 points] total loss per batch: 1.408
Policy (actual, predicted): 6 4
Policy data: tensor([0.1419, 0.1477, 0.1360, 0.1430, 0.1430, 0.1383, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1485, 0.1428, 0.1245, 0.1579, 0.1632, 0.1223, 0.1407],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9902728796005249
 
[Iteration 3] Process ID: 188609 [Epoch: 41,   384/ 591 points] total loss per batch: 1.286
Policy (actual, predicted): 2 2
Policy data: tensor([0.2505, 0.0671, 0.2990, 0.2149, 0.0902, 0.0433, 0.0351],
       device='cuda:0')
Policy pred: tensor([0.1896, 0.0989, 0.3218, 0.2142, 0.0843, 0.0650, 0.0262],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9809502363204956
 
[Iteration 3] Process ID: 188609 [Epoch: 41,   512/ 591 points] total loss per batch: 1.251
Policy (actual, predicted): 3 3
Policy data: tensor([8.3669e-04, 4.3403e-02, 2.7490e-03, 9.5136e-01, 1.2638e-03, 2.9462e-06,
        3.8919e-04], device='cuda:0')
Policy pred: tensor([2.3851e-03, 3.5524e-02, 3.1358e-03, 9.4849e-01, 4.4489e-03, 3.0056e-04,
        5.7206e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999839067459106
 
[Iteration 3] Process ID: 188609 [Epoch: 42,   128/ 591 points] total loss per batch: 1.291
Policy (actual, predicted): 1 4
Policy data: tensor([0.1454, 0.1524, 0.1313, 0.1419, 0.1395, 0.1430, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.0951, 0.1311, 0.1741, 0.0975, 0.1855, 0.1378, 0.1788],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999921321868896
 
[Iteration 3] Process ID: 188609 [Epoch: 42,   256/ 591 points] total loss per batch: 1.251
Policy (actual, predicted): 2 3
Policy data: tensor([0.1466, 0.1466, 0.1501, 0.1301, 0.1466, 0.1360, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1393, 0.1230, 0.1476, 0.1570, 0.1533, 0.1458, 0.1340],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9997614622116089
 
[Iteration 3] Process ID: 188609 [Epoch: 42,   384/ 591 points] total loss per batch: 1.279
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 4.6359e-12, 0.0000e+00, 1.6805e-17, 0.0000e+00, 3.7417e-17,
        6.7988e-07], device='cuda:0')
Policy pred: tensor([9.9886e-01, 8.7142e-04, 1.0065e-05, 1.0548e-04, 3.3819e-07, 7.7686e-06,
        1.4815e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999879002571106
 
[Iteration 3] Process ID: 188609 [Epoch: 42,   512/ 591 points] total loss per batch: 1.311
Policy (actual, predicted): 4 4
Policy data: tensor([0.0165, 0.0231, 0.0114, 0.0079, 0.8601, 0.0627, 0.0182],
       device='cuda:0')
Policy pred: tensor([0.0230, 0.0162, 0.0271, 0.0124, 0.8303, 0.0579, 0.0331],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9505575895309448
 
[Iteration 3] Process ID: 188609 [Epoch: 43,   128/ 591 points] total loss per batch: 1.371
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.3345, 0.2337, 0.0341, 0.2172, 0.0621, 0.1185],
       device='cuda:0')
Policy pred: tensor([5.6044e-05, 4.3492e-01, 2.4400e-01, 2.9613e-02, 1.6282e-01, 5.1582e-02,
        7.7011e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999977946281433
 
[Iteration 3] Process ID: 188609 [Epoch: 43,   256/ 591 points] total loss per batch: 1.293
Policy (actual, predicted): 6 6
Policy data: tensor([0.1932, 0.1356, 0.0778, 0.1239, 0.1621, 0.0778, 0.2296],
       device='cuda:0')
Policy pred: tensor([0.1675, 0.1401, 0.0616, 0.0966, 0.1280, 0.0713, 0.3349],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9965349435806274
 
[Iteration 3] Process ID: 188609 [Epoch: 43,   384/ 591 points] total loss per batch: 1.214
Policy (actual, predicted): 0 0
Policy data: tensor([0.8765, 0.0149, 0.0079, 0.0097, 0.0199, 0.0526, 0.0183],
       device='cuda:0')
Policy pred: tensor([0.8584, 0.0148, 0.0157, 0.0197, 0.0345, 0.0387, 0.0182],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.974369466304779
 
[Iteration 3] Process ID: 188609 [Epoch: 43,   512/ 591 points] total loss per batch: 1.226
Policy (actual, predicted): 1 1
Policy data: tensor([0.2079, 0.4663, 0.0840, 0.0056, 0.0904, 0.0775, 0.0683],
       device='cuda:0')
Policy pred: tensor([0.2274, 0.4641, 0.0806, 0.0159, 0.0870, 0.0604, 0.0645],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9818455576896667
 
[Iteration 3] Process ID: 188609 [Epoch: 44,   128/ 591 points] total loss per batch: 1.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0215, 0.0524, 0.0199, 0.0079, 0.0263, 0.0079, 0.8641],
       device='cuda:0')
Policy pred: tensor([0.0065, 0.0257, 0.0090, 0.0040, 0.0157, 0.0078, 0.9314],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9993798732757568
 
[Iteration 3] Process ID: 188609 [Epoch: 44,   256/ 591 points] total loss per batch: 1.303
Policy (actual, predicted): 6 1
Policy data: tensor([0.0982, 0.3116, 0.0799, 0.1372, 0.0000, 0.0000, 0.3732],
       device='cuda:0')
Policy pred: tensor([1.0826e-01, 3.7431e-01, 1.1655e-01, 1.5145e-01, 1.6228e-05, 3.2854e-04,
        2.4908e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999919533729553
 
[Iteration 3] Process ID: 188609 [Epoch: 44,   384/ 591 points] total loss per batch: 1.229
Policy (actual, predicted): 1 1
Policy data: tensor([1.0917e-12, 1.0000e+00, 1.0766e-17, 4.9118e-20, 2.1140e-09, 5.0296e-17,
        3.5801e-14], device='cuda:0')
Policy pred: tensor([0.0124, 0.9698, 0.0051, 0.0024, 0.0022, 0.0024, 0.0056],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9984034299850464
 
[Iteration 3] Process ID: 188609 [Epoch: 44,   512/ 591 points] total loss per batch: 1.188
Policy (actual, predicted): 4 4
Policy data: tensor([1.2920e-18, 4.4606e-07, 0.0000e+00, 3.3988e-19, 9.9999e-01, 3.1209e-17,
        4.6129e-06], device='cuda:0')
Policy pred: tensor([4.0986e-05, 4.1756e-03, 4.0327e-03, 2.4041e-03, 9.8571e-01, 1.1222e-03,
        2.5142e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998030662536621
 
[Iteration 3] Process ID: 188609 [Epoch: 45,   128/ 591 points] total loss per batch: 1.322
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.0000, 0.1095, 0.0000, 0.0000, 0.1585, 0.7320],
       device='cuda:0')
Policy pred: tensor([3.2708e-04, 8.2128e-05, 1.0169e-01, 3.7153e-04, 1.9719e-05, 1.4785e-01,
        7.4966e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999734163284302
 
[Iteration 3] Process ID: 188609 [Epoch: 45,   256/ 591 points] total loss per batch: 1.192
Policy (actual, predicted): 6 1
Policy data: tensor([0.1141, 0.2796, 0.0190, 0.1674, 0.0000, 0.0975, 0.3223],
       device='cuda:0')
Policy pred: tensor([6.2225e-02, 3.3501e-01, 2.0769e-02, 1.7787e-01, 2.7634e-05, 7.7143e-02,
        3.2696e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999291300773621
 
[Iteration 3] Process ID: 188609 [Epoch: 45,   384/ 591 points] total loss per batch: 1.397
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 1.7251e-07, 8.4322e-16, 2.3063e-21, 2.4183e-15, 6.9951e-14,
        2.4764e-22], device='cuda:0')
Policy pred: tensor([9.9844e-01, 5.7017e-04, 4.1826e-05, 3.8372e-05, 7.4496e-04, 3.6470e-05,
        1.3059e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999995231628418
 
[Iteration 3] Process ID: 188609 [Epoch: 45,   512/ 591 points] total loss per batch: 1.260
Policy (actual, predicted): 1 4
Policy data: tensor([0.1501, 0.1536, 0.1419, 0.1301, 0.1501, 0.1325, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1589, 0.1457, 0.1262, 0.1466, 0.1765, 0.1228, 0.1233],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998807907104
 
[Iteration 3] Process ID: 188609 [Epoch: 46,   128/ 591 points] total loss per batch: 1.295
Policy (actual, predicted): 6 6
Policy data: tensor([0.2005, 0.0000, 0.0790, 0.0339, 0.2685, 0.0857, 0.3324],
       device='cuda:0')
Policy pred: tensor([2.1423e-01, 5.0736e-05, 9.8006e-02, 3.4976e-02, 2.4254e-01, 9.6649e-02,
        3.1355e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999997019767761
 
[Iteration 3] Process ID: 188609 [Epoch: 46,   256/ 591 points] total loss per batch: 1.318
Policy (actual, predicted): 5 5
Policy data: tensor([0.0941, 0.0812, 0.0903, 0.0508, 0.0928, 0.5187, 0.0721],
       device='cuda:0')
Policy pred: tensor([0.0639, 0.0719, 0.0727, 0.0463, 0.0812, 0.5804, 0.0835],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998608827590942
 
[Iteration 3] Process ID: 188609 [Epoch: 46,   384/ 591 points] total loss per batch: 1.186
Policy (actual, predicted): 1 1
Policy data: tensor([2.9877e-02, 7.9537e-01, 3.2440e-02, 3.2749e-03, 1.1032e-03, 4.6567e-07,
        1.3793e-01], device='cuda:0')
Policy pred: tensor([0.0248, 0.8231, 0.0273, 0.0106, 0.0069, 0.0020, 0.1052],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999989926815033
 
[Iteration 3] Process ID: 188609 [Epoch: 46,   512/ 591 points] total loss per batch: 1.362
Policy (actual, predicted): 5 5
Policy data: tensor([0.0220, 0.0062, 0.0081, 0.0062, 0.0186, 0.9308, 0.0081],
       device='cuda:0')
Policy pred: tensor([0.0211, 0.0047, 0.0036, 0.0056, 0.0102, 0.9468, 0.0081],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9854779839515686
 
[Iteration 3] Process ID: 188609 [Epoch: 47,   128/ 591 points] total loss per batch: 1.388
Policy (actual, predicted): 0 0
Policy data: tensor([0.6231, 0.1329, 0.1020, 0.0218, 0.0187, 0.0494, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.6886, 0.1242, 0.0752, 0.0185, 0.0153, 0.0376, 0.0405],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.0075043668039143085
 
[Iteration 3] Process ID: 188609 [Epoch: 47,   256/ 591 points] total loss per batch: 1.278
Policy (actual, predicted): 4 6
Policy data: tensor([0.1372, 0.1465, 0.1442, 0.1313, 0.1535, 0.1419, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1049, 0.1357, 0.1629, 0.1544, 0.1571, 0.1208, 0.1642],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999997019767761
 
[Iteration 3] Process ID: 188609 [Epoch: 47,   384/ 591 points] total loss per batch: 1.207
Policy (actual, predicted): 1 1
Policy data: tensor([4.1290e-05, 9.9996e-01, 0.0000e+00, 1.1552e-13, 0.0000e+00, 1.1171e-14,
        5.6815e-08], device='cuda:0')
Policy pred: tensor([8.0081e-03, 9.8853e-01, 5.1947e-04, 6.4855e-04, 6.2313e-05, 1.8413e-04,
        2.0512e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999238848686218
 
[Iteration 3] Process ID: 188609 [Epoch: 47,   512/ 591 points] total loss per batch: 1.183
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 0.0000e+00, 4.7631e-08, 0.0000e+00, 2.2985e-07,
        4.4360e-07], device='cuda:0')
Policy pred: tensor([7.7222e-07, 9.9842e-01, 2.2254e-05, 3.5719e-05, 3.0370e-07, 5.5367e-05,
        1.4702e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999914169311523
 
[Iteration 3] Process ID: 188609 [Epoch: 48,   128/ 591 points] total loss per batch: 1.389
Policy (actual, predicted): 2 2
Policy data: tensor([2.7122e-01, 2.4647e-02, 4.8035e-01, 1.9650e-04, 1.8649e-01, 1.5233e-03,
        3.5576e-02], device='cuda:0')
Policy pred: tensor([0.2674, 0.0285, 0.4261, 0.0154, 0.1935, 0.0231, 0.0461],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999780058860779
 
[Iteration 3] Process ID: 188609 [Epoch: 48,   256/ 591 points] total loss per batch: 1.338
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9961e-01, 0.0000e+00, 3.8781e-04,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([1.6723e-05, 4.9338e-06, 1.7119e-04, 9.9696e-01, 4.8185e-04, 2.1018e-03,
        2.6854e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9994444251060486
 
[Iteration 3] Process ID: 188609 [Epoch: 48,   384/ 591 points] total loss per batch: 1.169
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000e+00, 4.7419e-05, 9.9995e-01, 1.2444e-19, 4.7303e-19, 4.7303e-19,
        4.7303e-19], device='cuda:0')
Policy pred: tensor([9.2238e-07, 5.9488e-04, 9.9882e-01, 4.8991e-05, 1.9825e-04, 3.4191e-05,
        3.0769e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998992681503296
 
[Iteration 3] Process ID: 188609 [Epoch: 48,   512/ 591 points] total loss per batch: 1.280
Policy (actual, predicted): 1 1
Policy data: tensor([0.2079, 0.4663, 0.0840, 0.0056, 0.0904, 0.0775, 0.0683],
       device='cuda:0')
Policy pred: tensor([0.1920, 0.4431, 0.1082, 0.0169, 0.0966, 0.0697, 0.0735],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9882460236549377
 
[Iteration 3] Process ID: 188609 [Epoch: 49,   128/ 591 points] total loss per batch: 1.276
Policy (actual, predicted): 2 4
Policy data: tensor([0.1372, 0.1419, 0.1500, 0.1419, 0.1430, 0.1407, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1303, 0.1191, 0.1514, 0.1466, 0.1603, 0.1521, 0.1401],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9944173693656921
 
[Iteration 3] Process ID: 188609 [Epoch: 49,   256/ 591 points] total loss per batch: 1.181
Policy (actual, predicted): 0 0
Policy data: tensor([0.2785, 0.1397, 0.1668, 0.0548, 0.1527, 0.0548, 0.1527],
       device='cuda:0')
Policy pred: tensor([0.2317, 0.1423, 0.1426, 0.0859, 0.1513, 0.0787, 0.1673],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999932050704956
 
[Iteration 3] Process ID: 188609 [Epoch: 49,   384/ 591 points] total loss per batch: 1.420
Policy (actual, predicted): 2 2
Policy data: tensor([0.0217, 0.1514, 0.4426, 0.0057, 0.3459, 0.0156, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.0241, 0.2261, 0.3807, 0.0165, 0.3108, 0.0191, 0.0227],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.6703540086746216
 
[Iteration 3] Process ID: 188609 [Epoch: 49,   512/ 591 points] total loss per batch: 1.225
Policy (actual, predicted): 0 0
Policy data: tensor([0.3128, 0.0000, 0.2017, 0.1191, 0.0000, 0.1891, 0.1773],
       device='cuda:0')
Policy pred: tensor([2.6315e-01, 2.2554e-05, 1.9713e-01, 1.0274e-01, 3.8893e-04, 2.0073e-01,
        2.3584e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998979568481445
 
[Iteration 3] Process ID: 188609 [Epoch: 50,   128/ 591 points] total loss per batch: 1.348
Policy (actual, predicted): 0 0
Policy data: tensor([0.9549, 0.0082, 0.0082, 0.0063, 0.0100, 0.0063, 0.0063],
       device='cuda:0')
Policy pred: tensor([0.9481, 0.0062, 0.0055, 0.0057, 0.0105, 0.0138, 0.0103],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9973636865615845
 
[Iteration 3] Process ID: 188609 [Epoch: 50,   256/ 591 points] total loss per batch: 1.269
Policy (actual, predicted): 1 1
Policy data: tensor([4.3506e-03, 9.6874e-01, 3.9572e-03, 1.5618e-04, 1.6343e-02, 1.5618e-04,
        6.3011e-03], device='cuda:0')
Policy pred: tensor([0.0103, 0.9592, 0.0060, 0.0021, 0.0087, 0.0033, 0.0104],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999999463558197
 
[Iteration 3] Process ID: 188609 [Epoch: 50,   384/ 591 points] total loss per batch: 1.180
Policy (actual, predicted): 0 0
Policy data: tensor([7.6937e-01, 6.6311e-02, 4.4232e-02, 1.6654e-05, 1.0559e-01, 2.8256e-04,
        1.4194e-02], device='cuda:0')
Policy pred: tensor([0.6915, 0.0799, 0.0382, 0.0070, 0.1320, 0.0124, 0.0391],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998707175254822
 
[Iteration 3] Process ID: 188609 [Epoch: 50,   512/ 591 points] total loss per batch: 1.381
Policy (actual, predicted): 6 6
Policy data: tensor([0.1291, 0.1977, 0.0124, 0.0124, 0.0124, 0.0619, 0.5741],
       device='cuda:0')
Policy pred: tensor([0.1288, 0.1941, 0.0849, 0.0900, 0.0847, 0.1251, 0.2924],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.010239370167255402
 
[Iteration 3] Process ID: 188609 [Epoch: 51,   128/ 591 points] total loss per batch: 1.203
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1091, 0.4658, 0.1351, 0.0581, 0.0677, 0.0729, 0.0913],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.0025822387542575598
 
[Iteration 3] Process ID: 188609 [Epoch: 51,   256/ 591 points] total loss per batch: 1.234
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000e+00, 0.0000e+00, 6.5395e-09, 2.8452e-17, 2.0306e-13, 1.0000e+00,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([1.7427e-05, 3.1027e-06, 8.4793e-04, 4.9083e-05, 5.9593e-06, 9.9906e-01,
        1.2737e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9969181418418884
 
[Iteration 3] Process ID: 188609 [Epoch: 51,   384/ 591 points] total loss per batch: 1.294
Policy (actual, predicted): 6 6
Policy data: tensor([0.0515, 0.2798, 0.0515, 0.0993, 0.1190, 0.0423, 0.3567],
       device='cuda:0')
Policy pred: tensor([0.0437, 0.2909, 0.0501, 0.0637, 0.1311, 0.0346, 0.3860],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999892711639404
 
[Iteration 3] Process ID: 188609 [Epoch: 51,   512/ 591 points] total loss per batch: 1.360
Policy (actual, predicted): 0 0
Policy data: tensor([0.2507, 0.1776, 0.1360, 0.1360, 0.0780, 0.0858, 0.1360],
       device='cuda:0')
Policy pred: tensor([0.2387, 0.1899, 0.1244, 0.1216, 0.1090, 0.0835, 0.1329],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9989596009254456
 
[Iteration 3] Process ID: 188609 [Epoch: 52,   128/ 591 points] total loss per batch: 1.323
Policy (actual, predicted): 6 6
Policy data: tensor([0.1360, 0.1465, 0.1336, 0.1489, 0.1442, 0.1407, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1222, 0.1132, 0.1089, 0.1215, 0.1276, 0.1276, 0.2791],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.1692609041929245
 
[Iteration 3] Process ID: 188609 [Epoch: 52,   256/ 591 points] total loss per batch: 1.135
Policy (actual, predicted): 2 2
Policy data: tensor([0.1155, 0.0361, 0.7418, 0.0128, 0.0160, 0.0255, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.0924, 0.0289, 0.7787, 0.0121, 0.0161, 0.0206, 0.0511],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9738516211509705
 
[Iteration 3] Process ID: 188609 [Epoch: 52,   384/ 591 points] total loss per batch: 1.299
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 1.0168e-10, 1.8056e-09, 1.0412e-17, 1.0168e-20, 1.6816e-18,
        5.8635e-19], device='cuda:0')
Policy pred: tensor([9.9648e-01, 1.9396e-03, 1.0429e-03, 4.9768e-05, 1.1016e-04, 1.1105e-04,
        2.6334e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999919593334198
 
[Iteration 3] Process ID: 188609 [Epoch: 52,   512/ 591 points] total loss per batch: 1.315
Policy (actual, predicted): 0 0
Policy data: tensor([0.3856, 0.2287, 0.0586, 0.0211, 0.1959, 0.0437, 0.0665],
       device='cuda:0')
Policy pred: tensor([0.3156, 0.2294, 0.0872, 0.0293, 0.2180, 0.0537, 0.0668],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9993618726730347
 
[Iteration 3] Process ID: 188609 [Epoch: 53,   128/ 591 points] total loss per batch: 1.373
Policy (actual, predicted): 3 3
Policy data: tensor([0.0562, 0.0617, 0.0423, 0.6655, 0.0617, 0.0316, 0.0811],
       device='cuda:0')
Policy pred: tensor([0.0486, 0.0500, 0.0225, 0.7343, 0.0535, 0.0222, 0.0688],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999920129776001
 
[Iteration 3] Process ID: 188609 [Epoch: 53,   256/ 591 points] total loss per batch: 1.190
Policy (actual, predicted): 0 0
Policy data: tensor([0.2710, 0.1486, 0.1135, 0.0484, 0.1931, 0.0484, 0.1771],
       device='cuda:0')
Policy pred: tensor([0.2948, 0.1803, 0.0999, 0.0381, 0.1459, 0.0460, 0.1950],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9997240304946899
 
[Iteration 3] Process ID: 188609 [Epoch: 53,   384/ 591 points] total loss per batch: 1.315
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 7.3711e-12, 0.0000e+00, 2.3437e-13, 0.0000e+00, 4.7764e-14,
        1.9392e-12], device='cuda:0')
Policy pred: tensor([9.9942e-01, 1.5732e-04, 4.2137e-07, 4.8872e-06, 2.2499e-10, 1.8293e-06,
        4.1947e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999968409538269
 
[Iteration 3] Process ID: 188609 [Epoch: 53,   512/ 591 points] total loss per batch: 1.223
Policy (actual, predicted): 0 0
Policy data: tensor([9.9480e-01, 2.4485e-05, 1.0343e-15, 2.9215e-17, 3.2123e-04, 3.6062e-16,
        4.8568e-03], device='cuda:0')
Policy pred: tensor([9.6166e-01, 6.7709e-03, 4.2652e-04, 2.7520e-04, 1.3111e-03, 2.4824e-04,
        2.9304e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999817609786987
 
[Iteration 3] Process ID: 188609 [Epoch: 54,   128/ 591 points] total loss per batch: 1.313
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000, 0.0000, 0.2250, 0.0784, 0.3889, 0.0682, 0.2395],
       device='cuda:0')
Policy pred: tensor([2.3454e-05, 1.0795e-05, 2.3631e-01, 7.0742e-02, 3.8162e-01, 6.9932e-02,
        2.4135e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999379515647888
 
[Iteration 3] Process ID: 188609 [Epoch: 54,   256/ 591 points] total loss per batch: 1.340
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000, 0.0000, 0.2546, 0.1031, 0.3662, 0.0512, 0.2248],
       device='cuda:0')
Policy pred: tensor([4.2657e-05, 3.5328e-05, 2.7941e-01, 1.1169e-01, 3.8081e-01, 4.5787e-02,
        1.8223e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999114274978638
 
[Iteration 3] Process ID: 188609 [Epoch: 54,   384/ 591 points] total loss per batch: 1.238
Policy (actual, predicted): 2 2
Policy data: tensor([0.0468, 0.0409, 0.7888, 0.0226, 0.0527, 0.0226, 0.0257],
       device='cuda:0')
Policy pred: tensor([0.0471, 0.0386, 0.8161, 0.0147, 0.0389, 0.0230, 0.0216],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9995995163917542
 
[Iteration 3] Process ID: 188609 [Epoch: 54,   512/ 591 points] total loss per batch: 1.205
Policy (actual, predicted): 0 0
Policy data: tensor([0.6687, 0.0664, 0.1092, 0.0295, 0.0412, 0.0526, 0.0324],
       device='cuda:0')
Policy pred: tensor([0.6736, 0.0582, 0.1074, 0.0315, 0.0319, 0.0633, 0.0341],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9995677471160889
 
[Iteration 3] Process ID: 188609 [Epoch: 55,   128/ 591 points] total loss per batch: 1.326
Policy (actual, predicted): 3 3
Policy data: tensor([0.1407, 0.1465, 0.1372, 0.1512, 0.1372, 0.1383, 0.1489],
       device='cuda:0')
Policy pred: tensor([0.1399, 0.1495, 0.1410, 0.1526, 0.1408, 0.1363, 0.1401],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9986153841018677
 
[Iteration 3] Process ID: 188609 [Epoch: 55,   256/ 591 points] total loss per batch: 1.317
Policy (actual, predicted): 0 2
Policy data: tensor([0.4056, 0.0692, 0.3775, 0.0274, 0.0154, 0.0745, 0.0304],
       device='cuda:0')
Policy pred: tensor([0.3724, 0.0727, 0.3980, 0.0166, 0.0184, 0.0819, 0.0400],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.15554019808769226
 
[Iteration 3] Process ID: 188609 [Epoch: 55,   384/ 591 points] total loss per batch: 1.195
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 1.0168e-10, 1.8056e-09, 1.0412e-17, 1.0168e-20, 1.6816e-18,
        5.8635e-19], device='cuda:0')
Policy pred: tensor([9.9622e-01, 2.0209e-03, 1.3604e-03, 3.7727e-05, 6.1690e-05, 1.1039e-04,
        1.8739e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999839186668396
 
[Iteration 3] Process ID: 188609 [Epoch: 55,   512/ 591 points] total loss per batch: 1.273
Policy (actual, predicted): 2 2
Policy data: tensor([0.0468, 0.0409, 0.7888, 0.0226, 0.0527, 0.0226, 0.0257],
       device='cuda:0')
Policy pred: tensor([0.0718, 0.0425, 0.7840, 0.0165, 0.0428, 0.0208, 0.0216],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998152256011963
 
[Iteration 3] Process ID: 188609 [Epoch: 56,   128/ 591 points] total loss per batch: 1.263
Policy (actual, predicted): 1 0
Policy data: tensor([0.1419, 0.1465, 0.1419, 0.1430, 0.1454, 0.1360, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1767, 0.1444, 0.1257, 0.1213, 0.1461, 0.1451, 0.1407],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9994495511054993
 
[Iteration 3] Process ID: 188609 [Epoch: 56,   256/ 591 points] total loss per batch: 1.361
Policy (actual, predicted): 1 1
Policy data: tensor([0.3208, 0.3748, 0.0242, 0.0137, 0.1310, 0.0257, 0.1098],
       device='cuda:0')
Policy pred: tensor([0.3994, 0.4000, 0.0084, 0.0088, 0.0872, 0.0106, 0.0856],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999548196792603
 
[Iteration 3] Process ID: 188609 [Epoch: 56,   384/ 591 points] total loss per batch: 1.231
Policy (actual, predicted): 5 5
Policy data: tensor([6.2106e-05, 8.1482e-04, 7.7706e-11, 1.5128e-13, 1.1037e-10, 9.9470e-01,
        4.4263e-03], device='cuda:0')
Policy pred: tensor([2.1664e-03, 6.3813e-03, 8.3165e-04, 6.4736e-04, 9.1242e-04, 9.8809e-01,
        9.6716e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999934434890747
 
[Iteration 3] Process ID: 188609 [Epoch: 56,   512/ 591 points] total loss per batch: 1.197
Policy (actual, predicted): 3 3
Policy data: tensor([0.1687, 0.0000, 0.1583, 0.3738, 0.0000, 0.0219, 0.2773],
       device='cuda:0')
Policy pred: tensor([1.6256e-01, 2.8681e-04, 1.4536e-01, 4.1977e-01, 6.6210e-05, 2.0602e-02,
        2.5136e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999962449073792
 
[Iteration 3] Process ID: 188609 [Epoch: 57,   128/ 591 points] total loss per batch: 1.332
Policy (actual, predicted): 0 4
Policy data: tensor([0.3448, 0.1150, 0.1150, 0.0548, 0.2934, 0.0273, 0.0498],
       device='cuda:0')
Policy pred: tensor([0.3211, 0.0997, 0.1273, 0.0434, 0.3366, 0.0273, 0.0446],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9995517134666443
 
[Iteration 3] Process ID: 188609 [Epoch: 57,   256/ 591 points] total loss per batch: 1.190
Policy (actual, predicted): 0 0
Policy data: tensor([0.8966, 0.0098, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.7721, 0.0333, 0.0384, 0.0292, 0.0507, 0.0332, 0.0433],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.06771843880414963
 
[Iteration 3] Process ID: 188609 [Epoch: 57,   384/ 591 points] total loss per batch: 1.297
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 7.7439e-18, 0.0000e+00, 7.8521e-13, 0.0000e+00, 1.7656e-14,
        1.9406e-09], device='cuda:0')
Policy pred: tensor([9.9434e-01, 1.9445e-03, 9.6194e-05, 7.1263e-04, 3.2892e-06, 2.5172e-04,
        2.6546e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999639987945557
 
[Iteration 3] Process ID: 188609 [Epoch: 57,   512/ 591 points] total loss per batch: 1.254
Policy (actual, predicted): 5 5
Policy data: tensor([0.0717, 0.0227, 0.0163, 0.0129, 0.0646, 0.8006, 0.0113],
       device='cuda:0')
Policy pred: tensor([0.0612, 0.0314, 0.0172, 0.0169, 0.0865, 0.7662, 0.0206],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9967659711837769
 
[Iteration 3] Process ID: 188609 [Epoch: 58,   128/ 591 points] total loss per batch: 1.271
Policy (actual, predicted): 2 2
Policy data: tensor([0.0374, 0.0476, 0.7277, 0.0159, 0.0175, 0.0576, 0.0962],
       device='cuda:0')
Policy pred: tensor([0.0942, 0.0962, 0.4081, 0.0712, 0.1088, 0.1021, 0.1194],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9989676475524902
 
[Iteration 3] Process ID: 188609 [Epoch: 58,   256/ 591 points] total loss per batch: 1.235
Policy (actual, predicted): 4 0
Policy data: tensor([0.1466, 0.1419, 0.1489, 0.1313, 0.1524, 0.1360, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1572, 0.1399, 0.1384, 0.1335, 0.1538, 0.1385, 0.1387],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9858490228652954
 
[Iteration 3] Process ID: 188609 [Epoch: 58,   384/ 591 points] total loss per batch: 1.267
Policy (actual, predicted): 4 6
Policy data: tensor([0.1578, 0.1214, 0.1578, 0.0264, 0.2835, 0.0125, 0.2406],
       device='cuda:0')
Policy pred: tensor([0.1684, 0.1286, 0.1534, 0.0340, 0.2438, 0.0214, 0.2503],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999787211418152
 
[Iteration 3] Process ID: 188609 [Epoch: 58,   512/ 591 points] total loss per batch: 1.260
Policy (actual, predicted): 5 5
Policy data: tensor([0.0682, 0.0271, 0.0440, 0.0467, 0.3544, 0.3566, 0.1030],
       device='cuda:0')
Policy pred: tensor([0.0870, 0.0294, 0.0383, 0.0496, 0.3093, 0.3748, 0.1115],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9931476712226868
 
[Iteration 3] Process ID: 188609 [Epoch: 59,   128/ 591 points] total loss per batch: 1.394
Policy (actual, predicted): 4 4
Policy data: tensor([2.1681e-01, 1.4692e-01, 7.6242e-02, 1.0391e-04, 4.5245e-01, 1.0673e-03,
        1.0641e-01], device='cuda:0')
Policy pred: tensor([0.1867, 0.1160, 0.0745, 0.0016, 0.5085, 0.0035, 0.1092],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998211860657
 
[Iteration 3] Process ID: 188609 [Epoch: 59,   256/ 591 points] total loss per batch: 1.243
Policy (actual, predicted): 3 6
Policy data: tensor([0.0000, 0.0000, 0.2208, 0.2506, 0.1943, 0.1400, 0.1943],
       device='cuda:0')
Policy pred: tensor([1.3463e-04, 1.7874e-06, 1.8744e-01, 2.4155e-01, 1.6709e-01, 1.5358e-01,
        2.5020e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999994695186615
 
[Iteration 3] Process ID: 188609 [Epoch: 59,   384/ 591 points] total loss per batch: 1.256
Policy (actual, predicted): 1 1
Policy data: tensor([4.2423e-17, 1.0000e+00, 2.3890e-18, 2.3656e-16, 1.1161e-17, 3.8584e-19,
        1.0008e-18], device='cuda:0')
Policy pred: tensor([3.3303e-04, 9.9576e-01, 1.6887e-03, 7.4330e-04, 5.5101e-04, 5.9809e-04,
        3.2953e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9981648921966553
 
[Iteration 3] Process ID: 188609 [Epoch: 59,   512/ 591 points] total loss per batch: 1.204
Policy (actual, predicted): 5 5
Policy data: tensor([0.0044, 0.0044, 0.0063, 0.0044, 0.0044, 0.9699, 0.0063],
       device='cuda:0')
Policy pred: tensor([0.0016, 0.0048, 0.0081, 0.0054, 0.0029, 0.9656, 0.0116],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9421021938323975
 
[Iteration 3] Process ID: 188609 [Epoch: 60,   128/ 591 points] total loss per batch: 1.309
Policy (actual, predicted): 4 4
Policy data: tensor([0.1501, 0.1442, 0.1454, 0.1325, 0.1512, 0.1360, 0.1407],
       device='cuda:0')
Policy pred: tensor([0.1378, 0.1313, 0.1669, 0.1249, 0.1678, 0.1275, 0.1437],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9767657518386841
 
[Iteration 3] Process ID: 188609 [Epoch: 60,   256/ 591 points] total loss per batch: 1.326
Policy (actual, predicted): 1 1
Policy data: tensor([0.0330, 0.6655, 0.0208, 0.0094, 0.0192, 0.2410, 0.0111],
       device='cuda:0')
Policy pred: tensor([0.0649, 0.4979, 0.0368, 0.0118, 0.0304, 0.3435, 0.0147],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9956352114677429
 
[Iteration 3] Process ID: 188609 [Epoch: 60,   384/ 591 points] total loss per batch: 1.286
Policy (actual, predicted): 0 2
Policy data: tensor([0.1524, 0.1442, 0.1466, 0.1313, 0.1489, 0.1360, 0.1407],
       device='cuda:0')
Policy pred: tensor([0.1441, 0.1217, 0.1541, 0.1453, 0.1450, 0.1482, 0.1415],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9844468235969543
 
[Iteration 3] Process ID: 188609 [Epoch: 60,   512/ 591 points] total loss per batch: 1.187
Policy (actual, predicted): 0 0
Policy data: tensor([0.2785, 0.1397, 0.1668, 0.0548, 0.1527, 0.0548, 0.1527],
       device='cuda:0')
Policy pred: tensor([0.2631, 0.1387, 0.1668, 0.0736, 0.1297, 0.0750, 0.1531],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999927878379822
 
[Iteration 3] Process ID: 188609 [Epoch: 61,   128/ 591 points] total loss per batch: 1.243
Policy (actual, predicted): 2 2
Policy data: tensor([1.3347e-17, 1.0152e-12, 1.0000e+00, 1.0396e-19, 5.9948e-18, 2.9366e-11,
        2.8005e-17], device='cuda:0')
Policy pred: tensor([1.0540e-03, 2.3850e-03, 9.9153e-01, 6.2514e-04, 2.6330e-03, 9.0015e-04,
        8.7310e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9952912330627441
 
[Iteration 3] Process ID: 188609 [Epoch: 61,   256/ 591 points] total loss per batch: 1.271
Policy (actual, predicted): 6 0
Policy data: tensor([0.1419, 0.1419, 0.1465, 0.1442, 0.1419, 0.1336, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1675, 0.1374, 0.1583, 0.1308, 0.1665, 0.1147, 0.1248],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9983981847763062
 
[Iteration 3] Process ID: 188609 [Epoch: 61,   384/ 591 points] total loss per batch: 1.184
Policy (actual, predicted): 6 6
Policy data: tensor([6.0353e-19, 0.0000e+00, 0.0000e+00, 1.4753e-15, 2.2723e-22, 1.4070e-21,
        1.0000e+00], device='cuda:0')
Policy pred: tensor([9.1881e-05, 5.4187e-08, 2.9072e-05, 7.9341e-04, 2.2664e-03, 5.9665e-05,
        9.9676e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9947375655174255
 
[Iteration 3] Process ID: 188609 [Epoch: 61,   512/ 591 points] total loss per batch: 1.390
Policy (actual, predicted): 0 0
Policy data: tensor([0.8949, 0.0116, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.7217, 0.0349, 0.0426, 0.0379, 0.0742, 0.0348, 0.0540],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.026785438880324364
 
[Iteration 3] Process ID: 188609 [Epoch: 62,   128/ 591 points] total loss per batch: 1.169
Policy (actual, predicted): 2 2
Policy data: tensor([5.5501e-20, 5.0910e-11, 1.0000e+00, 1.8023e-19, 1.3407e-18, 5.4200e-23,
        1.4601e-20], device='cuda:0')
Policy pred: tensor([2.4443e-03, 4.6938e-03, 9.7927e-01, 5.2475e-03, 5.2617e-03, 9.3891e-04,
        2.1449e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9995317459106445
 
[Iteration 3] Process ID: 188609 [Epoch: 62,   256/ 591 points] total loss per batch: 1.194
Policy (actual, predicted): 2 2
Policy data: tensor([0.1829, 0.1103, 0.5624, 0.0123, 0.0123, 0.0421, 0.0776],
       device='cuda:0')
Policy pred: tensor([0.2195, 0.1513, 0.4532, 0.0173, 0.0223, 0.0606, 0.0758],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9566618204116821
 
[Iteration 3] Process ID: 188609 [Epoch: 62,   384/ 591 points] total loss per batch: 1.246
Policy (actual, predicted): 3 3
Policy data: tensor([0.1524, 0.1524, 0.0799, 0.2164, 0.1162, 0.1162, 0.1666],
       device='cuda:0')
Policy pred: tensor([0.1393, 0.1556, 0.0745, 0.2135, 0.1278, 0.1156, 0.1737],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999979138374329
 
[Iteration 3] Process ID: 188609 [Epoch: 62,   512/ 591 points] total loss per batch: 1.360
Policy (actual, predicted): 1 4
Policy data: tensor([0.1454, 0.1477, 0.1360, 0.1465, 0.1442, 0.1348, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.0905, 0.0718, 0.0491, 0.0542, 0.6115, 0.0664, 0.0564],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9965521693229675
 
[Iteration 3] Process ID: 188609 [Epoch: 63,   128/ 591 points] total loss per batch: 1.187
Policy (actual, predicted): 4 6
Policy data: tensor([0.1607, 0.1026, 0.1472, 0.0479, 0.2469, 0.0479, 0.2469],
       device='cuda:0')
Policy pred: tensor([0.1497, 0.0824, 0.1627, 0.0457, 0.2375, 0.0487, 0.2734],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998807907104
 
[Iteration 3] Process ID: 188609 [Epoch: 63,   256/ 591 points] total loss per batch: 1.267
Policy (actual, predicted): 4 4
Policy data: tensor([0.1477, 0.1430, 0.1407, 0.1336, 0.1594, 0.1336, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1285, 0.1404, 0.1597, 0.1271, 0.1741, 0.1150, 0.1552],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999997615814209
 
[Iteration 3] Process ID: 188609 [Epoch: 63,   384/ 591 points] total loss per batch: 1.261
Policy (actual, predicted): 1 2
Policy data: tensor([0.1336, 0.1489, 0.1489, 0.1454, 0.1407, 0.1372, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1141, 0.1607, 0.1633, 0.1372, 0.1514, 0.1329, 0.1403],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999889135360718
 
[Iteration 3] Process ID: 188609 [Epoch: 63,   512/ 591 points] total loss per batch: 1.335
Policy (actual, predicted): 0 0
Policy data: tensor([0.2726, 0.1151, 0.1639, 0.0448, 0.2509, 0.0269, 0.1258],
       device='cuda:0')
Policy pred: tensor([0.3020, 0.1191, 0.1788, 0.0433, 0.2171, 0.0346, 0.1051],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999974370002747
 
[Iteration 3] Process ID: 188609 [Epoch: 64,   128/ 591 points] total loss per batch: 1.277
Policy (actual, predicted): 0 0
Policy data: tensor([0.6231, 0.1329, 0.1020, 0.0218, 0.0187, 0.0494, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.6393, 0.1140, 0.0999, 0.0237, 0.0214, 0.0509, 0.0507],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.0010635923827067018
 
[Iteration 3] Process ID: 188609 [Epoch: 64,   256/ 591 points] total loss per batch: 1.316
Policy (actual, predicted): 0 0
Policy data: tensor([0.7867, 0.0395, 0.0146, 0.0041, 0.0868, 0.0425, 0.0258],
       device='cuda:0')
Policy pred: tensor([0.8085, 0.0319, 0.0198, 0.0087, 0.0729, 0.0408, 0.0175],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9979395866394043
 
[Iteration 3] Process ID: 188609 [Epoch: 64,   384/ 591 points] total loss per batch: 1.233
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1210, 0.4379, 0.1309, 0.0689, 0.0771, 0.0783, 0.0859],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 9.94652509689331e-05
 
[Iteration 3] Process ID: 188609 [Epoch: 64,   512/ 591 points] total loss per batch: 1.283
Policy (actual, predicted): 3 3
Policy data: tensor([0.1010, 0.0631, 0.0695, 0.3116, 0.2641, 0.0695, 0.1211],
       device='cuda:0')
Policy pred: tensor([0.1082, 0.0533, 0.0669, 0.3267, 0.2747, 0.0462, 0.1240],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999653100967407
 
[Iteration 3] Process ID: 188609 [Epoch: 65,   128/ 591 points] total loss per batch: 1.092
Policy (actual, predicted): 0 0
Policy data: tensor([0.3022, 0.1275, 0.1665, 0.0604, 0.1665, 0.0604, 0.1165],
       device='cuda:0')
Policy pred: tensor([0.3215, 0.1303, 0.1623, 0.0510, 0.1755, 0.0536, 0.1059],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9993165731430054
 
[Iteration 3] Process ID: 188609 [Epoch: 65,   256/ 591 points] total loss per batch: 1.242
Policy (actual, predicted): 6 6
Policy data: tensor([0.2152, 0.0000, 0.2019, 0.1041, 0.0000, 0.2019, 0.2768],
       device='cuda:0')
Policy pred: tensor([2.4246e-01, 2.3471e-05, 1.7833e-01, 9.8391e-02, 9.5997e-05, 1.9286e-01,
        2.8784e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999995231628418
 
[Iteration 3] Process ID: 188609 [Epoch: 65,   384/ 591 points] total loss per batch: 1.385
Policy (actual, predicted): 5 5
Policy data: tensor([0.0313, 0.1597, 0.0387, 0.0475, 0.0143, 0.6910, 0.0175],
       device='cuda:0')
Policy pred: tensor([0.0169, 0.1104, 0.0244, 0.0304, 0.0050, 0.7984, 0.0144],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999599456787109
 
[Iteration 3] Process ID: 188609 [Epoch: 65,   512/ 591 points] total loss per batch: 1.333
Policy (actual, predicted): 5 3
Policy data: tensor([0.0000, 0.0000, 0.0847, 0.1998, 0.0737, 0.4154, 0.2265],
       device='cuda:0')
Policy pred: tensor([1.4196e-03, 1.7306e-04, 8.2031e-02, 3.7616e-01, 3.5596e-02, 3.2805e-01,
        1.7657e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999775886535645
 
[Iteration 3] Process ID: 188609 [Epoch: 66,   128/ 591 points] total loss per batch: 1.197
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 3.2985e-10, 8.8103e-19, 5.0805e-17, 5.0805e-17,
        8.4022e-15], device='cuda:0')
Policy pred: tensor([6.9568e-08, 9.9891e-01, 7.9514e-04, 4.5776e-06, 2.2945e-04, 1.8703e-05,
        4.3226e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9960599541664124
 
[Iteration 3] Process ID: 188609 [Epoch: 66,   256/ 591 points] total loss per batch: 1.207
Policy (actual, predicted): 0 0
Policy data: tensor([0.8949, 0.0116, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.7525, 0.0365, 0.0403, 0.0331, 0.0557, 0.0328, 0.0491],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.028149884194135666
 
[Iteration 3] Process ID: 188609 [Epoch: 66,   384/ 591 points] total loss per batch: 1.310
Policy (actual, predicted): 2 2
Policy data: tensor([0.0767, 0.2093, 0.5781, 0.0074, 0.0686, 0.0336, 0.0263],
       device='cuda:0')
Policy pred: tensor([0.0820, 0.2194, 0.5618, 0.0078, 0.0752, 0.0245, 0.0293],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9997975826263428
 
[Iteration 3] Process ID: 188609 [Epoch: 66,   512/ 591 points] total loss per batch: 1.298
Policy (actual, predicted): 2 2
Policy data: tensor([0.1917, 0.1435, 0.5590, 0.0247, 0.0232, 0.0407, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.2041, 0.1652, 0.5188, 0.0261, 0.0246, 0.0449, 0.0164],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9949833750724792
 
[Iteration 3] Process ID: 188609 [Epoch: 67,   128/ 591 points] total loss per batch: 1.295
Policy (actual, predicted): 3 3
Policy data: tensor([0.0665, 0.0665, 0.0879, 0.5117, 0.0963, 0.0452, 0.1260],
       device='cuda:0')
Policy pred: tensor([0.0814, 0.0541, 0.1092, 0.4716, 0.1077, 0.0446, 0.1315],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999814629554749
 
[Iteration 3] Process ID: 188609 [Epoch: 67,   256/ 591 points] total loss per batch: 1.275
Policy (actual, predicted): 6 2
Policy data: tensor([0.1395, 0.1465, 0.1348, 0.1489, 0.1407, 0.1383, 0.1512],
       device='cuda:0')
Policy pred: tensor([0.1313, 0.1473, 0.1523, 0.1306, 0.1482, 0.1424, 0.1479],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9987322092056274
 
[Iteration 3] Process ID: 188609 [Epoch: 67,   384/ 591 points] total loss per batch: 1.271
Policy (actual, predicted): 0 0
Policy data: tensor([9.9660e-01, 0.0000e+00, 9.8604e-05, 1.8227e-05, 1.8227e-05, 3.9518e-05,
        3.2289e-03], device='cuda:0')
Policy pred: tensor([9.9525e-01, 2.6615e-05, 1.0144e-03, 8.7860e-04, 1.8149e-04, 8.5628e-04,
        1.7891e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999993443489075
 
[Iteration 3] Process ID: 188609 [Epoch: 67,   512/ 591 points] total loss per batch: 1.250
Policy (actual, predicted): 1 1
Policy data: tensor([1.2350e-18, 1.0000e+00, 3.4590e-21, 3.7141e-22, 2.0171e-07, 6.4565e-10,
        3.8945e-16], device='cuda:0')
Policy pred: tensor([2.2057e-04, 9.9850e-01, 1.7154e-04, 2.4975e-04, 1.8926e-04, 4.9444e-05,
        6.2134e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9993280172348022
 
[Iteration 3] Process ID: 188609 [Epoch: 68,   128/ 591 points] total loss per batch: 1.211
Policy (actual, predicted): 2 1
Policy data: tensor([0.2128, 0.2892, 0.4030, 0.0122, 0.0122, 0.0583, 0.0122],
       device='cuda:0')
Policy pred: tensor([0.2158, 0.4096, 0.2547, 0.0136, 0.0230, 0.0658, 0.0175],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9873912334442139
 
[Iteration 3] Process ID: 188609 [Epoch: 68,   256/ 591 points] total loss per batch: 1.199
Policy (actual, predicted): 2 0
Policy data: tensor([0.1489, 0.1430, 0.1512, 0.1336, 0.1501, 0.1336, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1562, 0.1325, 0.1379, 0.1535, 0.1309, 0.1465, 0.1426],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998156428337097
 
[Iteration 3] Process ID: 188609 [Epoch: 68,   384/ 591 points] total loss per batch: 1.403
Policy (actual, predicted): 0 0
Policy data: tensor([0.4016, 0.0000, 0.0630, 0.0000, 0.0000, 0.3145, 0.2210],
       device='cuda:0')
Policy pred: tensor([3.8400e-01, 1.6430e-05, 7.1706e-02, 8.4833e-04, 1.0112e-05, 3.0373e-01,
        2.3970e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.99997478723526
 
[Iteration 3] Process ID: 188609 [Epoch: 68,   512/ 591 points] total loss per batch: 1.262
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.0788, 0.1151, 0.0103, 0.2357, 0.1455, 0.3346],
       device='cuda:0')
Policy pred: tensor([0.0586, 0.0698, 0.1349, 0.0130, 0.2511, 0.1698, 0.3029],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9628315567970276
 
[Iteration 3] Process ID: 188609 [Epoch: 69,   128/ 591 points] total loss per batch: 1.321
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9906e-01, 0.0000e+00, 9.3862e-04,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([2.2409e-05, 2.3571e-06, 1.6485e-05, 9.9973e-01, 1.0184e-06, 2.0526e-04,
        2.2738e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9904899001121521
 
[Iteration 3] Process ID: 188609 [Epoch: 69,   256/ 591 points] total loss per batch: 1.324
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0023, 0.0000, 0.9977, 0.0000],
       device='cuda:0')
Policy pred: tensor([2.8268e-04, 1.6613e-05, 2.9363e-04, 1.5753e-02, 5.7084e-06, 9.8353e-01,
        1.1916e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998409152030945
 
[Iteration 3] Process ID: 188609 [Epoch: 69,   384/ 591 points] total loss per batch: 1.277
Policy (actual, predicted): 2 2
Policy data: tensor([0.1977, 0.1435, 0.5532, 0.0247, 0.0232, 0.0407, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.1832, 0.1307, 0.5883, 0.0210, 0.0209, 0.0394, 0.0165],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9943642020225525
 
[Iteration 3] Process ID: 188609 [Epoch: 69,   512/ 591 points] total loss per batch: 1.136
Policy (actual, predicted): 2 2
Policy data: tensor([0.0169, 0.0099, 0.9266, 0.0099, 0.0117, 0.0099, 0.0152],
       device='cuda:0')
Policy pred: tensor([0.0216, 0.0163, 0.9051, 0.0115, 0.0133, 0.0114, 0.0208],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9213078022003174
 
[Iteration 3] Process ID: 188609 [Epoch: 70,   128/ 591 points] total loss per batch: 1.236
Policy (actual, predicted): 6 6
Policy data: tensor([0.0117, 0.0134, 0.0099, 0.0081, 0.0168, 0.0168, 0.9233],
       device='cuda:0')
Policy pred: tensor([0.1130, 0.1111, 0.1189, 0.1300, 0.1201, 0.1260, 0.2810],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.17840857803821564
 
[Iteration 3] Process ID: 188609 [Epoch: 70,   256/ 591 points] total loss per batch: 1.311
Policy (actual, predicted): 0 0
Policy data: tensor([0.8966, 0.0098, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.7241, 0.0386, 0.0428, 0.0391, 0.0579, 0.0422, 0.0553],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.012422310188412666
 
[Iteration 3] Process ID: 188609 [Epoch: 70,   384/ 591 points] total loss per batch: 1.249
Policy (actual, predicted): 1 1
Policy data: tensor([2.9505e-19, 9.9981e-01, 6.4039e-15, 1.9154e-04, 6.0232e-16, 1.0200e-20,
        6.4039e-15], device='cuda:0')
Policy pred: tensor([5.9582e-04, 9.9603e-01, 6.3871e-04, 1.5757e-03, 4.5670e-05, 4.7329e-04,
        6.3942e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999984502792358
 
[Iteration 3] Process ID: 188609 [Epoch: 70,   512/ 591 points] total loss per batch: 1.326
Policy (actual, predicted): 6 1
Policy data: tensor([0.1407, 0.1442, 0.1419, 0.1336, 0.1407, 0.1384, 0.1605],
       device='cuda:0')
Policy pred: tensor([0.1364, 0.1833, 0.1625, 0.1418, 0.1147, 0.1022, 0.1591],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999914765357971
 
[Iteration 3] Process ID: 188609 [Epoch: 71,   128/ 591 points] total loss per batch: 1.211
Policy (actual, predicted): 4 0
Policy data: tensor([0.1501, 0.1430, 0.1442, 0.1313, 0.1559, 0.1360, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1670, 0.1606, 0.1401, 0.1127, 0.1454, 0.1389, 0.1353],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.924898087978363
 
[Iteration 3] Process ID: 188609 [Epoch: 71,   256/ 591 points] total loss per batch: 1.187
Policy (actual, predicted): 0 0
Policy data: tensor([0.5066, 0.0453, 0.1017, 0.0668, 0.0548, 0.1130, 0.1118],
       device='cuda:0')
Policy pred: tensor([0.5156, 0.0425, 0.0824, 0.0637, 0.0518, 0.1123, 0.1318],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998874068260193
 
[Iteration 3] Process ID: 188609 [Epoch: 71,   384/ 591 points] total loss per batch: 1.361
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0048, 0.0000, 0.9952, 0.0000],
       device='cuda:0')
Policy pred: tensor([1.8227e-04, 8.0292e-06, 1.5433e-04, 7.5647e-03, 1.3916e-06, 9.9137e-01,
        7.1816e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999967098236084
 
[Iteration 3] Process ID: 188609 [Epoch: 71,   512/ 591 points] total loss per batch: 1.208
Policy (actual, predicted): 2 2
Policy data: tensor([0.1282, 0.1678, 0.1833, 0.1536, 0.0732, 0.1536, 0.1404],
       device='cuda:0')
Policy pred: tensor([0.1285, 0.1491, 0.2043, 0.1579, 0.0782, 0.1394, 0.1426],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999962449073792
 
[Iteration 3] Process ID: 188609 [Epoch: 72,   128/ 591 points] total loss per batch: 1.269
Policy (actual, predicted): 4 4
Policy data: tensor([0.2260, 0.1036, 0.0947, 0.0365, 0.4309, 0.0365, 0.0718],
       device='cuda:0')
Policy pred: tensor([0.2023, 0.0930, 0.0796, 0.0340, 0.4886, 0.0351, 0.0675],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9994509816169739
 
[Iteration 3] Process ID: 188609 [Epoch: 72,   256/ 591 points] total loss per batch: 1.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0515, 0.2798, 0.0515, 0.0993, 0.1190, 0.0423, 0.3567],
       device='cuda:0')
Policy pred: tensor([0.0578, 0.2616, 0.0560, 0.0855, 0.1499, 0.0387, 0.3506],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999984860420227
 
[Iteration 3] Process ID: 188609 [Epoch: 72,   384/ 591 points] total loss per batch: 1.184
Policy (actual, predicted): 4 4
Policy data: tensor([0.0755, 0.0567, 0.1094, 0.2027, 0.3624, 0.0624, 0.1310],
       device='cuda:0')
Policy pred: tensor([0.0913, 0.0620, 0.1224, 0.2288, 0.2842, 0.0810, 0.1302],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999997079372406
 
[Iteration 3] Process ID: 188609 [Epoch: 72,   512/ 591 points] total loss per batch: 1.220
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0087, 0.0118, 0.0186, 0.0070, 0.0230, 0.0164, 0.9145],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.3021106421947479
 
[Iteration 3] Process ID: 188609 [Epoch: 73,   128/ 591 points] total loss per batch: 1.274
Policy (actual, predicted): 1 1
Policy data: tensor([4.3506e-03, 9.6874e-01, 3.9572e-03, 1.5618e-04, 1.6343e-02, 1.5618e-04,
        6.3011e-03], device='cuda:0')
Policy pred: tensor([0.0053, 0.9675, 0.0050, 0.0016, 0.0102, 0.0027, 0.0077],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999954104423523
 
[Iteration 3] Process ID: 188609 [Epoch: 73,   256/ 591 points] total loss per batch: 1.310
Policy (actual, predicted): 4 4
Policy data: tensor([0.2228, 0.1722, 0.1008, 0.1104, 0.2863, 0.0314, 0.0762],
       device='cuda:0')
Policy pred: tensor([0.2124, 0.1458, 0.1082, 0.1300, 0.2902, 0.0344, 0.0791],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9991136193275452
 
[Iteration 3] Process ID: 188609 [Epoch: 73,   384/ 591 points] total loss per batch: 1.351
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.0788, 0.1151, 0.0103, 0.2357, 0.1455, 0.3346],
       device='cuda:0')
Policy pred: tensor([0.0702, 0.0509, 0.0888, 0.0112, 0.2709, 0.1493, 0.3586],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9881138205528259
 
[Iteration 3] Process ID: 188609 [Epoch: 73,   512/ 591 points] total loss per batch: 1.233
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 1.3453e-19, 0.0000e+00, 3.8584e-19, 1.0000e+00, 5.1790e-13,
        1.0248e-15], device='cuda:0')
Policy pred: tensor([3.0871e-07, 1.4334e-03, 7.7257e-06, 5.2718e-06, 9.9854e-01, 6.0881e-06,
        1.0005e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9992631077766418
 
[Iteration 3] Process ID: 188609 [Epoch: 74,   128/ 591 points] total loss per batch: 1.295
Policy (actual, predicted): 2 5
Policy data: tensor([0.1282, 0.1678, 0.1833, 0.1536, 0.0732, 0.1536, 0.1404],
       device='cuda:0')
Policy pred: tensor([0.1176, 0.1505, 0.1660, 0.1676, 0.0735, 0.1704, 0.1545],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999960660934448
 
[Iteration 3] Process ID: 188609 [Epoch: 74,   256/ 591 points] total loss per batch: 1.331
Policy (actual, predicted): 4 4
Policy data: tensor([0.0183, 0.0297, 0.0133, 0.0150, 0.8920, 0.0167, 0.0150],
       device='cuda:0')
Policy pred: tensor([0.0191, 0.0255, 0.0070, 0.0103, 0.9029, 0.0174, 0.0178],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9967936873435974
 
[Iteration 3] Process ID: 188609 [Epoch: 74,   384/ 591 points] total loss per batch: 1.290
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1263, 0.4184, 0.1541, 0.0673, 0.0733, 0.0743, 0.0862],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.0026885876432061195
 
[Iteration 3] Process ID: 188609 [Epoch: 74,   512/ 591 points] total loss per batch: 1.219
Policy (actual, predicted): 1 0
Policy data: tensor([0.1454, 0.1500, 0.1465, 0.1325, 0.1477, 0.1360, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1867, 0.1377, 0.1486, 0.1171, 0.1284, 0.1382, 0.1433],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999846816062927
 
[Iteration 3] Process ID: 188609 [Epoch: 75,   128/ 591 points] total loss per batch: 1.218
Policy (actual, predicted): 6 6
Policy data: tensor([0.0953, 0.1405, 0.0144, 0.0094, 0.0144, 0.0111, 0.7149],
       device='cuda:0')
Policy pred: tensor([0.0750, 0.1294, 0.0080, 0.0054, 0.0095, 0.0053, 0.7674],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9979490637779236
 
[Iteration 3] Process ID: 188609 [Epoch: 75,   256/ 591 points] total loss per batch: 1.144
Policy (actual, predicted): 2 2
Policy data: tensor([0.1156, 0.0361, 0.7432, 0.0128, 0.0160, 0.0240, 0.0523],
       device='cuda:0')
Policy pred: tensor([0.1233, 0.0399, 0.7185, 0.0149, 0.0166, 0.0255, 0.0612],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9842550158500671
 
[Iteration 3] Process ID: 188609 [Epoch: 75,   384/ 591 points] total loss per batch: 1.288
Policy (actual, predicted): 6 6
Policy data: tensor([0.1271, 0.1174, 0.1498, 0.0625, 0.1814, 0.0573, 0.3045],
       device='cuda:0')
Policy pred: tensor([0.1496, 0.1462, 0.1351, 0.0608, 0.1971, 0.0552, 0.2559],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9942972660064697
 
[Iteration 3] Process ID: 188609 [Epoch: 75,   512/ 591 points] total loss per batch: 1.345
Policy (actual, predicted): 0 0
Policy data: tensor([7.4563e-01, 4.7480e-04, 2.5335e-01, 7.2741e-10, 6.9555e-05, 3.4502e-07,
        4.7480e-04], device='cuda:0')
Policy pred: tensor([7.5537e-01, 4.0244e-03, 2.3497e-01, 5.3896e-04, 3.6121e-03, 3.6602e-04,
        1.1206e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999997019767761
 
[Iteration 3] Process ID: 188609 [Epoch: 76,   128/ 591 points] total loss per batch: 1.278
Policy (actual, predicted): 2 2
Policy data: tensor([0.2808, 0.0266, 0.5874, 0.0173, 0.0220, 0.0471, 0.0189],
       device='cuda:0')
Policy pred: tensor([0.2991, 0.0276, 0.5663, 0.0159, 0.0217, 0.0468, 0.0226],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9933943152427673
 
[Iteration 3] Process ID: 188609 [Epoch: 76,   256/ 591 points] total loss per batch: 1.278
Policy (actual, predicted): 4 1
Policy data: tensor([0.1512, 0.1454, 0.1477, 0.1289, 0.1524, 0.1360, 0.1384],
       device='cuda:0')
Policy pred: tensor([0.1043, 0.4772, 0.1319, 0.0621, 0.0756, 0.0735, 0.0754],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.0018807105952873826
 
[Iteration 3] Process ID: 188609 [Epoch: 76,   384/ 591 points] total loss per batch: 1.241
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 2.5725e-22, 0.0000e+00, 1.0000e+00, 1.6310e-18, 1.5928e-21,
        2.9658e-14], device='cuda:0')
Policy pred: tensor([1.7673e-04, 4.1437e-04, 8.3249e-06, 9.9749e-01, 7.0287e-04, 3.9196e-04,
        8.1887e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9987431168556213
 
[Iteration 3] Process ID: 188609 [Epoch: 76,   512/ 591 points] total loss per batch: 1.342
Policy (actual, predicted): 6 0
Policy data: tensor([0.1419, 0.1454, 0.1348, 0.1465, 0.1430, 0.1383, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1689, 0.1035, 0.1309, 0.1553, 0.1336, 0.1533, 0.1545],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999987006187439
 
[Iteration 3] Process ID: 188609 [Epoch: 77,   128/ 591 points] total loss per batch: 1.255
Policy (actual, predicted): 1 1
Policy data: tensor([0.0149, 0.3927, 0.1064, 0.1443, 0.0000, 0.0436, 0.2980],
       device='cuda:0')
Policy pred: tensor([1.5217e-02, 4.3459e-01, 9.2526e-02, 1.5569e-01, 1.8758e-05, 3.4034e-02,
        2.6792e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999984085559845
 
[Iteration 3] Process ID: 188609 [Epoch: 77,   256/ 591 points] total loss per batch: 1.345
Policy (actual, predicted): 2 2
Policy data: tensor([1.3347e-17, 1.0152e-12, 1.0000e+00, 1.0396e-19, 5.9948e-18, 2.9366e-11,
        2.8005e-17], device='cuda:0')
Policy pred: tensor([8.4341e-04, 5.6417e-03, 9.8952e-01, 5.9485e-04, 1.6695e-03, 9.4173e-04,
        7.9013e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9907935857772827
 
[Iteration 3] Process ID: 188609 [Epoch: 77,   384/ 591 points] total loss per batch: 1.298
Policy (actual, predicted): 3 3
Policy data: tensor([0.0738, 0.0904, 0.2061, 0.3947, 0.1288, 0.0712, 0.0351],
       device='cuda:0')
Policy pred: tensor([0.0596, 0.1215, 0.2454, 0.2534, 0.1939, 0.0767, 0.0495],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998727440834045
 
[Iteration 3] Process ID: 188609 [Epoch: 77,   512/ 591 points] total loss per batch: 1.176
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 0.0000e+00, 8.6775e-11, 0.0000e+00, 4.4870e-10,
        6.3314e-09], device='cuda:0')
Policy pred: tensor([3.4945e-08, 9.9838e-01, 4.7374e-06, 1.0504e-04, 8.2408e-08, 4.7845e-05,
        1.4586e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999803900718689
 
[Iteration 3] Process ID: 188609 [Epoch: 78,   128/ 591 points] total loss per batch: 1.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0926, 0.1207, 0.1029, 0.0514, 0.0332, 0.0374, 0.5617],
       device='cuda:0')
Policy pred: tensor([0.0998, 0.1090, 0.1125, 0.0509, 0.0364, 0.0333, 0.5581],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9997538924217224
 
[Iteration 3] Process ID: 188609 [Epoch: 78,   256/ 591 points] total loss per batch: 1.255
Policy (actual, predicted): 3 3
Policy data: tensor([1.6772e-04, 0.0000e+00, 5.7598e-04, 9.9391e-01, 6.8750e-04, 1.3587e-05,
        4.6453e-03], device='cuda:0')
Policy pred: tensor([7.2701e-04, 1.0955e-04, 1.1606e-03, 9.9260e-01, 1.2180e-03, 6.9800e-05,
        4.1120e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9936953783035278
 
[Iteration 3] Process ID: 188609 [Epoch: 78,   384/ 591 points] total loss per batch: 1.267
Policy (actual, predicted): 2 0
Policy data: tensor([0.1466, 0.1489, 0.1501, 0.1313, 0.1489, 0.1301, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1622, 0.1167, 0.1419, 0.1402, 0.1438, 0.1462, 0.1490],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9995018839836121
 
[Iteration 3] Process ID: 188609 [Epoch: 78,   512/ 591 points] total loss per batch: 1.228
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.0000, 0.2208, 0.2506, 0.1943, 0.1400, 0.1943],
       device='cuda:0')
Policy pred: tensor([7.7899e-05, 1.4235e-06, 2.0699e-01, 2.2559e-01, 1.9979e-01, 1.5665e-01,
        2.1090e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999918341636658
 
[Iteration 3] Process ID: 188609 [Epoch: 79,   128/ 591 points] total loss per batch: 1.377
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1256, 0.3437, 0.1485, 0.0875, 0.0957, 0.1042, 0.0948],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.0057971724309027195
 
[Iteration 3] Process ID: 188609 [Epoch: 79,   256/ 591 points] total loss per batch: 1.160
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.0000, 0.2208, 0.2506, 0.1943, 0.1400, 0.1943],
       device='cuda:0')
Policy pred: tensor([2.0747e-04, 4.9680e-06, 2.2299e-01, 2.9937e-01, 1.6853e-01, 1.2930e-01,
        1.7960e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999927282333374
 
[Iteration 3] Process ID: 188609 [Epoch: 79,   384/ 591 points] total loss per batch: 1.247
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 4.6359e-12, 0.0000e+00, 1.6805e-17, 0.0000e+00, 3.7417e-17,
        6.7988e-07], device='cuda:0')
Policy pred: tensor([9.9911e-01, 5.6659e-04, 4.4339e-06, 4.3788e-05, 4.3409e-07, 6.6269e-06,
        2.6352e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999975323677063
 
[Iteration 3] Process ID: 188609 [Epoch: 79,   512/ 591 points] total loss per batch: 1.311
Policy (actual, predicted): 1 1
Policy data: tensor([0.0943, 0.2116, 0.1628, 0.1940, 0.0710, 0.1035, 0.1628],
       device='cuda:0')
Policy pred: tensor([0.1104, 0.2218, 0.1504, 0.1903, 0.0687, 0.1086, 0.1498],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999966025352478
 
[Iteration 3] Process ID: 188609 [Epoch: 80,   128/ 591 points] total loss per batch: 1.247
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 1.3434e-10, 0.0000e+00, 1.4982e-09, 1.0000e+00, 1.8493e-18,
        1.1451e-07], device='cuda:0')
Policy pred: tensor([8.6520e-07, 4.8675e-05, 1.1785e-04, 1.3214e-03, 9.9756e-01, 5.2564e-05,
        8.9469e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999577403068542
 
[Iteration 3] Process ID: 188609 [Epoch: 80,   256/ 591 points] total loss per batch: 1.236
Policy (actual, predicted): 1 1
Policy data: tensor([0.1738, 0.6543, 0.0834, 0.0015, 0.0541, 0.0044, 0.0285],
       device='cuda:0')
Policy pred: tensor([0.1330, 0.6953, 0.0873, 0.0047, 0.0502, 0.0061, 0.0233],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999877214431763
 
[Iteration 3] Process ID: 188609 [Epoch: 80,   384/ 591 points] total loss per batch: 1.344
Policy (actual, predicted): 2 2
Policy data: tensor([0.0762, 0.1530, 0.5734, 0.0074, 0.0775, 0.0894, 0.0231],
       device='cuda:0')
Policy pred: tensor([0.0954, 0.1800, 0.5474, 0.0089, 0.0749, 0.0692, 0.0242],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9972344040870667
 
[Iteration 3] Process ID: 188609 [Epoch: 80,   512/ 591 points] total loss per batch: 1.287
Policy (actual, predicted): 0 5
Policy data: tensor([0.4016, 0.0000, 0.0630, 0.0000, 0.0000, 0.3145, 0.2210],
       device='cuda:0')
Policy pred: tensor([3.4089e-01, 2.6760e-05, 6.9961e-02, 5.7203e-04, 4.7284e-05, 3.6475e-01,
        2.2375e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9958235621452332
 
[Iteration 3] Process ID: 188609 [Epoch: 81,   128/ 591 points] total loss per batch: 1.274
Policy (actual, predicted): 6 5
Policy data: tensor([0.1419, 0.1419, 0.1465, 0.1442, 0.1419, 0.1336, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1276, 0.1496, 0.1500, 0.1388, 0.1404, 0.1588, 0.1348],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9980633854866028
 
[Iteration 3] Process ID: 188609 [Epoch: 81,   256/ 591 points] total loss per batch: 1.264
Policy (actual, predicted): 2 2
Policy data: tensor([0.1925, 0.1132, 0.2284, 0.0322, 0.1925, 0.0646, 0.1766],
       device='cuda:0')
Policy pred: tensor([0.2310, 0.1294, 0.2515, 0.0306, 0.1411, 0.0538, 0.1627],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999985694885254
 
[Iteration 3] Process ID: 188609 [Epoch: 81,   384/ 591 points] total loss per batch: 1.261
Policy (actual, predicted): 2 2
Policy data: tensor([0.1419, 0.1477, 0.1489, 0.1442, 0.1336, 0.1372, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1554, 0.1370, 0.1763, 0.1274, 0.1503, 0.1159, 0.1376],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9994595050811768
 
[Iteration 3] Process ID: 188609 [Epoch: 81,   512/ 591 points] total loss per batch: 1.299
Policy (actual, predicted): 6 6
Policy data: tensor([0.1209, 0.0000, 0.0712, 0.3629, 0.0000, 0.0141, 0.4309],
       device='cuda:0')
Policy pred: tensor([1.0332e-01, 5.1607e-06, 6.3286e-02, 3.6918e-01, 8.3509e-06, 1.2315e-02,
        4.5189e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998807907104
 
[Iteration 3] Process ID: 188609 [Epoch: 82,   128/ 591 points] total loss per batch: 1.232
Policy (actual, predicted): 1 1
Policy data: tensor([4.3506e-03, 9.6874e-01, 3.9572e-03, 1.5618e-04, 1.6343e-02, 1.5618e-04,
        6.3011e-03], device='cuda:0')
Policy pred: tensor([0.0052, 0.9750, 0.0037, 0.0016, 0.0077, 0.0017, 0.0050],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999996423721313
 
[Iteration 3] Process ID: 188609 [Epoch: 82,   256/ 591 points] total loss per batch: 1.241
Policy (actual, predicted): 2 0
Policy data: tensor([0.2505, 0.0671, 0.2990, 0.2149, 0.0902, 0.0433, 0.0351],
       device='cuda:0')
Policy pred: tensor([0.2970, 0.0674, 0.2824, 0.2173, 0.0692, 0.0412, 0.0253],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9951237440109253
 
[Iteration 3] Process ID: 188609 [Epoch: 82,   384/ 591 points] total loss per batch: 1.231
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 7.3577e-09, 1.4096e-06, 5.9101e-21, 1.7095e-19, 8.3431e-17,
        3.6594e-20], device='cuda:0')
Policy pred: tensor([9.9530e-01, 3.0795e-03, 1.1162e-03, 1.8227e-05, 1.2370e-04, 7.2251e-05,
        2.9183e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.996320903301239
 
[Iteration 3] Process ID: 188609 [Epoch: 82,   512/ 591 points] total loss per batch: 1.337
Policy (actual, predicted): 6 6
Policy data: tensor([0.2152, 0.0000, 0.2019, 0.1041, 0.0000, 0.2019, 0.2768],
       device='cuda:0')
Policy pred: tensor([2.3270e-01, 7.4905e-06, 1.7476e-01, 1.1210e-01, 1.0816e-04, 2.1493e-01,
        2.6540e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999992847442627
 
[Iteration 3] Process ID: 188609 [Epoch: 83,   128/ 591 points] total loss per batch: 1.241
Policy (actual, predicted): 4 6
Policy data: tensor([0.1501, 0.1501, 0.1442, 0.1266, 0.1571, 0.1230, 0.1489],
       device='cuda:0')
Policy pred: tensor([0.1401, 0.1366, 0.1493, 0.1318, 0.1483, 0.1304, 0.1635],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999091625213623
 
[Iteration 3] Process ID: 188609 [Epoch: 83,   256/ 591 points] total loss per batch: 1.362
Policy (actual, predicted): 6 6
Policy data: tensor([0.0184, 0.0062, 0.0299, 0.0062, 0.0062, 0.0283, 0.9049],
       device='cuda:0')
Policy pred: tensor([0.0295, 0.0098, 0.0320, 0.0133, 0.0146, 0.0286, 0.8723],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9786865711212158
 
[Iteration 3] Process ID: 188609 [Epoch: 83,   384/ 591 points] total loss per batch: 1.246
Policy (actual, predicted): 4 4
Policy data: tensor([0.0867, 0.1944, 0.0789, 0.1143, 0.2510, 0.0441, 0.2306],
       device='cuda:0')
Policy pred: tensor([0.0782, 0.2096, 0.0843, 0.1080, 0.2433, 0.0460, 0.2305],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999836087226868
 
[Iteration 3] Process ID: 188609 [Epoch: 83,   512/ 591 points] total loss per batch: 1.243
Policy (actual, predicted): 1 6
Policy data: tensor([0.1407, 0.1512, 0.1454, 0.1430, 0.1372, 0.1360, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1271, 0.1558, 0.1341, 0.1514, 0.1225, 0.1484, 0.1606],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999229311943054
 
[Iteration 3] Process ID: 188609 [Epoch: 84,   128/ 591 points] total loss per batch: 1.157
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 2.3479e-07, 6.9915e-09, 3.0720e-19, 3.1457e-16, 1.7056e-09,
        1.1677e-18], device='cuda:0')
Policy pred: tensor([9.9877e-01, 4.2162e-04, 1.2375e-05, 4.2465e-05, 5.1465e-04, 3.6641e-05,
        2.0699e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999998211860657
 
[Iteration 3] Process ID: 188609 [Epoch: 84,   256/ 591 points] total loss per batch: 1.368
Policy (actual, predicted): 0 0
Policy data: tensor([0.8966, 0.0098, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.6886, 0.0408, 0.0491, 0.0435, 0.0735, 0.0427, 0.0616],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.04651868715882301
 
[Iteration 3] Process ID: 188609 [Epoch: 84,   384/ 591 points] total loss per batch: 1.239
Policy (actual, predicted): 1 6
Policy data: tensor([0.1395, 0.1489, 0.1419, 0.1465, 0.1407, 0.1395, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1505, 0.1411, 0.1358, 0.1202, 0.1438, 0.1524, 0.1562],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999679923057556
 
[Iteration 3] Process ID: 188609 [Epoch: 84,   512/ 591 points] total loss per batch: 1.326
Policy (actual, predicted): 3 3
Policy data: tensor([0.0665, 0.0665, 0.0879, 0.5117, 0.0963, 0.0452, 0.1260],
       device='cuda:0')
Policy pred: tensor([0.0595, 0.0700, 0.1029, 0.4970, 0.0986, 0.0413, 0.1308],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999720454216003
 
[Iteration 3] Process ID: 188609 [Epoch: 85,   128/ 591 points] total loss per batch: 1.350
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000e+00, 0.0000e+00, 2.8111e-08, 9.8017e-19, 3.0184e-09, 1.0037e-15,
        1.0000e+00], device='cuda:0')
Policy pred: tensor([2.1211e-06, 4.3711e-06, 3.1314e-03, 4.1337e-05, 1.5632e-05, 1.5764e-05,
        9.9679e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999978542327881
 
[Iteration 3] Process ID: 188609 [Epoch: 85,   256/ 591 points] total loss per batch: 1.171
Policy (actual, predicted): 6 6
Policy data: tensor([0.1360, 0.1465, 0.1336, 0.1489, 0.1442, 0.1407, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1140, 0.1157, 0.1042, 0.1129, 0.1091, 0.1092, 0.3349],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.17064900696277618
 
[Iteration 3] Process ID: 188609 [Epoch: 85,   384/ 591 points] total loss per batch: 1.269
Policy (actual, predicted): 6 6
Policy data: tensor([0.1291, 0.1977, 0.0124, 0.0124, 0.0124, 0.0619, 0.5741],
       device='cuda:0')
Policy pred: tensor([0.1250, 0.2229, 0.0889, 0.0610, 0.0731, 0.0917, 0.3373],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.17401887476444244
 
[Iteration 3] Process ID: 188609 [Epoch: 85,   512/ 591 points] total loss per batch: 1.329
Policy (actual, predicted): 6 2
Policy data: tensor([0.1430, 0.1442, 0.1348, 0.1454, 0.1407, 0.1454, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1539, 0.1240, 0.1616, 0.1392, 0.1311, 0.1521, 0.1382],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999622702598572
 
[Iteration 3] Process ID: 188609 [Epoch: 86,   128/ 591 points] total loss per batch: 1.312
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.2865, 0.2094, 0.2376, 0.0823, 0.1842],
       device='cuda:0')
Policy pred: tensor([5.6491e-05, 3.2189e-05, 3.0892e-01, 1.7376e-01, 2.0915e-01, 1.0176e-01,
        2.0631e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998708963394165
 
[Iteration 3] Process ID: 188609 [Epoch: 86,   256/ 591 points] total loss per batch: 1.272
Policy (actual, predicted): 0 0
Policy data: tensor([7.4563e-01, 4.7480e-04, 2.5335e-01, 7.2741e-10, 6.9555e-05, 3.4502e-07,
        4.7480e-04], device='cuda:0')
Policy pred: tensor([0.8705, 0.0048, 0.1165, 0.0012, 0.0042, 0.0015, 0.0012],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999967217445374
 
[Iteration 3] Process ID: 188609 [Epoch: 86,   384/ 591 points] total loss per batch: 1.232
Policy (actual, predicted): 6 2
Policy data: tensor([0.1419, 0.1489, 0.1395, 0.1419, 0.1383, 0.1383, 0.1512],
       device='cuda:0')
Policy pred: tensor([0.1292, 0.1564, 0.1687, 0.1255, 0.1261, 0.1499, 0.1442],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999939203262329
 
[Iteration 3] Process ID: 188609 [Epoch: 86,   512/ 591 points] total loss per batch: 1.217
Policy (actual, predicted): 0 0
Policy data: tensor([0.6244, 0.1329, 0.1033, 0.0218, 0.0187, 0.0466, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.6701, 0.1272, 0.0877, 0.0160, 0.0167, 0.0427, 0.0396],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.0014447484863922
 
[Iteration 3] Process ID: 188609 [Epoch: 87,   128/ 591 points] total loss per batch: 1.310
Policy (actual, predicted): 1 1
Policy data: tensor([0.1752, 0.2272, 0.0522, 0.1020, 0.1604, 0.1226, 0.1604],
       device='cuda:0')
Policy pred: tensor([0.1572, 0.2406, 0.0609, 0.0966, 0.1560, 0.1277, 0.1609],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9988762736320496
 
[Iteration 3] Process ID: 188609 [Epoch: 87,   256/ 591 points] total loss per batch: 1.317
Policy (actual, predicted): 6 0
Policy data: tensor([0.2673, 0.0000, 0.1837, 0.0324, 0.0000, 0.1958, 0.3208],
       device='cuda:0')
Policy pred: tensor([3.0105e-01, 4.1514e-05, 1.7055e-01, 4.5191e-02, 1.6630e-05, 1.9567e-01,
        2.8749e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 188609 [Epoch: 87,   384/ 591 points] total loss per batch: 1.234
Policy (actual, predicted): 1 1
Policy data: tensor([3.5696e-07, 1.0000e+00, 4.3223e-10, 5.2102e-13, 3.5401e-09, 1.2888e-08,
        1.2750e-07], device='cuda:0')
Policy pred: tensor([3.0609e-03, 9.9160e-01, 2.7035e-03, 3.6419e-04, 2.1817e-04, 1.7080e-03,
        3.4338e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999998927116394
 
[Iteration 3] Process ID: 188609 [Epoch: 87,   512/ 591 points] total loss per batch: 1.337
Policy (actual, predicted): 0 0
Policy data: tensor([0.9052, 0.0150, 0.0234, 0.0062, 0.0184, 0.0150, 0.0167],
       device='cuda:0')
Policy pred: tensor([0.9287, 0.0098, 0.0191, 0.0057, 0.0164, 0.0069, 0.0135],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9958347678184509
 
[Iteration 3] Process ID: 188609 [Epoch: 88,   128/ 591 points] total loss per batch: 1.274
Policy (actual, predicted): 2 2
Policy data: tensor([0.0215, 0.0263, 0.8668, 0.0114, 0.0311, 0.0182, 0.0247],
       device='cuda:0')
Policy pred: tensor([0.0173, 0.0174, 0.9149, 0.0070, 0.0181, 0.0113, 0.0139],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.24641284346580505
 
[Iteration 3] Process ID: 188609 [Epoch: 88,   256/ 591 points] total loss per batch: 1.321
Policy (actual, predicted): 5 5
Policy data: tensor([0.0044, 0.0044, 0.0044, 0.0118, 0.0063, 0.9644, 0.0044],
       device='cuda:0')
Policy pred: tensor([0.0059, 0.0043, 0.0082, 0.0102, 0.0050, 0.9603, 0.0061],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999948143959045
 
[Iteration 3] Process ID: 188609 [Epoch: 88,   384/ 591 points] total loss per batch: 1.144
Policy (actual, predicted): 3 0
Policy data: tensor([0.1430, 0.1465, 0.1372, 0.1500, 0.1407, 0.1348, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1612, 0.1356, 0.1350, 0.1224, 0.1390, 0.1462, 0.1606],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9873828291893005
 
[Iteration 3] Process ID: 188609 [Epoch: 88,   512/ 591 points] total loss per batch: 1.359
Policy (actual, predicted): 6 6
Policy data: tensor([0.1419, 0.1442, 0.1372, 0.1465, 0.1372, 0.1395, 0.1535],
       device='cuda:0')
Policy pred: tensor([0.1164, 0.1324, 0.1345, 0.1254, 0.1297, 0.1298, 0.2317],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.14940111339092255
 
[Iteration 3] Process ID: 188609 [Epoch: 89,   128/ 591 points] total loss per batch: 1.251
Policy (actual, predicted): 6 3
Policy data: tensor([1.2919e-02, 0.0000e+00, 4.8996e-04, 2.4311e-01, 1.7533e-02, 2.5643e-06,
        7.2595e-01], device='cuda:0')
Policy pred: tensor([2.1507e-02, 8.1698e-05, 1.6634e-03, 4.8094e-01, 2.6598e-02, 3.5868e-04,
        4.6886e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998303055763245
 
[Iteration 3] Process ID: 188609 [Epoch: 89,   256/ 591 points] total loss per batch: 1.236
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 4.6359e-12, 0.0000e+00, 1.6805e-17, 0.0000e+00, 3.7417e-17,
        6.7988e-07], device='cuda:0')
Policy pred: tensor([9.9907e-01, 6.4355e-04, 3.5123e-06, 5.4247e-05, 2.8220e-07, 5.2452e-06,
        2.2561e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999918937683105
 
[Iteration 3] Process ID: 188609 [Epoch: 89,   384/ 591 points] total loss per batch: 1.186
Policy (actual, predicted): 6 6
Policy data: tensor([0.1291, 0.1977, 0.0124, 0.0124, 0.0124, 0.0619, 0.5741],
       device='cuda:0')
Policy pred: tensor([0.1264, 0.2170, 0.0956, 0.0789, 0.0774, 0.1026, 0.3020],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.03710029646754265
 
[Iteration 3] Process ID: 188609 [Epoch: 89,   512/ 591 points] total loss per batch: 1.252
Policy (actual, predicted): 4 0
Policy data: tensor([0.1501, 0.1466, 0.1442, 0.1254, 0.1512, 0.1348, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1612, 0.1487, 0.1408, 0.1229, 0.1496, 0.1371, 0.1397],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999854564666748
 
[Iteration 3] Process ID: 188609 [Epoch: 90,   128/ 591 points] total loss per batch: 1.314
Policy (actual, predicted): 4 4
Policy data: tensor([0.1501, 0.1477, 0.1466, 0.1265, 0.1524, 0.1372, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1335, 0.1546, 0.1506, 0.1291, 0.1628, 0.1314, 0.1379],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999998927116394
 
[Iteration 3] Process ID: 188609 [Epoch: 90,   256/ 591 points] total loss per batch: 1.217
Policy (actual, predicted): 1 1
Policy data: tensor([0.0217, 0.8925, 0.0167, 0.0061, 0.0167, 0.0150, 0.0313],
       device='cuda:0')
Policy pred: tensor([0.0245, 0.8976, 0.0153, 0.0049, 0.0115, 0.0141, 0.0320],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9970542788505554
 
[Iteration 3] Process ID: 188609 [Epoch: 90,   384/ 591 points] total loss per batch: 1.287
Policy (actual, predicted): 6 3
Policy data: tensor([0.1419, 0.1477, 0.1360, 0.1430, 0.1430, 0.1383, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1551, 0.1472, 0.1206, 0.1712, 0.1621, 0.1203, 0.1235],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9859676957130432
 
[Iteration 3] Process ID: 188609 [Epoch: 90,   512/ 591 points] total loss per batch: 1.301
Policy (actual, predicted): 1 1
Policy data: tensor([0.0934, 0.6501, 0.1337, 0.0188, 0.0173, 0.0412, 0.0455],
       device='cuda:0')
Policy pred: tensor([0.1071, 0.3860, 0.1422, 0.0826, 0.0916, 0.0945, 0.0961],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.0017289501847699285
 
[Iteration 3] Process ID: 188609 [Epoch: 91,   128/ 591 points] total loss per batch: 1.214
Policy (actual, predicted): 2 2
Policy data: tensor([0.0484, 0.0335, 0.7996, 0.0304, 0.0320, 0.0320, 0.0242],
       device='cuda:0')
Policy pred: tensor([0.0493, 0.0336, 0.8143, 0.0209, 0.0298, 0.0277, 0.0244],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9927072525024414
 
[Iteration 3] Process ID: 188609 [Epoch: 91,   256/ 591 points] total loss per batch: 1.227
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0075, 0.0125, 0.0234, 0.0092, 0.0287, 0.0163, 0.9025],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.3411904275417328
 
[Iteration 3] Process ID: 188609 [Epoch: 91,   384/ 591 points] total loss per batch: 1.288
Policy (actual, predicted): 2 2
Policy data: tensor([0.2808, 0.0266, 0.5874, 0.0173, 0.0220, 0.0471, 0.0189],
       device='cuda:0')
Policy pred: tensor([0.3292, 0.0191, 0.5289, 0.0209, 0.0224, 0.0600, 0.0195],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9919084310531616
 
[Iteration 3] Process ID: 188609 [Epoch: 91,   512/ 591 points] total loss per batch: 1.281
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 1.3434e-10, 0.0000e+00, 1.4982e-09, 1.0000e+00, 1.8493e-18,
        1.1451e-07], device='cuda:0')
Policy pred: tensor([1.0090e-06, 4.9537e-05, 1.1475e-04, 5.9826e-04, 9.9829e-01, 2.5920e-05,
        9.2041e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998494982719421
 
[Iteration 3] Process ID: 188609 [Epoch: 92,   128/ 591 points] total loss per batch: 1.289
Policy (actual, predicted): 0 0
Policy data: tensor([0.3448, 0.1150, 0.1150, 0.0548, 0.2934, 0.0273, 0.0498],
       device='cuda:0')
Policy pred: tensor([0.3651, 0.1110, 0.1137, 0.0578, 0.2883, 0.0225, 0.0417],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9970833659172058
 
[Iteration 3] Process ID: 188609 [Epoch: 92,   256/ 591 points] total loss per batch: 1.316
Policy (actual, predicted): 0 0
Policy data: tensor([0.2507, 0.1776, 0.1360, 0.1360, 0.0780, 0.0858, 0.1360],
       device='cuda:0')
Policy pred: tensor([0.2573, 0.1858, 0.1198, 0.1013, 0.0793, 0.0927, 0.1639],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9986976981163025
 
[Iteration 3] Process ID: 188609 [Epoch: 92,   384/ 591 points] total loss per batch: 1.180
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 0.0000e+00, 0.0000e+00, 1.1499e-11, 1.7638e-13, 1.1775e-08,
        4.1179e-07], device='cuda:0')
Policy pred: tensor([9.9630e-01, 1.8269e-07, 3.2231e-05, 1.5459e-03, 4.1141e-04, 4.8948e-04,
        1.2218e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999938607215881
 
[Iteration 3] Process ID: 188609 [Epoch: 92,   512/ 591 points] total loss per batch: 1.294
Policy (actual, predicted): 3 6
Policy data: tensor([0.1176, 0.1539, 0.0501, 0.2581, 0.1287, 0.0738, 0.2179],
       device='cuda:0')
Policy pred: tensor([0.1694, 0.1589, 0.0589, 0.1168, 0.1575, 0.1014, 0.2371],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999944269657135
 
[Iteration 3] Process ID: 188609 [Epoch: 93,   128/ 591 points] total loss per batch: 1.261
Policy (actual, predicted): 0 0
Policy data: tensor([0.3448, 0.1150, 0.1150, 0.0548, 0.2934, 0.0273, 0.0498],
       device='cuda:0')
Policy pred: tensor([0.3212, 0.1326, 0.1319, 0.0553, 0.2879, 0.0247, 0.0463],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.987387478351593
 
[Iteration 3] Process ID: 188609 [Epoch: 93,   256/ 591 points] total loss per batch: 1.296
Policy (actual, predicted): 2 2
Policy data: tensor([0.0217, 0.1514, 0.4414, 0.0057, 0.3471, 0.0156, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.0186, 0.1455, 0.4678, 0.0059, 0.3356, 0.0140, 0.0125],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.5864404439926147
 
[Iteration 3] Process ID: 188609 [Epoch: 93,   384/ 591 points] total loss per batch: 1.237
Policy (actual, predicted): 0 0
Policy data: tensor([0.8966, 0.0098, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.7717, 0.0325, 0.0313, 0.0300, 0.0547, 0.0341, 0.0457],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.0558183528482914
 
[Iteration 3] Process ID: 188609 [Epoch: 93,   512/ 591 points] total loss per batch: 1.249
Policy (actual, predicted): 4 0
Policy data: tensor([0.1348, 0.1442, 0.1430, 0.1442, 0.1489, 0.1395, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.8234, 0.0254, 0.0275, 0.0227, 0.0417, 0.0253, 0.0339],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.0496278777718544
 
[Iteration 3] Process ID: 188609 [Epoch: 94,   128/ 591 points] total loss per batch: 1.293
Policy (actual, predicted): 2 2
Policy data: tensor([5.5501e-20, 5.0910e-11, 1.0000e+00, 1.8023e-19, 1.3407e-18, 5.4200e-23,
        1.4601e-20], device='cuda:0')
Policy pred: tensor([3.5125e-04, 1.0606e-03, 9.9411e-01, 1.9372e-03, 1.5827e-03, 1.1921e-04,
        8.4217e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999736547470093
 
[Iteration 3] Process ID: 188609 [Epoch: 94,   256/ 591 points] total loss per batch: 1.333
Policy (actual, predicted): 3 3
Policy data: tensor([1.3693e-09, 8.0170e-06, 4.9153e-13, 9.9092e-01, 9.0683e-03, 2.2646e-17,
        1.2439e-06], device='cuda:0')
Policy pred: tensor([1.5953e-04, 2.6758e-03, 2.7951e-03, 9.6552e-01, 2.5824e-02, 3.0182e-04,
        2.7191e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999985694885254
 
[Iteration 3] Process ID: 188609 [Epoch: 94,   384/ 591 points] total loss per batch: 1.276
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0064, 0.0126, 0.0201, 0.0079, 0.0239, 0.0172, 0.9119],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.33183571696281433
 
[Iteration 3] Process ID: 188609 [Epoch: 94,   512/ 591 points] total loss per batch: 1.169
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000e+00, 0.0000e+00, 9.9718e-01, 1.1090e-08, 5.5423e-13, 2.8185e-03,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([1.9293e-08, 1.6574e-06, 9.9914e-01, 1.3273e-05, 1.1560e-06, 8.4767e-04,
        1.2452e-06], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999869465827942
 
[Iteration 3] Process ID: 188609 [Epoch: 95,   128/ 591 points] total loss per batch: 1.295
Policy (actual, predicted): 2 2
Policy data: tensor([0.0215, 0.0263, 0.8668, 0.0114, 0.0311, 0.0182, 0.0247],
       device='cuda:0')
Policy pred: tensor([0.0246, 0.0291, 0.8524, 0.0097, 0.0318, 0.0200, 0.0324],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.27462026476860046
 
[Iteration 3] Process ID: 188609 [Epoch: 95,   256/ 591 points] total loss per batch: 1.175
Policy (actual, predicted): 1 1
Policy data: tensor([0.1407, 0.1512, 0.1454, 0.1430, 0.1372, 0.1360, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1435, 0.1781, 0.1357, 0.1425, 0.1136, 0.1241, 0.1626],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999216794967651
 
[Iteration 3] Process ID: 188609 [Epoch: 95,   384/ 591 points] total loss per batch: 1.304
Policy (actual, predicted): 1 6
Policy data: tensor([0.1407, 0.1465, 0.1465, 0.1407, 0.1442, 0.1372, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1372, 0.1542, 0.1430, 0.1343, 0.1373, 0.1374, 0.1566],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999407529830933
 
[Iteration 3] Process ID: 188609 [Epoch: 95,   512/ 591 points] total loss per batch: 1.279
Policy (actual, predicted): 4 4
Policy data: tensor([4.3964e-19, 1.6074e-13, 2.6583e-21, 1.5329e-19, 1.0000e+00, 1.2717e-17,
        4.8339e-17], device='cuda:0')
Policy pred: tensor([2.3546e-05, 1.7900e-03, 1.1455e-04, 4.5584e-03, 9.9261e-01, 1.1643e-04,
        7.8944e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999974966049194
 
[Iteration 3] Process ID: 188609 [Epoch: 96,   128/ 591 points] total loss per batch: 1.262
Policy (actual, predicted): 4 6
Policy data: tensor([0.1419, 0.1465, 0.1419, 0.1383, 0.1489, 0.1372, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1124, 0.1135, 0.1313, 0.1098, 0.1238, 0.1169, 0.2922],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.44093549251556396
 
[Iteration 3] Process ID: 188609 [Epoch: 96,   256/ 591 points] total loss per batch: 1.290
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1264, 0.4079, 0.1429, 0.0680, 0.0852, 0.0876, 0.0820],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.006879347842186689
 
[Iteration 3] Process ID: 188609 [Epoch: 96,   384/ 591 points] total loss per batch: 1.410
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 1.3434e-10, 0.0000e+00, 1.4982e-09, 1.0000e+00, 1.8493e-18,
        1.1451e-07], device='cuda:0')
Policy pred: tensor([3.3081e-07, 3.1451e-05, 6.2638e-05, 2.4055e-03, 9.9725e-01, 1.2606e-05,
        2.3862e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999626278877258
 
[Iteration 3] Process ID: 188609 [Epoch: 96,   512/ 591 points] total loss per batch: 1.113
Policy (actual, predicted): 1 1
Policy data: tensor([0.1479, 0.7307, 0.0150, 0.0030, 0.0042, 0.0772, 0.0221],
       device='cuda:0')
Policy pred: tensor([0.1585, 0.6727, 0.0194, 0.0077, 0.0108, 0.0913, 0.0396],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9991633892059326
 
[Iteration 3] Process ID: 188609 [Epoch: 97,   128/ 591 points] total loss per batch: 1.209
Policy (actual, predicted): 0 0
Policy data: tensor([0.2507, 0.1776, 0.1360, 0.1360, 0.0780, 0.0858, 0.1360],
       device='cuda:0')
Policy pred: tensor([0.2603, 0.1587, 0.1653, 0.1320, 0.0706, 0.0846, 0.1286],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9992817640304565
 
[Iteration 3] Process ID: 188609 [Epoch: 97,   256/ 591 points] total loss per batch: 1.263
Policy (actual, predicted): 0 6
Policy data: tensor([0.3483, 0.0502, 0.0622, 0.1521, 0.0596, 0.0476, 0.2800],
       device='cuda:0')
Policy pred: tensor([0.3352, 0.0385, 0.0483, 0.1227, 0.0643, 0.0494, 0.3416],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9980782866477966
 
[Iteration 3] Process ID: 188609 [Epoch: 97,   384/ 591 points] total loss per batch: 1.337
Policy (actual, predicted): 6 6
Policy data: tensor([0.1583, 0.1887, 0.1448, 0.1210, 0.1210, 0.0422, 0.2242],
       device='cuda:0')
Policy pred: tensor([0.1488, 0.2102, 0.1310, 0.1346, 0.1061, 0.0392, 0.2300],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999951720237732
 
[Iteration 3] Process ID: 188609 [Epoch: 97,   512/ 591 points] total loss per batch: 1.297
Policy (actual, predicted): 4 2
Policy data: tensor([0.1477, 0.1477, 0.1477, 0.1265, 0.1524, 0.1348, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1513, 0.1375, 0.1551, 0.1252, 0.1502, 0.1373, 0.1435],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.00247579300776124
 
[Iteration 3] Process ID: 188609 [Epoch: 98,   128/ 591 points] total loss per batch: 1.236
Policy (actual, predicted): 2 2
Policy data: tensor([0.1465, 0.1372, 0.1489, 0.1348, 0.1454, 0.1477, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1393, 0.1457, 0.1655, 0.1282, 0.1358, 0.1448, 0.1406],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999846816062927
 
[Iteration 3] Process ID: 188609 [Epoch: 98,   256/ 591 points] total loss per batch: 1.263
Policy (actual, predicted): 2 2
Policy data: tensor([0.1917, 0.1435, 0.5590, 0.0247, 0.0232, 0.0407, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.2208, 0.1247, 0.5352, 0.0282, 0.0240, 0.0445, 0.0227],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9914716482162476
 
[Iteration 3] Process ID: 188609 [Epoch: 98,   384/ 591 points] total loss per batch: 1.216
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000e+00, 5.4318e-05, 9.9995e-01, 1.2911e-19, 4.9078e-19, 4.9078e-19,
        4.9078e-19], device='cuda:0')
Policy pred: tensor([2.3184e-07, 2.8117e-04, 9.9933e-01, 1.1011e-05, 3.9191e-05, 1.0454e-05,
        3.2992e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998961091041565
 
[Iteration 3] Process ID: 188609 [Epoch: 98,   512/ 591 points] total loss per batch: 1.388
Policy (actual, predicted): 2 2
Policy data: tensor([0.0081, 0.0135, 0.9478, 0.0063, 0.0063, 0.0118, 0.0063],
       device='cuda:0')
Policy pred: tensor([0.0182, 0.0218, 0.9299, 0.0047, 0.0091, 0.0113, 0.0051],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9986944198608398
 
[Iteration 3] Process ID: 188609 [Epoch: 99,   128/ 591 points] total loss per batch: 1.323
Policy (actual, predicted): 1 0
Policy data: tensor([0.1372, 0.1477, 0.1454, 0.1419, 0.1442, 0.1360, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.7285, 0.0362, 0.0403, 0.0342, 0.0710, 0.0386, 0.0512],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.029373984783887863
 
[Iteration 3] Process ID: 188609 [Epoch: 99,   256/ 591 points] total loss per batch: 1.229
Policy (actual, predicted): 6 6
Policy data: tensor([0.1583, 0.1887, 0.1448, 0.1210, 0.1210, 0.0422, 0.2242],
       device='cuda:0')
Policy pred: tensor([0.1521, 0.1879, 0.1238, 0.1277, 0.1131, 0.0332, 0.2621],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999991059303284
 
[Iteration 3] Process ID: 188609 [Epoch: 99,   384/ 591 points] total loss per batch: 1.310
Policy (actual, predicted): 2 2
Policy data: tensor([1.3690e-21, 6.3954e-21, 1.0000e+00, 3.1212e-18, 2.3740e-23, 1.3690e-21,
        2.4310e-20], device='cuda:0')
Policy pred: tensor([3.9180e-04, 2.0360e-04, 9.9915e-01, 4.5713e-05, 1.0131e-04, 4.0068e-05,
        6.9610e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998820424079895
 
[Iteration 3] Process ID: 188609 [Epoch: 99,   512/ 591 points] total loss per batch: 1.193
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 1.4355e-15, 1.8845e-19, 7.8128e-12, 1.5633e-17,
        5.8033e-20], device='cuda:0')
Policy pred: tensor([3.3612e-07, 9.9924e-01, 6.9166e-05, 2.3398e-06, 4.6941e-04, 2.8104e-06,
        2.1767e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999847412109375
 
[Iteration 3] Process ID: 188609 [Epoch: 100,   128/ 591 points] total loss per batch: 1.319
Policy (actual, predicted): 4 4
Policy data: tensor([0.1453, 0.0850, 0.0643, 0.0156, 0.4241, 0.0776, 0.1880],
       device='cuda:0')
Policy pred: tensor([0.1708, 0.0740, 0.0634, 0.0149, 0.3848, 0.0622, 0.2300],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999987483024597
 
[Iteration 3] Process ID: 188609 [Epoch: 100,   256/ 591 points] total loss per batch: 1.347
Policy (actual, predicted): 4 4
Policy data: tensor([0.0113, 0.0096, 0.0444, 0.0078, 0.8089, 0.0180, 0.1000],
       device='cuda:0')
Policy pred: tensor([0.0153, 0.0162, 0.0487, 0.0084, 0.8013, 0.0212, 0.0888],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9691333174705505
 
[Iteration 3] Process ID: 188609 [Epoch: 100,   384/ 591 points] total loss per batch: 1.121
Policy (actual, predicted): 3 3
Policy data: tensor([0.0665, 0.0665, 0.0879, 0.5117, 0.0963, 0.0452, 0.1260],
       device='cuda:0')
Policy pred: tensor([0.0751, 0.0826, 0.1124, 0.4039, 0.1197, 0.0586, 0.1477],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9996967315673828
 
[Iteration 3] Process ID: 188609 [Epoch: 100,   512/ 591 points] total loss per batch: 1.307
Policy (actual, predicted): 4 4
Policy data: tensor([7.5326e-19, 5.8547e-17, 2.8361e-22, 3.0452e-23, 1.0000e+00, 2.9041e-19,
        1.6747e-17], device='cuda:0')
Policy pred: tensor([7.6583e-04, 1.1107e-03, 8.8308e-05, 1.4528e-04, 9.9739e-01, 4.6686e-05,
        4.4889e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9980912804603577
 
[Iteration 3] Process ID: 188609 [Epoch: 101,   128/ 591 points] total loss per batch: 1.253
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9998e-01, 0.0000e+00, 2.0791e-05,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([1.4805e-05, 1.9010e-06, 3.1557e-05, 9.9876e-01, 1.7106e-05, 9.2339e-04,
        2.5609e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9986453652381897
 
[Iteration 3] Process ID: 188609 [Epoch: 101,   256/ 591 points] total loss per batch: 1.419
Policy (actual, predicted): 2 2
Policy data: tensor([0.0217, 0.1514, 0.4403, 0.0057, 0.3482, 0.0156, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.0159, 0.1345, 0.5314, 0.0054, 0.2861, 0.0127, 0.0139],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.597557783126831
 
[Iteration 3] Process ID: 188609 [Epoch: 101,   384/ 591 points] total loss per batch: 1.318
Policy (actual, predicted): 1 1
Policy data: tensor([0.4448, 0.4653, 0.0267, 0.0109, 0.0142, 0.0190, 0.0190],
       device='cuda:0')
Policy pred: tensor([0.3785, 0.5077, 0.0397, 0.0138, 0.0158, 0.0237, 0.0209],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9875015020370483
 
[Iteration 3] Process ID: 188609 [Epoch: 101,   512/ 591 points] total loss per batch: 1.170
Policy (actual, predicted): 6 6
Policy data: tensor([0.1145, 0.0262, 0.0247, 0.0217, 0.2126, 0.0364, 0.5639],
       device='cuda:0')
Policy pred: tensor([0.1008, 0.0355, 0.0375, 0.0231, 0.2207, 0.0305, 0.5519],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9558653235435486
 
[Iteration 3] Process ID: 188609 [Epoch: 102,   128/ 591 points] total loss per batch: 1.260
Policy (actual, predicted): 2 2
Policy data: tensor([0.2505, 0.0671, 0.2990, 0.2149, 0.0902, 0.0433, 0.0351],
       device='cuda:0')
Policy pred: tensor([0.2915, 0.0658, 0.3195, 0.1474, 0.0929, 0.0405, 0.0425],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9958890676498413
 
[Iteration 3] Process ID: 188609 [Epoch: 102,   256/ 591 points] total loss per batch: 1.292
Policy (actual, predicted): 4 4
Policy data: tensor([0.0715, 0.0958, 0.0503, 0.0517, 0.3876, 0.2703, 0.0728],
       device='cuda:0')
Policy pred: tensor([0.1122, 0.1324, 0.0867, 0.0959, 0.2721, 0.1945, 0.1061],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999985694885254
 
[Iteration 3] Process ID: 188609 [Epoch: 102,   384/ 591 points] total loss per batch: 1.298
Policy (actual, predicted): 1 1
Policy data: tensor([2.9877e-02, 7.9537e-01, 3.2440e-02, 3.2749e-03, 1.1032e-03, 4.6567e-07,
        1.3793e-01], device='cuda:0')
Policy pred: tensor([0.0177, 0.8424, 0.0246, 0.0063, 0.0041, 0.0013, 0.1035],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999971985816956
 
[Iteration 3] Process ID: 188609 [Epoch: 102,   512/ 591 points] total loss per batch: 1.222
Policy (actual, predicted): 2 1
Policy data: tensor([0.1489, 0.1477, 0.1524, 0.1325, 0.1489, 0.1301, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1184, 0.4186, 0.1400, 0.0709, 0.0790, 0.0834, 0.0897],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 1.9997358322143555e-05
 
[Iteration 3] Process ID: 188609 [Epoch: 103,   128/ 591 points] total loss per batch: 1.330
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1173, 0.4108, 0.1436, 0.0718, 0.0848, 0.0844, 0.0874],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.00028834491968154907
 
[Iteration 3] Process ID: 188609 [Epoch: 103,   256/ 591 points] total loss per batch: 1.253
Policy (actual, predicted): 3 1
Policy data: tensor([0.1348, 0.1430, 0.1454, 0.1489, 0.1442, 0.1348, 0.1489],
       device='cuda:0')
Policy pred: tensor([0.1324, 0.1586, 0.1543, 0.1492, 0.1386, 0.1306, 0.1363],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9997405409812927
 
[Iteration 3] Process ID: 188609 [Epoch: 103,   384/ 591 points] total loss per batch: 1.321
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 4.8059e-10, 1.6616e-09, 2.7480e-17, 1.0000e+00, 5.0091e-09,
        4.7592e-06], device='cuda:0')
Policy pred: tensor([7.2298e-09, 1.6449e-03, 4.0420e-05, 1.7200e-05, 9.9675e-01, 6.9166e-06,
        1.5452e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -1.0
 
[Iteration 3] Process ID: 188609 [Epoch: 103,   512/ 591 points] total loss per batch: 1.160
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 1.4355e-15, 1.8845e-19, 7.8128e-12, 1.5633e-17,
        5.8033e-20], device='cuda:0')
Policy pred: tensor([3.6429e-08, 9.9989e-01, 1.0418e-05, 4.5122e-07, 6.0761e-05, 2.5794e-07,
        3.6579e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999996542930603
 
[Iteration 3] Process ID: 188609 [Epoch: 104,   128/ 591 points] total loss per batch: 1.267
Policy (actual, predicted): 5 5
Policy data: tensor([0.0044, 0.0044, 0.0063, 0.0044, 0.0044, 0.9699, 0.0063],
       device='cuda:0')
Policy pred: tensor([0.0018, 0.0058, 0.0085, 0.0055, 0.0033, 0.9620, 0.0131],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9524878263473511
 
[Iteration 3] Process ID: 188609 [Epoch: 104,   256/ 591 points] total loss per batch: 1.296
Policy (actual, predicted): 1 1
Policy data: tensor([0.1501, 0.1512, 0.1466, 0.1277, 0.1454, 0.1372, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1478, 0.1506, 0.1477, 0.1334, 0.1340, 0.1457, 0.1406],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999969601631165
 
[Iteration 3] Process ID: 188609 [Epoch: 104,   384/ 591 points] total loss per batch: 1.275
Policy (actual, predicted): 2 1
Policy data: tensor([2.6643e-15, 4.3561e-01, 5.6439e-01, 6.7388e-17, 1.1145e-14, 1.1412e-11,
        5.5903e-15], device='cuda:0')
Policy pred: tensor([0.0005, 0.5195, 0.4756, 0.0006, 0.0019, 0.0010, 0.0009],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999807596206665
 
[Iteration 3] Process ID: 188609 [Epoch: 104,   512/ 591 points] total loss per batch: 1.152
Policy (actual, predicted): 1 1
Policy data: tensor([5.0161e-11, 1.0000e+00, 4.6716e-20, 9.7656e-24, 1.3555e-13, 7.1247e-11,
        2.7585e-15], device='cuda:0')
Policy pred: tensor([2.2381e-03, 9.9633e-01, 3.3720e-04, 1.1296e-04, 3.0091e-04, 1.1583e-04,
        5.6509e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9993850588798523
 
[Iteration 3] Process ID: 188609 [Epoch: 105,   128/ 591 points] total loss per batch: 1.239
Policy (actual, predicted): 0 0
Policy data: tensor([0.6244, 0.1329, 0.1020, 0.0218, 0.0187, 0.0480, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.6277, 0.1312, 0.1068, 0.0212, 0.0186, 0.0473, 0.0472],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.0008543876465409994
 
[Iteration 3] Process ID: 188609 [Epoch: 105,   256/ 591 points] total loss per batch: 1.326
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000e+00, 0.0000e+00, 6.1127e-09, 5.5685e-16, 3.7121e-08, 1.0000e+00,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([4.7424e-06, 1.0931e-06, 5.4826e-04, 3.9266e-05, 1.9808e-06, 9.9935e-01,
        5.8391e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999125599861145
 
[Iteration 3] Process ID: 188609 [Epoch: 105,   384/ 591 points] total loss per batch: 1.220
Policy (actual, predicted): 1 1
Policy data: tensor([0.0149, 0.3927, 0.1064, 0.1443, 0.0000, 0.0436, 0.2980],
       device='cuda:0')
Policy pred: tensor([1.4319e-02, 4.2151e-01, 1.0541e-01, 1.3945e-01, 1.2700e-05, 3.7483e-02,
        2.8182e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999967217445374
 
[Iteration 3] Process ID: 188609 [Epoch: 105,   512/ 591 points] total loss per batch: 1.250
Policy (actual, predicted): 3 3
Policy data: tensor([0.0665, 0.0665, 0.0879, 0.5117, 0.0963, 0.0452, 0.1260],
       device='cuda:0')
Policy pred: tensor([0.0624, 0.0611, 0.0848, 0.5252, 0.1003, 0.0441, 0.1220],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999523758888245
 
[Iteration 3] Process ID: 188609 [Epoch: 106,   128/ 591 points] total loss per batch: 1.277
Policy (actual, predicted): 0 0
Policy data: tensor([4.7743e-01, 0.0000e+00, 5.1976e-02, 1.9841e-05, 4.5169e-01, 5.4369e-05,
        1.8829e-02], device='cuda:0')
Policy pred: tensor([5.1722e-01, 4.3432e-05, 4.5500e-02, 1.1327e-03, 4.2157e-01, 1.1068e-03,
        1.3428e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999279260635376
 
[Iteration 3] Process ID: 188609 [Epoch: 106,   256/ 591 points] total loss per batch: 1.262
Policy (actual, predicted): 1 2
Policy data: tensor([0.1454, 0.1500, 0.1430, 0.1348, 0.1477, 0.1336, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1332, 0.1550, 0.1609, 0.1438, 0.1455, 0.1171, 0.1447],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998522400856018
 
[Iteration 3] Process ID: 188609 [Epoch: 106,   384/ 591 points] total loss per batch: 1.261
Policy (actual, predicted): 0 0
Policy data: tensor([0.6245, 0.1342, 0.1020, 0.0218, 0.0187, 0.0466, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.6501, 0.1257, 0.0992, 0.0185, 0.0166, 0.0439, 0.0461],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.006163444370031357
 
[Iteration 3] Process ID: 188609 [Epoch: 106,   512/ 591 points] total loss per batch: 1.221
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 3.2557e-17, 0.0000e+00, 1.0000e+00, 1.2720e-08, 3.2557e-17,
        1.7303e-09], device='cuda:0')
Policy pred: tensor([5.2372e-06, 5.0735e-04, 6.2417e-05, 9.9718e-01, 1.4110e-03, 1.4630e-04,
        6.8837e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9986535310745239
 
[Iteration 3] Process ID: 188609 [Epoch: 107,   128/ 591 points] total loss per batch: 1.310
Policy (actual, predicted): 1 1
Policy data: tensor([0.0650, 0.3209, 0.0485, 0.1777, 0.1139, 0.1247, 0.1491],
       device='cuda:0')
Policy pred: tensor([0.0583, 0.3323, 0.0499, 0.1719, 0.1150, 0.1170, 0.1556],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999718070030212
 
[Iteration 3] Process ID: 188609 [Epoch: 107,   256/ 591 points] total loss per batch: 1.363
Policy (actual, predicted): 1 1
Policy data: tensor([0.1419, 0.1465, 0.1419, 0.1430, 0.1454, 0.1360, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1425, 0.1554, 0.1351, 0.1435, 0.1437, 0.1336, 0.1462],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9997488260269165
 
[Iteration 3] Process ID: 188609 [Epoch: 107,   384/ 591 points] total loss per batch: 1.117
Policy (actual, predicted): 4 2
Policy data: tensor([0.1454, 0.1465, 0.1477, 0.1325, 0.1501, 0.1336, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1515, 0.1341, 0.1520, 0.1300, 0.1508, 0.1372, 0.1443],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.027205659076571465
 
[Iteration 3] Process ID: 188609 [Epoch: 107,   512/ 591 points] total loss per batch: 1.211
Policy (actual, predicted): 4 4
Policy data: tensor([0.1533, 0.1533, 0.0975, 0.0550, 0.2571, 0.0668, 0.2170],
       device='cuda:0')
Policy pred: tensor([0.1478, 0.1408, 0.1058, 0.0667, 0.2456, 0.0736, 0.2196],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999927878379822
 
[Iteration 3] Process ID: 188609 [Epoch: 108,   128/ 591 points] total loss per batch: 1.252
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 1.6843e-07, 0.0000e+00, 2.6493e-09, 1.0000e+00, 1.3457e-18,
        2.4653e-07], device='cuda:0')
Policy pred: tensor([3.7649e-07, 8.7745e-06, 1.5523e-05, 1.0903e-03, 9.9831e-01, 8.9030e-06,
        5.6308e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999812841415405
 
[Iteration 3] Process ID: 188609 [Epoch: 108,   256/ 591 points] total loss per batch: 1.181
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0069, 0.0120, 0.0220, 0.0093, 0.0238, 0.0209, 0.9050],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.35508599877357483
 
[Iteration 3] Process ID: 188609 [Epoch: 108,   384/ 591 points] total loss per batch: 1.308
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1191, 0.4583, 0.1320, 0.0622, 0.0710, 0.0778, 0.0797],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 0.0087322648614645
 
[Iteration 3] Process ID: 188609 [Epoch: 108,   512/ 591 points] total loss per batch: 1.230
Policy (actual, predicted): 2 2
Policy data: tensor([0.1977, 0.1435, 0.5532, 0.0247, 0.0232, 0.0407, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.2124, 0.1474, 0.5225, 0.0246, 0.0286, 0.0437, 0.0209],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9832372665405273
 
[Iteration 3] Process ID: 188609 [Epoch: 109,   128/ 591 points] total loss per batch: 1.221
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000e+00, 0.0000e+00, 9.4087e-02, 1.4492e-08, 7.1820e-06, 6.4460e-06,
        9.0590e-01], device='cuda:0')
Policy pred: tensor([2.3725e-06, 6.8871e-08, 8.5571e-02, 5.8115e-03, 3.8721e-04, 4.0082e-04,
        9.0783e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9995721578598022
 
[Iteration 3] Process ID: 188609 [Epoch: 109,   256/ 591 points] total loss per batch: 1.206
Policy (actual, predicted): 4 1
Policy data: tensor([0.1466, 0.1466, 0.1489, 0.1336, 0.1524, 0.1325, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1160, 0.4082, 0.1469, 0.0698, 0.0787, 0.0840, 0.0964],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.0034819776192307472
 
[Iteration 3] Process ID: 188609 [Epoch: 109,   384/ 591 points] total loss per batch: 1.240
Policy (actual, predicted): 2 2
Policy data: tensor([0.0263, 0.0165, 0.8683, 0.0132, 0.0311, 0.0247, 0.0199],
       device='cuda:0')
Policy pred: tensor([0.0199, 0.0171, 0.9055, 0.0083, 0.0192, 0.0149, 0.0152],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9851978421211243
 
[Iteration 3] Process ID: 188609 [Epoch: 109,   512/ 591 points] total loss per batch: 1.400
Policy (actual, predicted): 5 5
Policy data: tensor([0.0764, 0.0659, 0.1061, 0.0524, 0.0855, 0.5439, 0.0698],
       device='cuda:0')
Policy pred: tensor([0.0686, 0.0607, 0.1012, 0.0531, 0.0866, 0.5641, 0.0657],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9997280836105347
 
[Iteration 3] Process ID: 188609 [Epoch: 110,   128/ 591 points] total loss per batch: 1.270
Policy (actual, predicted): 3 3
Policy data: tensor([0.0738, 0.0904, 0.2061, 0.3947, 0.1288, 0.0712, 0.0351],
       device='cuda:0')
Policy pred: tensor([0.0744, 0.0731, 0.2059, 0.4343, 0.1067, 0.0659, 0.0396],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999268651008606
 
[Iteration 3] Process ID: 188609 [Epoch: 110,   256/ 591 points] total loss per batch: 1.150
Policy (actual, predicted): 3 3
Policy data: tensor([0.0729, 0.1156, 0.0729, 0.3226, 0.1056, 0.0368, 0.2738],
       device='cuda:0')
Policy pred: tensor([0.0669, 0.1028, 0.0722, 0.3656, 0.0981, 0.0382, 0.2562],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998407959938049
 
[Iteration 3] Process ID: 188609 [Epoch: 110,   384/ 591 points] total loss per batch: 1.314
Policy (actual, predicted): 4 4
Policy data: tensor([0.1043, 0.2590, 0.0589, 0.0541, 0.3447, 0.0000, 0.1791],
       device='cuda:0')
Policy pred: tensor([0.0900, 0.2715, 0.0535, 0.0602, 0.3630, 0.0006, 0.1611],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999934434890747
 
[Iteration 3] Process ID: 188609 [Epoch: 110,   512/ 591 points] total loss per batch: 1.325
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 8.6767e-12, 3.5443e-14, 3.6294e-21, 2.0929e-19, 3.5443e-24,
        3.7165e-18], device='cuda:0')
Policy pred: tensor([9.9961e-01, 2.1456e-04, 2.5987e-06, 2.8949e-06, 8.0355e-06, 3.3687e-06,
        1.6187e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999995231628418
 
[Iteration 3] Process ID: 188609 [Epoch: 111,   128/ 591 points] total loss per batch: 1.396
Policy (actual, predicted): 0 0
Policy data: tensor([0.6244, 0.1329, 0.1033, 0.0218, 0.0187, 0.0466, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.5897, 0.1305, 0.0999, 0.0301, 0.0268, 0.0594, 0.0637],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.004026428330689669
 
[Iteration 3] Process ID: 188609 [Epoch: 111,   256/ 591 points] total loss per batch: 1.186
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000e+00, 0.0000e+00, 6.1127e-09, 5.5685e-16, 3.7121e-08, 1.0000e+00,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([1.6793e-05, 6.6431e-06, 3.3568e-03, 1.2091e-04, 2.3386e-05, 9.9603e-01,
        4.4230e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999443888664246
 
[Iteration 3] Process ID: 188609 [Epoch: 111,   384/ 591 points] total loss per batch: 1.264
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9961e-01, 0.0000e+00, 3.8781e-04,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([2.0649e-06, 1.8446e-06, 1.7805e-05, 9.9937e-01, 5.8206e-05, 4.1919e-04,
        1.3157e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9989855289459229
 
[Iteration 3] Process ID: 188609 [Epoch: 111,   512/ 591 points] total loss per batch: 1.274
Policy (actual, predicted): 1 1
Policy data: tensor([0.0998, 0.6876, 0.0159, 0.0000, 0.0000, 0.1552, 0.0416],
       device='cuda:0')
Policy pred: tensor([1.2743e-01, 6.2594e-01, 1.5115e-02, 1.0963e-04, 3.2920e-05, 1.7435e-01,
        5.7021e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998075366020203
 
[Iteration 3] Process ID: 188609 [Epoch: 112,   128/ 591 points] total loss per batch: 1.289
Policy (actual, predicted): 0 4
Policy data: tensor([0.2726, 0.1151, 0.1639, 0.0448, 0.2509, 0.0269, 0.1258],
       device='cuda:0')
Policy pred: tensor([0.2674, 0.1039, 0.1777, 0.0383, 0.2785, 0.0277, 0.1066],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999988853931427
 
[Iteration 3] Process ID: 188609 [Epoch: 112,   256/ 591 points] total loss per batch: 1.234
Policy (actual, predicted): 1 1
Policy data: tensor([0.0246, 0.8483, 0.0131, 0.0114, 0.0551, 0.0214, 0.0262],
       device='cuda:0')
Policy pred: tensor([0.0269, 0.8161, 0.0169, 0.0210, 0.0651, 0.0247, 0.0293],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.99870365858078
 
[Iteration 3] Process ID: 188609 [Epoch: 112,   384/ 591 points] total loss per batch: 1.148
Policy (actual, predicted): 1 1
Policy data: tensor([1.2350e-18, 1.0000e+00, 3.4590e-21, 3.7141e-22, 2.0171e-07, 6.4565e-10,
        3.8945e-16], device='cuda:0')
Policy pred: tensor([2.6121e-04, 9.9917e-01, 4.8191e-05, 1.2164e-04, 3.4648e-05, 2.9988e-05,
        3.3487e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9996521472930908
 
[Iteration 3] Process ID: 188609 [Epoch: 112,   512/ 591 points] total loss per batch: 1.296
Policy (actual, predicted): 2 2
Policy data: tensor([0.0146, 0.0715, 0.7924, 0.0112, 0.0162, 0.0730, 0.0211],
       device='cuda:0')
Policy pred: tensor([0.0112, 0.0656, 0.8149, 0.0094, 0.0195, 0.0615, 0.0181],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9987779855728149
 
[Iteration 3] Process ID: 188609 [Epoch: 113,   128/ 591 points] total loss per batch: 1.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0313, 0.1597, 0.0387, 0.0475, 0.0143, 0.6910, 0.0175],
       device='cuda:0')
Policy pred: tensor([0.0232, 0.1258, 0.0445, 0.0503, 0.0111, 0.7238, 0.0212],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998644590377808
 
[Iteration 3] Process ID: 188609 [Epoch: 113,   256/ 591 points] total loss per batch: 1.249
Policy (actual, predicted): 1 0
Policy data: tensor([0.2195, 0.2730, 0.0182, 0.1893, 0.0000, 0.1107, 0.1893],
       device='cuda:0')
Policy pred: tensor([3.0437e-01, 2.2846e-01, 1.3238e-02, 1.6740e-01, 6.1099e-07, 9.5760e-02,
        1.9077e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999983906745911
 
[Iteration 3] Process ID: 188609 [Epoch: 113,   384/ 591 points] total loss per batch: 1.189
Policy (actual, predicted): 4 4
Policy data: tensor([0.0183, 0.0281, 0.0150, 0.0080, 0.8907, 0.0150, 0.0249],
       device='cuda:0')
Policy pred: tensor([0.0182, 0.0288, 0.0230, 0.0113, 0.8765, 0.0147, 0.0276],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9933039546012878
 
[Iteration 3] Process ID: 188609 [Epoch: 113,   512/ 591 points] total loss per batch: 1.350
Policy (actual, predicted): 1 5
Policy data: tensor([0.1395, 0.1489, 0.1419, 0.1465, 0.1407, 0.1395, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1167, 0.1344, 0.1273, 0.1590, 0.1185, 0.1755, 0.1687],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999839663505554
 
[Iteration 3] Process ID: 188609 [Epoch: 114,   128/ 591 points] total loss per batch: 1.349
Policy (actual, predicted): 6 6
Policy data: tensor([0.1579, 0.1354, 0.2204, 0.0586, 0.0990, 0.0599, 0.2688],
       device='cuda:0')
Policy pred: tensor([0.1895, 0.1236, 0.2186, 0.0597, 0.1065, 0.0521, 0.2501],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9872726798057556
 
[Iteration 3] Process ID: 188609 [Epoch: 114,   256/ 591 points] total loss per batch: 1.140
Policy (actual, predicted): 5 5
Policy data: tensor([0.0044, 0.0044, 0.0044, 0.0118, 0.0063, 0.9644, 0.0044],
       device='cuda:0')
Policy pred: tensor([0.0054, 0.0036, 0.0055, 0.0104, 0.0041, 0.9668, 0.0043],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999836087226868
 
[Iteration 3] Process ID: 188609 [Epoch: 114,   384/ 591 points] total loss per batch: 1.219
Policy (actual, predicted): 0 0
Policy data: tensor([0.4056, 0.0692, 0.3775, 0.0274, 0.0154, 0.0745, 0.0304],
       device='cuda:0')
Policy pred: tensor([0.4367, 0.0626, 0.3708, 0.0163, 0.0136, 0.0677, 0.0322],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.23662586510181427
 
[Iteration 3] Process ID: 188609 [Epoch: 114,   512/ 591 points] total loss per batch: 1.345
Policy (actual, predicted): 1 1
Policy data: tensor([0.1430, 0.1500, 0.1348, 0.1442, 0.1419, 0.1395, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1519, 0.1557, 0.1150, 0.1428, 0.1424, 0.1389, 0.1532],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999966025352478
 
[Iteration 3] Process ID: 188609 [Epoch: 115,   128/ 591 points] total loss per batch: 1.325
Policy (actual, predicted): 0 0
Policy data: tensor([0.4028, 0.3378, 0.0385, 0.0000, 0.0000, 0.0592, 0.1617],
       device='cuda:0')
Policy pred: tensor([4.4061e-01, 3.4297e-01, 3.0327e-02, 1.1306e-04, 1.1155e-05, 5.5320e-02,
        1.3065e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999986886978149
 
[Iteration 3] Process ID: 188609 [Epoch: 115,   256/ 591 points] total loss per batch: 1.362
Policy (actual, predicted): 4 0
Policy data: tensor([0.1348, 0.1442, 0.1430, 0.1442, 0.1489, 0.1395, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.7358, 0.0414, 0.0424, 0.0318, 0.0591, 0.0384, 0.0511],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.07235949486494064
 
[Iteration 3] Process ID: 188609 [Epoch: 115,   384/ 591 points] total loss per batch: 1.225
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0057, 0.0093, 0.0180, 0.0068, 0.0236, 0.0170, 0.9196],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.38390597701072693
 
[Iteration 3] Process ID: 188609 [Epoch: 115,   512/ 591 points] total loss per batch: 1.158
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 8.2964e-01, 2.5501e-13, 1.8525e-15, 1.0572e-05, 4.6371e-09,
        1.7035e-01], device='cuda:0')
Policy pred: tensor([1.0332e-05, 7.8429e-01, 6.6272e-04, 6.4229e-04, 2.6324e-03, 1.0094e-03,
        2.1075e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999094605445862
 
[Iteration 3] Process ID: 188609 [Epoch: 116,   128/ 591 points] total loss per batch: 1.296
Policy (actual, predicted): 2 2
Policy data: tensor([1.6581e-13, 2.0790e-14, 1.0000e+00, 3.0322e-14, 4.0053e-12, 2.9689e-16,
        6.1009e-15], device='cuda:0')
Policy pred: tensor([5.1430e-06, 3.7479e-05, 9.9988e-01, 2.4053e-06, 4.2821e-05, 2.0460e-05,
        1.4735e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999595284461975
 
[Iteration 3] Process ID: 188609 [Epoch: 116,   256/ 591 points] total loss per batch: 1.249
Policy (actual, predicted): 0 4
Policy data: tensor([0.1512, 0.1454, 0.1466, 0.1360, 0.1512, 0.1277, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1487, 0.1398, 0.1364, 0.1392, 0.1691, 0.1258, 0.1410],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999998152256012
 
[Iteration 3] Process ID: 188609 [Epoch: 116,   384/ 591 points] total loss per batch: 1.234
Policy (actual, predicted): 4 4
Policy data: tensor([0.0245, 0.0652, 0.0245, 0.0096, 0.8400, 0.0213, 0.0148],
       device='cuda:0')
Policy pred: tensor([0.0225, 0.0595, 0.0225, 0.0096, 0.8515, 0.0206, 0.0138],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9997473359107971
 
[Iteration 3] Process ID: 188609 [Epoch: 116,   512/ 591 points] total loss per batch: 1.336
Policy (actual, predicted): 1 5
Policy data: tensor([0.1395, 0.1489, 0.1419, 0.1465, 0.1407, 0.1395, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1471, 0.1268, 0.1354, 0.1343, 0.1512, 0.1639, 0.1413],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999539256095886
 
[Iteration 3] Process ID: 188609 [Epoch: 117,   128/ 591 points] total loss per batch: 1.261
Policy (actual, predicted): 4 4
Policy data: tensor([0.1578, 0.1214, 0.1578, 0.0264, 0.2835, 0.0125, 0.2406],
       device='cuda:0')
Policy pred: tensor([0.1739, 0.1181, 0.1730, 0.0282, 0.2531, 0.0150, 0.2387],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999867081642151
 
[Iteration 3] Process ID: 188609 [Epoch: 117,   256/ 591 points] total loss per batch: 1.327
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000, 0.1285, 0.2486, 0.0208, 0.4932, 0.0456, 0.0634],
       device='cuda:0')
Policy pred: tensor([1.1660e-05, 1.4517e-01, 2.4243e-01, 2.2801e-02, 4.7340e-01, 4.4346e-02,
        7.1844e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999995827674866
 
[Iteration 3] Process ID: 188609 [Epoch: 117,   384/ 591 points] total loss per batch: 1.209
Policy (actual, predicted): 5 5
Policy data: tensor([5.1689e-19, 5.5501e-20, 1.4951e-17, 2.9807e-17, 5.1689e-19, 1.0000e+00,
        1.7601e-12], device='cuda:0')
Policy pred: tensor([8.0760e-04, 7.7299e-04, 6.3534e-04, 8.1085e-04, 7.7721e-04, 9.9592e-01,
        2.8011e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9996852874755859
 
[Iteration 3] Process ID: 188609 [Epoch: 117,   512/ 591 points] total loss per batch: 1.231
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 8.6020e-15, 3.1931e-17, 1.7560e-21, 1.0000e+00, 8.2035e-21,
        2.8361e-22], device='cuda:0')
Policy pred: tensor([8.7111e-10, 8.8622e-04, 3.1839e-05, 9.5196e-06, 9.9899e-01, 4.0374e-06,
        8.3329e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999997019767761
 
[Iteration 3] Process ID: 188609 [Epoch: 118,   128/ 591 points] total loss per batch: 1.245
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 2.3553e-09, 7.0268e-20, 2.6710e-19, 1.4689e-13,
        1.5992e-09], device='cuda:0')
Policy pred: tensor([2.0373e-08, 9.9901e-01, 6.3417e-04, 4.2511e-06, 4.7979e-05, 7.1604e-06,
        2.9334e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9992706179618835
 
[Iteration 3] Process ID: 188609 [Epoch: 118,   256/ 591 points] total loss per batch: 1.265
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1219, 0.4169, 0.1397, 0.0686, 0.0759, 0.0886, 0.0883],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.007282344624400139
 
[Iteration 3] Process ID: 188609 [Epoch: 118,   384/ 591 points] total loss per batch: 1.295
Policy (actual, predicted): 4 4
Policy data: tensor([7.5326e-19, 5.8547e-17, 2.8361e-22, 3.0452e-23, 1.0000e+00, 2.9041e-19,
        1.6747e-17], device='cuda:0')
Policy pred: tensor([6.0678e-04, 1.1350e-03, 1.6701e-04, 2.1242e-04, 9.9751e-01, 1.0931e-04,
        2.5649e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9989433288574219
 
[Iteration 3] Process ID: 188609 [Epoch: 118,   512/ 591 points] total loss per batch: 1.263
Policy (actual, predicted): 2 2
Policy data: tensor([0.0184, 0.0217, 0.8952, 0.0080, 0.0200, 0.0167, 0.0200],
       device='cuda:0')
Policy pred: tensor([0.0214, 0.0278, 0.8580, 0.0141, 0.0281, 0.0186, 0.0320],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9924419522285461
 
[Iteration 3] Process ID: 188609 [Epoch: 119,   128/ 591 points] total loss per batch: 1.111
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000, 0.2224, 0.2090, 0.0355, 0.4307, 0.1025, 0.0000],
       device='cuda:0')
Policy pred: tensor([1.5261e-04, 2.4258e-01, 1.7301e-01, 4.2869e-02, 4.1953e-01, 1.2116e-01,
        6.9853e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999996423721313
 
[Iteration 3] Process ID: 188609 [Epoch: 119,   256/ 591 points] total loss per batch: 1.326
Policy (actual, predicted): 4 4
Policy data: tensor([0.2044, 0.1439, 0.1573, 0.0419, 0.2227, 0.1097, 0.1202],
       device='cuda:0')
Policy pred: tensor([0.2098, 0.1470, 0.1697, 0.0335, 0.2112, 0.1174, 0.1113],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999940395355225
 
[Iteration 3] Process ID: 188609 [Epoch: 119,   384/ 591 points] total loss per batch: 1.262
Policy (actual, predicted): 4 0
Policy data: tensor([0.1477, 0.1489, 0.1442, 0.1301, 0.1570, 0.1313, 0.1407],
       device='cuda:0')
Policy pred: tensor([0.1725, 0.1433, 0.1594, 0.1359, 0.1461, 0.1278, 0.1151],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.027443943545222282
 
[Iteration 3] Process ID: 188609 [Epoch: 119,   512/ 591 points] total loss per batch: 1.359
Policy (actual, predicted): 0 0
Policy data: tensor([0.8949, 0.0116, 0.0133, 0.0062, 0.0392, 0.0116, 0.0233],
       device='cuda:0')
Policy pred: tensor([0.6487, 0.0527, 0.0516, 0.0473, 0.0828, 0.0502, 0.0667],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.06542040407657623
 
[Iteration 3] Process ID: 188609 [Epoch: 120,   128/ 591 points] total loss per batch: 1.279
Policy (actual, predicted): 4 4
Policy data: tensor([0.1477, 0.1477, 0.1477, 0.1265, 0.1524, 0.1348, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1542, 0.1379, 0.1460, 0.1230, 0.1570, 0.1379, 0.1439],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.0017952079651877284
 
[Iteration 3] Process ID: 188609 [Epoch: 120,   256/ 591 points] total loss per batch: 1.291
Policy (actual, predicted): 0 0
Policy data: tensor([4.7694e-01, 4.4778e-01, 4.6184e-03, 4.5488e-05, 6.8740e-02, 2.6008e-04,
        1.6104e-03], device='cuda:0')
Policy pred: tensor([0.5111, 0.4189, 0.0064, 0.0028, 0.0510, 0.0041, 0.0057],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9979317784309387
 
[Iteration 3] Process ID: 188609 [Epoch: 120,   384/ 591 points] total loss per batch: 1.216
Policy (actual, predicted): 0 0
Policy data: tensor([0.5066, 0.0453, 0.1017, 0.0668, 0.0548, 0.1130, 0.1118],
       device='cuda:0')
Policy pred: tensor([0.4785, 0.0473, 0.1014, 0.0775, 0.0630, 0.1199, 0.1125],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999772906303406
 
[Iteration 3] Process ID: 188609 [Epoch: 120,   512/ 591 points] total loss per batch: 1.250
Policy (actual, predicted): 1 1
Policy data: tensor([0.1140, 0.2503, 0.1140, 0.1492, 0.0864, 0.0359, 0.2503],
       device='cuda:0')
Policy pred: tensor([0.0923, 0.2931, 0.1352, 0.1391, 0.0941, 0.0439, 0.2023],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999966025352478
 
[Iteration 3] Process ID: 188609 [Epoch: 121,   128/ 591 points] total loss per batch: 1.263
Policy (actual, predicted): 2 2
Policy data: tensor([0.1489, 0.1430, 0.1512, 0.1336, 0.1501, 0.1336, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1474, 0.1376, 0.1655, 0.1313, 0.1537, 0.1313, 0.1333],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998268485069275
 
[Iteration 3] Process ID: 188609 [Epoch: 121,   256/ 591 points] total loss per batch: 1.304
Policy (actual, predicted): 1 1
Policy data: tensor([2.0347e-20, 1.0000e+00, 0.0000e+00, 1.3130e-10, 4.4600e-18, 2.0347e-20,
        5.9039e-14], device='cuda:0')
Policy pred: tensor([2.0107e-04, 9.9872e-01, 6.4156e-04, 2.3000e-04, 1.6571e-04, 1.7469e-05,
        2.0547e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9997467994689941
 
[Iteration 3] Process ID: 188609 [Epoch: 121,   384/ 591 points] total loss per batch: 1.293
Policy (actual, predicted): 0 2
Policy data: tensor([0.4737, 0.0441, 0.4088, 0.0092, 0.0125, 0.0250, 0.0265],
       device='cuda:0')
Policy pred: tensor([0.3734, 0.0487, 0.4994, 0.0088, 0.0157, 0.0248, 0.0291],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9990597367286682
 
[Iteration 3] Process ID: 188609 [Epoch: 121,   512/ 591 points] total loss per batch: 1.245
Policy (actual, predicted): 2 2
Policy data: tensor([5.5501e-20, 5.0910e-11, 1.0000e+00, 1.8023e-19, 1.3407e-18, 5.4200e-23,
        1.4601e-20], device='cuda:0')
Policy pred: tensor([2.8818e-04, 5.8266e-04, 9.9691e-01, 8.3077e-04, 8.5237e-04, 1.0949e-04,
        4.3106e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999889731407166
 
[Iteration 3] Process ID: 188609 [Epoch: 122,   128/ 591 points] total loss per batch: 1.350
Policy (actual, predicted): 4 6
Policy data: tensor([0.1501, 0.1466, 0.1442, 0.1254, 0.1512, 0.1348, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1450, 0.1276, 0.1555, 0.1263, 0.1544, 0.1356, 0.1556],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999973773956299
 
[Iteration 3] Process ID: 188609 [Epoch: 122,   256/ 591 points] total loss per batch: 1.324
Policy (actual, predicted): 0 0
Policy data: tensor([0.7043, 0.0000, 0.0655, 0.0000, 0.0000, 0.0414, 0.1888],
       device='cuda:0')
Policy pred: tensor([6.9275e-01, 1.6718e-05, 5.6665e-02, 1.0066e-04, 1.8021e-05, 4.4076e-02,
        2.0637e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999979138374329
 
[Iteration 3] Process ID: 188609 [Epoch: 122,   384/ 591 points] total loss per batch: 1.295
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 1.3434e-10, 0.0000e+00, 1.4982e-09, 1.0000e+00, 1.8493e-18,
        1.1451e-07], device='cuda:0')
Policy pred: tensor([1.5221e-06, 7.9057e-05, 1.4847e-04, 4.0080e-03, 9.9491e-01, 9.1375e-05,
        7.6021e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998731017112732
 
[Iteration 3] Process ID: 188609 [Epoch: 122,   512/ 591 points] total loss per batch: 1.131
Policy (actual, predicted): 0 0
Policy data: tensor([0.6244, 0.1329, 0.1033, 0.0218, 0.0187, 0.0466, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.6246, 0.1367, 0.1055, 0.0197, 0.0170, 0.0462, 0.0504],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.006084758322685957
 
[Iteration 3] Process ID: 188609 [Epoch: 123,   128/ 591 points] total loss per batch: 1.236
Policy (actual, predicted): 2 2
Policy data: tensor([0.1917, 0.1435, 0.5590, 0.0247, 0.0232, 0.0407, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.1915, 0.1500, 0.5393, 0.0265, 0.0291, 0.0414, 0.0220],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9935607314109802
 
[Iteration 3] Process ID: 188609 [Epoch: 123,   256/ 591 points] total loss per batch: 1.249
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 8.2964e-01, 2.5501e-13, 1.8525e-15, 1.0572e-05, 4.6371e-09,
        1.7035e-01], device='cuda:0')
Policy pred: tensor([1.0503e-04, 8.7880e-01, 4.0611e-03, 2.0610e-03, 7.3750e-03, 2.8410e-03,
        1.0476e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9995129108428955
 
[Iteration 3] Process ID: 188609 [Epoch: 123,   384/ 591 points] total loss per batch: 1.288
Policy (actual, predicted): 4 4
Policy data: tensor([0.1017, 0.1906, 0.0697, 0.0768, 0.2265, 0.1600, 0.1747],
       device='cuda:0')
Policy pred: tensor([0.0916, 0.1812, 0.0791, 0.0814, 0.2262, 0.1333, 0.2072],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999965250492096
 
[Iteration 3] Process ID: 188609 [Epoch: 123,   512/ 591 points] total loss per batch: 1.230
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 4.6844e-06, 0.0000e+00, 1.8136e-02,
        9.8186e-01], device='cuda:0')
Policy pred: tensor([6.1967e-06, 1.5373e-06, 2.4654e-06, 1.2268e-04, 9.3189e-10, 1.3284e-02,
        9.8658e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999996423721313
 
[Iteration 3] Process ID: 188609 [Epoch: 124,   128/ 591 points] total loss per batch: 1.283
Policy (actual, predicted): 0 0
Policy data: tensor([0.8765, 0.0149, 0.0079, 0.0097, 0.0199, 0.0526, 0.0183],
       device='cuda:0')
Policy pred: tensor([0.8666, 0.0143, 0.0105, 0.0121, 0.0238, 0.0512, 0.0215],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9935815334320068
 
[Iteration 3] Process ID: 188609 [Epoch: 124,   256/ 591 points] total loss per batch: 1.180
Policy (actual, predicted): 4 4
Policy data: tensor([0.1501, 0.1477, 0.1466, 0.1265, 0.1524, 0.1372, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1530, 0.1431, 0.1314, 0.1348, 0.1579, 0.1351, 0.1447],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999926686286926
 
[Iteration 3] Process ID: 188609 [Epoch: 124,   384/ 591 points] total loss per batch: 1.250
Policy (actual, predicted): 4 4
Policy data: tensor([0.0734, 0.2334, 0.0410, 0.0551, 0.3249, 0.1063, 0.1658],
       device='cuda:0')
Policy pred: tensor([0.0756, 0.2264, 0.0377, 0.0523, 0.3454, 0.0986, 0.1639],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999869465827942
 
[Iteration 3] Process ID: 188609 [Epoch: 124,   512/ 591 points] total loss per batch: 1.338
Policy (actual, predicted): 4 4
Policy data: tensor([0.0183, 0.0297, 0.0133, 0.0150, 0.8920, 0.0167, 0.0150],
       device='cuda:0')
Policy pred: tensor([0.0175, 0.0289, 0.0081, 0.0105, 0.9029, 0.0168, 0.0154],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9984832406044006
 
[Iteration 3] Process ID: 188609 [Epoch: 125,   128/ 591 points] total loss per batch: 1.262
Policy (actual, predicted): 2 2
Policy data: tensor([0.1156, 0.0361, 0.7432, 0.0128, 0.0160, 0.0240, 0.0523],
       device='cuda:0')
Policy pred: tensor([0.1305, 0.0401, 0.7245, 0.0120, 0.0151, 0.0265, 0.0513],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9953718781471252
 
[Iteration 3] Process ID: 188609 [Epoch: 125,   256/ 591 points] total loss per batch: 1.246
Policy (actual, predicted): 3 3
Policy data: tensor([0.1430, 0.1465, 0.1372, 0.1500, 0.1407, 0.1348, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1474, 0.1534, 0.1422, 0.1552, 0.1375, 0.1313, 0.1329],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9942572712898254
 
[Iteration 3] Process ID: 188609 [Epoch: 125,   384/ 591 points] total loss per batch: 1.290
Policy (actual, predicted): 1 1
Policy data: tensor([0.1501, 0.1536, 0.1419, 0.1301, 0.1501, 0.1325, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1497, 0.1526, 0.1441, 0.1263, 0.1509, 0.1371, 0.1392],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999997615814209
 
[Iteration 3] Process ID: 188609 [Epoch: 125,   512/ 591 points] total loss per batch: 1.265
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 8.6020e-15, 3.1931e-17, 1.7560e-21, 1.0000e+00, 8.2035e-21,
        2.8361e-22], device='cuda:0')
Policy pred: tensor([1.0171e-09, 1.0424e-03, 7.0767e-05, 1.0746e-05, 9.9881e-01, 8.0630e-06,
        6.2834e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999999403953552
 
[Iteration 3] Process ID: 188609 [Epoch: 126,   128/ 591 points] total loss per batch: 1.266
Policy (actual, predicted): 6 6
Policy data: tensor([0.0062, 0.0133, 0.0218, 0.0080, 0.0250, 0.0184, 0.9072],
       device='cuda:0')
Policy pred: tensor([0.0065, 0.0112, 0.0209, 0.0076, 0.0244, 0.0147, 0.9147],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.29292261600494385
 
[Iteration 3] Process ID: 188609 [Epoch: 126,   256/ 591 points] total loss per batch: 1.223
Policy (actual, predicted): 5 5
Policy data: tensor([0.0764, 0.0659, 0.1061, 0.0524, 0.0855, 0.5439, 0.0698],
       device='cuda:0')
Policy pred: tensor([0.0637, 0.0726, 0.0984, 0.0523, 0.0908, 0.5501, 0.0722],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9992766976356506
 
[Iteration 3] Process ID: 188609 [Epoch: 126,   384/ 591 points] total loss per batch: 1.237
Policy (actual, predicted): 4 4
Policy data: tensor([0.1331, 0.0605, 0.0905, 0.0148, 0.4779, 0.0000, 0.2231],
       device='cuda:0')
Policy pred: tensor([1.2830e-01, 6.1090e-02, 1.0066e-01, 1.1881e-02, 4.6712e-01, 1.2444e-04,
        2.3083e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999972581863403
 
[Iteration 3] Process ID: 188609 [Epoch: 126,   512/ 591 points] total loss per batch: 1.298
Policy (actual, predicted): 3 3
Policy data: tensor([0.0955, 0.1501, 0.0539, 0.2739, 0.0955, 0.0792, 0.2519],
       device='cuda:0')
Policy pred: tensor([0.0938, 0.1428, 0.0547, 0.3062, 0.0847, 0.0795, 0.2384],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9996964335441589
 
[Iteration 3] Process ID: 188609 [Epoch: 127,   128/ 591 points] total loss per batch: 1.282
Policy (actual, predicted): 4 4
Policy data: tensor([0.0755, 0.0567, 0.1094, 0.2027, 0.3624, 0.0624, 0.1310],
       device='cuda:0')
Policy pred: tensor([0.0800, 0.0653, 0.1101, 0.1818, 0.3769, 0.0613, 0.1246],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999859929084778
 
[Iteration 3] Process ID: 188609 [Epoch: 127,   256/ 591 points] total loss per batch: 1.263
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1247, 0.3961, 0.1493, 0.0727, 0.0838, 0.0834, 0.0899],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.0044036670587956905
 
[Iteration 3] Process ID: 188609 [Epoch: 127,   384/ 591 points] total loss per batch: 1.282
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 2.5064e-16, 0.0000e+00, 5.6356e-18, 4.0880e-19, 3.8986e-15,
        2.5064e-16], device='cuda:0')
Policy pred: tensor([9.9821e-01, 9.6299e-04, 1.8559e-05, 7.5029e-04, 3.5546e-05, 4.0208e-06,
        1.8653e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9990224242210388
 
[Iteration 3] Process ID: 188609 [Epoch: 127,   512/ 591 points] total loss per batch: 1.305
Policy (actual, predicted): 2 2
Policy data: tensor([0.0374, 0.0476, 0.7277, 0.0159, 0.0175, 0.0576, 0.0962],
       device='cuda:0')
Policy pred: tensor([0.1039, 0.1060, 0.3587, 0.0878, 0.0972, 0.1168, 0.1297],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9995771646499634
 
[Iteration 3] Process ID: 188609 [Epoch: 128,   128/ 591 points] total loss per batch: 1.233
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000e+00, 5.4318e-05, 9.9995e-01, 1.2911e-19, 4.9078e-19, 4.9078e-19,
        4.9078e-19], device='cuda:0')
Policy pred: tensor([1.8532e-08, 8.0434e-05, 9.9982e-01, 2.6044e-06, 1.2167e-05, 2.1675e-06,
        8.1994e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999822974205017
 
[Iteration 3] Process ID: 188609 [Epoch: 128,   256/ 591 points] total loss per batch: 1.257
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 1.0000e+00, 0.0000e+00, 7.5759e-14, 0.0000e+00, 2.9489e-20,
        1.5837e-17], device='cuda:0')
Policy pred: tensor([1.9256e-05, 9.9965e-01, 3.4876e-05, 6.4989e-06, 3.8612e-07, 8.6584e-06,
        2.8386e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999984502792358
 
[Iteration 3] Process ID: 188609 [Epoch: 128,   384/ 591 points] total loss per batch: 1.276
Policy (actual, predicted): 4 0
Policy data: tensor([0.1547, 0.1360, 0.1454, 0.1289, 0.1594, 0.1337, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1592, 0.1285, 0.1319, 0.1249, 0.1584, 0.1535, 0.1437],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999146461486816
 
[Iteration 3] Process ID: 188609 [Epoch: 128,   512/ 591 points] total loss per batch: 1.251
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000, 0.0000, 0.2546, 0.1031, 0.3662, 0.0512, 0.2248],
       device='cuda:0')
Policy pred: tensor([4.7505e-05, 4.1235e-05, 2.5967e-01, 9.1409e-02, 3.7259e-01, 4.9553e-02,
        2.2670e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999798536300659
 
[Iteration 3] Process ID: 188609 [Epoch: 129,   128/ 591 points] total loss per batch: 1.197
Policy (actual, predicted): 1 6
Policy data: tensor([0.1580, 0.2054, 0.0568, 0.0758, 0.1884, 0.1102, 0.2054],
       device='cuda:0')
Policy pred: tensor([0.1528, 0.1889, 0.0631, 0.0813, 0.1877, 0.1048, 0.2215],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999384880065918
 
[Iteration 3] Process ID: 188609 [Epoch: 129,   256/ 591 points] total loss per batch: 1.274
Policy (actual, predicted): 1 1
Policy data: tensor([0.0330, 0.6655, 0.0208, 0.0094, 0.0192, 0.2410, 0.0111],
       device='cuda:0')
Policy pred: tensor([0.0179, 0.7718, 0.0104, 0.0040, 0.0111, 0.1793, 0.0056],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.998968780040741
 
[Iteration 3] Process ID: 188609 [Epoch: 129,   384/ 591 points] total loss per batch: 1.341
Policy (actual, predicted): 0 0
Policy data: tensor([0.4056, 0.0692, 0.3775, 0.0274, 0.0154, 0.0745, 0.0304],
       device='cuda:0')
Policy pred: tensor([0.4777, 0.0622, 0.3284, 0.0140, 0.0142, 0.0783, 0.0251],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 -0.19157694280147552
 
[Iteration 3] Process ID: 188609 [Epoch: 129,   512/ 591 points] total loss per batch: 1.298
Policy (actual, predicted): 4 4
Policy data: tensor([0.0113, 0.0096, 0.0444, 0.0078, 0.8089, 0.0180, 0.1000],
       device='cuda:0')
Policy pred: tensor([0.0140, 0.0144, 0.0544, 0.0081, 0.7792, 0.0191, 0.1108],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.980640709400177
 
[Iteration 3] Process ID: 188609 [Epoch: 130,   128/ 591 points] total loss per batch: 1.259
Policy (actual, predicted): 4 4
Policy data: tensor([0.0263, 0.0263, 0.0263, 0.0079, 0.8608, 0.0294, 0.0231],
       device='cuda:0')
Policy pred: tensor([0.0270, 0.0290, 0.0310, 0.0083, 0.8542, 0.0292, 0.0212],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9897042512893677
 
[Iteration 3] Process ID: 188609 [Epoch: 130,   256/ 591 points] total loss per batch: 1.346
Policy (actual, predicted): 0 0
Policy data: tensor([9.9639e-01, 5.9633e-10, 1.2765e-11, 7.0197e-16, 3.6057e-03, 2.2856e-14,
        1.3496e-09], device='cuda:0')
Policy pred: tensor([9.9813e-01, 6.6265e-06, 3.3216e-06, 3.6466e-07, 1.7950e-03, 2.0057e-06,
        6.6156e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999872446060181
 
[Iteration 3] Process ID: 188609 [Epoch: 130,   384/ 591 points] total loss per batch: 1.299
Policy (actual, predicted): 0 2
Policy data: tensor([0.4737, 0.0441, 0.4088, 0.0092, 0.0125, 0.0250, 0.0265],
       device='cuda:0')
Policy pred: tensor([0.3621, 0.0519, 0.5070, 0.0104, 0.0128, 0.0246, 0.0313],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9989227652549744
 
[Iteration 3] Process ID: 188609 [Epoch: 130,   512/ 591 points] total loss per batch: 1.131
Policy (actual, predicted): 1 1
Policy data: tensor([0.0934, 0.6501, 0.1337, 0.0188, 0.0173, 0.0412, 0.0455],
       device='cuda:0')
Policy pred: tensor([0.1188, 0.4006, 0.1398, 0.0719, 0.0862, 0.0833, 0.0993],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.006667932495474815
 
[Iteration 3] Process ID: 188609 [Epoch: 131,   128/ 591 points] total loss per batch: 1.217
Policy (actual, predicted): 3 3
Policy data: tensor([0.1407, 0.1430, 0.1395, 0.1559, 0.1372, 0.1407, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1253, 0.1343, 0.1389, 0.1672, 0.1414, 0.1504, 0.1424],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999938607215881
 
[Iteration 3] Process ID: 188609 [Epoch: 131,   256/ 591 points] total loss per batch: 1.139
Policy (actual, predicted): 2 2
Policy data: tensor([0.2128, 0.2892, 0.4030, 0.0122, 0.0122, 0.0583, 0.0122],
       device='cuda:0')
Policy pred: tensor([0.2199, 0.2369, 0.4463, 0.0119, 0.0165, 0.0531, 0.0154],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9863256216049194
 
[Iteration 3] Process ID: 188609 [Epoch: 131,   384/ 591 points] total loss per batch: 1.302
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0048, 0.0000, 0.9952, 0.0000],
       device='cuda:0')
Policy pred: tensor([2.4482e-04, 4.7827e-05, 1.8034e-04, 2.5247e-03, 1.4363e-06, 9.9669e-01,
        3.0838e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999732971191406
 
[Iteration 3] Process ID: 188609 [Epoch: 131,   512/ 591 points] total loss per batch: 1.322
Policy (actual, predicted): 1 6
Policy data: tensor([0.1395, 0.1501, 0.1407, 0.1477, 0.1454, 0.1301, 0.1465],
       device='cuda:0')
Policy pred: tensor([0.1287, 0.1487, 0.1473, 0.1487, 0.1386, 0.1307, 0.1572],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9995588064193726
 
[Iteration 3] Process ID: 188609 [Epoch: 132,   128/ 591 points] total loss per batch: 1.306
Policy (actual, predicted): 0 0
Policy data: tensor([0.4016, 0.0000, 0.0630, 0.0000, 0.0000, 0.3145, 0.2210],
       device='cuda:0')
Policy pred: tensor([4.3625e-01, 3.7519e-06, 6.0161e-02, 8.3652e-05, 4.1897e-06, 3.0535e-01,
        1.9815e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9991519451141357
 
[Iteration 3] Process ID: 188609 [Epoch: 132,   256/ 591 points] total loss per batch: 1.275
Policy (actual, predicted): 4 4
Policy data: tensor([0.1501, 0.1430, 0.1442, 0.1313, 0.1559, 0.1360, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1355, 0.1488, 0.1486, 0.1403, 0.1490, 0.1399, 0.1379],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9918712377548218
 
[Iteration 3] Process ID: 188609 [Epoch: 132,   384/ 591 points] total loss per batch: 1.278
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 4.8059e-10, 1.6616e-09, 2.7480e-17, 1.0000e+00, 5.0091e-09,
        4.7592e-06], device='cuda:0')
Policy pred: tensor([2.8040e-06, 3.1093e-03, 4.8549e-04, 6.7988e-05, 9.9419e-01, 6.0130e-05,
        2.0831e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999997019767761
 
[Iteration 3] Process ID: 188609 [Epoch: 132,   512/ 591 points] total loss per batch: 1.219
Policy (actual, predicted): 3 3
Policy data: tensor([5.2778e-18, 0.0000e+00, 0.0000e+00, 9.0147e-01, 5.0333e-24, 5.1541e-21,
        9.8525e-02], device='cuda:0')
Policy pred: tensor([5.1724e-04, 7.2896e-07, 1.9390e-04, 9.4410e-01, 6.1999e-04, 1.6938e-03,
        5.2875e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9995419979095459
 
[Iteration 3] Process ID: 188609 [Epoch: 133,   128/ 591 points] total loss per batch: 1.205
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7696e-15, 1.3965e-13, 1.0000e+00,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([2.2542e-04, 3.1234e-06, 1.1006e-05, 2.2720e-04, 3.7273e-05, 9.9941e-01,
        8.5083e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998905658721924
 
[Iteration 3] Process ID: 188609 [Epoch: 133,   256/ 591 points] total loss per batch: 1.337
Policy (actual, predicted): 1 1
Policy data: tensor([0.0933, 0.6487, 0.1337, 0.0188, 0.0188, 0.0412, 0.0454],
       device='cuda:0')
Policy pred: tensor([0.1292, 0.3579, 0.1519, 0.0755, 0.0912, 0.0945, 0.0999],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.001217163517139852
 
[Iteration 3] Process ID: 188609 [Epoch: 133,   384/ 591 points] total loss per batch: 1.289
Policy (actual, predicted): 0 0
Policy data: tensor([0.6245, 0.1342, 0.1020, 0.0218, 0.0187, 0.0466, 0.0522],
       device='cuda:0')
Policy pred: tensor([0.5803, 0.1529, 0.1067, 0.0258, 0.0234, 0.0495, 0.0613],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.002845801180228591
 
[Iteration 3] Process ID: 188609 [Epoch: 133,   512/ 591 points] total loss per batch: 1.231
Policy (actual, predicted): 2 1
Policy data: tensor([0.1489, 0.1477, 0.1524, 0.1325, 0.1489, 0.1301, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1296, 0.4220, 0.1339, 0.0658, 0.0828, 0.0712, 0.0947],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.004970226436853409
 
[Iteration 3] Process ID: 188609 [Epoch: 134,   128/ 591 points] total loss per batch: 1.166
Policy (actual, predicted): 4 4
Policy data: tensor([0.2260, 0.1036, 0.0947, 0.0365, 0.4309, 0.0365, 0.0718],
       device='cuda:0')
Policy pred: tensor([0.2139, 0.0988, 0.0991, 0.0374, 0.4346, 0.0420, 0.0742],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9995050430297852
 
[Iteration 3] Process ID: 188609 [Epoch: 134,   256/ 591 points] total loss per batch: 1.307
Policy (actual, predicted): 3 3
Policy data: tensor([1.6772e-04, 0.0000e+00, 5.7598e-04, 9.9391e-01, 6.8750e-04, 1.3587e-05,
        4.6453e-03], device='cuda:0')
Policy pred: tensor([3.9582e-04, 6.0673e-05, 7.2120e-04, 9.9418e-01, 7.2088e-04, 6.7451e-05,
        3.8583e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9996770620346069
 
[Iteration 3] Process ID: 188609 [Epoch: 134,   384/ 591 points] total loss per batch: 1.298
Policy (actual, predicted): 0 6
Policy data: tensor([0.3297, 0.0255, 0.0255, 0.0241, 0.0938, 0.2501, 0.2513],
       device='cuda:0')
Policy pred: tensor([0.2774, 0.0346, 0.0276, 0.0244, 0.0865, 0.2695, 0.2801],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9962777495384216
 
[Iteration 3] Process ID: 188609 [Epoch: 134,   512/ 591 points] total loss per batch: 1.191
Policy (actual, predicted): 6 6
Policy data: tensor([0.1441, 0.0000, 0.1638, 0.2384, 0.0000, 0.0213, 0.4324],
       device='cuda:0')
Policy pred: tensor([1.3408e-01, 9.7994e-05, 1.8156e-01, 2.3230e-01, 3.7813e-04, 2.1481e-02,
        4.3010e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999890327453613
 
[Iteration 3] Process ID: 188609 [Epoch: 135,   128/ 591 points] total loss per batch: 1.125
Policy (actual, predicted): 6 6
Policy data: tensor([0.2005, 0.0000, 0.0790, 0.0339, 0.2685, 0.0857, 0.3324],
       device='cuda:0')
Policy pred: tensor([2.3527e-01, 1.0356e-04, 8.5711e-02, 4.5100e-02, 2.2881e-01, 9.0968e-02,
        3.1405e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 188609 [Epoch: 135,   256/ 591 points] total loss per batch: 1.256
Policy (actual, predicted): 0 0
Policy data: tensor([0.3856, 0.2287, 0.0586, 0.0211, 0.1959, 0.0437, 0.0665],
       device='cuda:0')
Policy pred: tensor([0.3923, 0.2163, 0.0797, 0.0235, 0.1781, 0.0414, 0.0687],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9990147948265076
 
[Iteration 3] Process ID: 188609 [Epoch: 135,   384/ 591 points] total loss per batch: 1.415
Policy (actual, predicted): 1 1
Policy data: tensor([0.2073, 0.2587, 0.0449, 0.0955, 0.0000, 0.1531, 0.2404],
       device='cuda:0')
Policy pred: tensor([1.7950e-01, 2.8092e-01, 4.6955e-02, 8.3275e-02, 3.7273e-06, 1.6680e-01,
        2.4254e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999999463558197
 
[Iteration 3] Process ID: 188609 [Epoch: 135,   512/ 591 points] total loss per batch: 1.206
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9906e-01, 0.0000e+00, 9.3862e-04,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([1.5909e-05, 8.5623e-06, 2.8350e-05, 9.9877e-01, 8.9718e-07, 1.1261e-03,
        4.8745e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9973225593566895
 
[Iteration 3] Process ID: 188609 [Epoch: 136,   128/ 591 points] total loss per batch: 1.373
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 1.7251e-07, 8.4322e-16, 2.3063e-21, 2.4183e-15, 6.9951e-14,
        2.4764e-22], device='cuda:0')
Policy pred: tensor([9.9988e-01, 3.4131e-05, 2.2921e-06, 2.7038e-06, 5.6216e-05, 2.1399e-06,
        2.4152e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999999403953552
 
[Iteration 3] Process ID: 188609 [Epoch: 136,   256/ 591 points] total loss per batch: 1.172
Policy (actual, predicted): 2 2
Policy data: tensor([1.1480e-21, 0.0000e+00, 1.0000e+00, 5.3631e-21, 1.1211e-24, 5.3631e-21,
        2.0386e-20], device='cuda:0')
Policy pred: tensor([4.0515e-06, 1.9360e-06, 9.9905e-01, 1.8385e-04, 2.5904e-04, 1.1230e-04,
        3.9030e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9995524287223816
 
[Iteration 3] Process ID: 188609 [Epoch: 136,   384/ 591 points] total loss per batch: 1.274
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000, 0.1610, 0.3009, 0.0376, 0.3395, 0.1610, 0.0000],
       device='cuda:0')
Policy pred: tensor([8.1873e-05, 1.6699e-01, 2.9948e-01, 3.3523e-02, 3.4910e-01, 1.5062e-01,
        1.9832e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999988079071045
 
[Iteration 3] Process ID: 188609 [Epoch: 136,   512/ 591 points] total loss per batch: 1.205
Policy (actual, predicted): 3 1
Policy data: tensor([0.1383, 0.1454, 0.1442, 0.1477, 0.1419, 0.1383, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1476, 0.1482, 0.1403, 0.1400, 0.1462, 0.1365, 0.1412],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9997237324714661
 
[Iteration 3] Process ID: 188609 [Epoch: 137,   128/ 591 points] total loss per batch: 1.272
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0023, 0.0000, 0.9977, 0.0000],
       device='cuda:0')
Policy pred: tensor([4.9814e-05, 1.6851e-06, 2.7003e-05, 1.8017e-03, 2.1002e-07, 9.9811e-01,
        7.9152e-06], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998905062675476
 
[Iteration 3] Process ID: 188609 [Epoch: 137,   256/ 591 points] total loss per batch: 1.245
Policy (actual, predicted): 0 0
Policy data: tensor([0.3134, 0.1113, 0.1456, 0.0474, 0.1893, 0.0474, 0.1456],
       device='cuda:0')
Policy pred: tensor([0.3334, 0.1005, 0.1537, 0.0455, 0.1742, 0.0508, 0.1420],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999986886978149
 
[Iteration 3] Process ID: 188609 [Epoch: 137,   384/ 591 points] total loss per batch: 1.266
Policy (actual, predicted): 4 4
Policy data: tensor([0.1477, 0.1407, 0.1501, 0.1289, 0.1605, 0.1266, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1513, 0.1474, 0.1573, 0.1239, 0.1664, 0.1185, 0.1352],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999993443489075
 
[Iteration 3] Process ID: 188609 [Epoch: 137,   512/ 591 points] total loss per batch: 1.265
Policy (actual, predicted): 4 4
Policy data: tensor([0.1489, 0.1466, 0.1477, 0.1336, 0.1536, 0.1301, 0.1395],
       device='cuda:0')
Policy pred: tensor([0.1400, 0.1560, 0.1414, 0.1324, 0.1604, 0.1323, 0.1376],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.017848122864961624
 
[Iteration 3] Process ID: 188609 [Epoch: 138,   128/ 591 points] total loss per batch: 1.317
Policy (actual, predicted): 0 0
Policy data: tensor([0.3128, 0.0000, 0.2017, 0.1191, 0.0000, 0.1891, 0.1773],
       device='cuda:0')
Policy pred: tensor([3.1541e-01, 3.5159e-05, 2.0389e-01, 1.1748e-01, 1.2399e-04, 2.1090e-01,
        1.5216e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999931454658508
 
[Iteration 3] Process ID: 188609 [Epoch: 138,   256/ 591 points] total loss per batch: 1.355
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 4.6359e-12, 0.0000e+00, 1.6805e-17, 0.0000e+00, 3.7417e-17,
        6.7988e-07], device='cuda:0')
Policy pred: tensor([9.9963e-01, 2.4757e-04, 5.3407e-07, 1.2930e-05, 4.3694e-08, 1.2239e-06,
        1.0840e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999998152256012
 
[Iteration 3] Process ID: 188609 [Epoch: 138,   384/ 591 points] total loss per batch: 1.256
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 9.9906e-01, 0.0000e+00, 9.3862e-04,
        0.0000e+00], device='cuda:0')
Policy pred: tensor([7.2580e-06, 3.4975e-06, 1.7545e-05, 9.9951e-01, 4.3176e-07, 4.4320e-04,
        1.7002e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9961500763893127
 
[Iteration 3] Process ID: 188609 [Epoch: 138,   512/ 591 points] total loss per batch: 1.164
Policy (actual, predicted): 2 2
Policy data: tensor([0.1156, 0.0361, 0.7432, 0.0128, 0.0160, 0.0240, 0.0523],
       device='cuda:0')
Policy pred: tensor([0.1130, 0.0344, 0.7336, 0.0148, 0.0231, 0.0275, 0.0535],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9904056787490845
 
[Iteration 3] Process ID: 188609 [Epoch: 139,   128/ 591 points] total loss per batch: 1.289
Policy (actual, predicted): 1 6
Policy data: tensor([0.1442, 0.1477, 0.1336, 0.1419, 0.1395, 0.1454, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1380, 0.1521, 0.1303, 0.1498, 0.1345, 0.1389, 0.1565],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999639391899109
 
[Iteration 3] Process ID: 188609 [Epoch: 139,   256/ 591 points] total loss per batch: 1.287
Policy (actual, predicted): 2 2
Policy data: tensor([1.1003e-06, 2.9133e-06, 9.9991e-01, 3.8581e-19, 7.9434e-05, 1.0247e-15,
        1.9120e-06], device='cuda:0')
Policy pred: tensor([2.4945e-05, 2.6899e-04, 9.9931e-01, 1.0825e-05, 2.9392e-04, 4.8339e-05,
        4.5770e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999983012676239
 
[Iteration 3] Process ID: 188609 [Epoch: 139,   384/ 591 points] total loss per batch: 1.229
Policy (actual, predicted): 6 6
Policy data: tensor([0.1407, 0.1442, 0.1419, 0.1336, 0.1407, 0.1384, 0.1605],
       device='cuda:0')
Policy pred: tensor([0.1443, 0.1533, 0.1291, 0.1193, 0.1410, 0.1428, 0.1702],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999953508377075
 
[Iteration 3] Process ID: 188609 [Epoch: 139,   512/ 591 points] total loss per batch: 1.215
Policy (actual, predicted): 6 6
Policy data: tensor([0.1407, 0.1383, 0.1383, 0.1489, 0.1383, 0.1430, 0.1524],
       device='cuda:0')
Policy pred: tensor([0.1451, 0.1396, 0.1278, 0.1478, 0.1380, 0.1460, 0.1557],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999574422836304
 
[Iteration 3] Process ID: 188609 [Epoch: 140,   128/ 591 points] total loss per batch: 1.234
Policy (actual, predicted): 0 0
Policy data: tensor([9.7685e-01, 0.0000e+00, 2.5326e-03, 5.5271e-05, 5.5271e-05, 1.6543e-05,
        2.0495e-02], device='cuda:0')
Policy pred: tensor([9.6890e-01, 4.4051e-06, 1.7831e-03, 2.1473e-03, 5.4107e-04, 1.0534e-03,
        2.5572e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999987483024597
 
[Iteration 3] Process ID: 188609 [Epoch: 140,   256/ 591 points] total loss per batch: 1.271
Policy (actual, predicted): 0 2
Policy data: tensor([0.1501, 0.1407, 0.1477, 0.1325, 0.1501, 0.1348, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1483, 0.1589, 0.1794, 0.1265, 0.1353, 0.1088, 0.1428],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999991059303284
 
[Iteration 3] Process ID: 188609 [Epoch: 140,   384/ 591 points] total loss per batch: 1.303
Policy (actual, predicted): 3 4
Policy data: tensor([0.0000, 0.1798, 0.0882, 0.2429, 0.2255, 0.1318, 0.1318],
       device='cuda:0')
Policy pred: tensor([2.1206e-04, 1.7229e-01, 9.9026e-02, 2.3178e-01, 2.3278e-01, 1.4165e-01,
        1.2227e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999957084655762
 
[Iteration 3] Process ID: 188609 [Epoch: 140,   512/ 591 points] total loss per batch: 1.271
Policy (actual, predicted): 0 1
Policy data: tensor([0.1536, 0.1466, 0.1442, 0.1289, 0.1477, 0.1313, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1492, 0.1600, 0.1475, 0.1197, 0.1452, 0.1313, 0.1472],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.976294994354248
 
[Iteration 3] Process ID: 188609 [Epoch: 141,   128/ 591 points] total loss per batch: 1.283
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 7.3256e-14, 2.3293e-15, 4.0286e-18, 1.4744e-06, 6.6626e-16,
        3.2178e-08], device='cuda:0')
Policy pred: tensor([9.9452e-01, 1.4936e-04, 7.0988e-05, 4.1515e-05, 3.6563e-03, 2.7965e-05,
        1.5346e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999999463558197
 
[Iteration 3] Process ID: 188609 [Epoch: 141,   256/ 591 points] total loss per batch: 1.428
Policy (actual, predicted): 0 0
Policy data: tensor([0.7867, 0.0395, 0.0146, 0.0041, 0.0868, 0.0425, 0.0258],
       device='cuda:0')
Policy pred: tensor([0.7876, 0.0435, 0.0167, 0.0061, 0.0810, 0.0394, 0.0258],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9987062811851501
 
[Iteration 3] Process ID: 188609 [Epoch: 141,   384/ 591 points] total loss per batch: 1.223
Policy (actual, predicted): 1 1
Policy data: tensor([8.6393e-18, 1.0000e+00, 8.6393e-18, 1.7223e-17, 2.9168e-22, 3.2070e-20,
        3.2070e-20], device='cuda:0')
Policy pred: tensor([3.3476e-04, 9.9736e-01, 1.3268e-03, 6.0808e-04, 1.8593e-04, 1.4273e-04,
        4.4737e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9942430853843689
 
[Iteration 3] Process ID: 188609 [Epoch: 141,   512/ 591 points] total loss per batch: 1.210
Policy (actual, predicted): 2 2
Policy data: tensor([3.2442e-01, 2.0857e-02, 5.1191e-01, 1.1861e-05, 1.4092e-01, 3.1682e-04,
        1.5668e-03], device='cuda:0')
Policy pred: tensor([0.3559, 0.0221, 0.4938, 0.0011, 0.1209, 0.0012, 0.0049],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999792218208313
 
[Iteration 3] Process ID: 188609 [Epoch: 142,   128/ 591 points] total loss per batch: 1.291
Policy (actual, predicted): 6 6
Policy data: tensor([0.1561, 0.1427, 0.1304, 0.1427, 0.1427, 0.0989, 0.1864],
       device='cuda:0')
Policy pred: tensor([0.1697, 0.1399, 0.1497, 0.1198, 0.1291, 0.0839, 0.2079],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999837279319763
 
[Iteration 3] Process ID: 188609 [Epoch: 142,   256/ 591 points] total loss per batch: 1.243
Policy (actual, predicted): 6 6
Policy data: tensor([0.1441, 0.0000, 0.1638, 0.2384, 0.0000, 0.0213, 0.4324],
       device='cuda:0')
Policy pred: tensor([1.4621e-01, 5.9427e-05, 1.6615e-01, 2.3946e-01, 1.1140e-04, 3.0504e-02,
        4.1751e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999881386756897
 
[Iteration 3] Process ID: 188609 [Epoch: 142,   384/ 591 points] total loss per batch: 1.244
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000e+00, 9.9834e-01, 0.0000e+00, 1.6033e-17, 9.9269e-17, 1.0165e-13,
        1.6558e-03], device='cuda:0')
Policy pred: tensor([1.2647e-05, 9.9706e-01, 3.5220e-07, 1.9166e-05, 1.3077e-05, 1.3841e-05,
        2.8823e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999032020568848
 
[Iteration 3] Process ID: 188609 [Epoch: 142,   512/ 591 points] total loss per batch: 1.268
Policy (actual, predicted): 1 1
Policy data: tensor([0.0998, 0.6876, 0.0159, 0.0000, 0.0000, 0.1552, 0.0416],
       device='cuda:0')
Policy pred: tensor([1.0591e-01, 6.5000e-01, 1.6934e-02, 5.6155e-05, 1.6487e-05, 1.7612e-01,
        5.0963e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9996302127838135
 
[Iteration 3] Process ID: 188609 [Epoch: 143,   128/ 591 points] total loss per batch: 1.245
Policy (actual, predicted): 1 1
Policy data: tensor([8.6393e-18, 1.0000e+00, 8.6393e-18, 1.7223e-17, 2.9168e-22, 3.2070e-20,
        3.2070e-20], device='cuda:0')
Policy pred: tensor([4.7556e-04, 9.9711e-01, 1.7906e-03, 3.6401e-04, 1.4004e-04, 9.9544e-05,
        2.1583e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9989931583404541
 
[Iteration 3] Process ID: 188609 [Epoch: 143,   256/ 591 points] total loss per batch: 1.225
Policy (actual, predicted): 4 2
Policy data: tensor([0.1477, 0.1477, 0.1477, 0.1265, 0.1524, 0.1348, 0.1430],
       device='cuda:0')
Policy pred: tensor([0.1508, 0.1467, 0.1618, 0.1264, 0.1430, 0.1446, 0.1267],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 0.020584793761372566
 
[Iteration 3] Process ID: 188609 [Epoch: 143,   384/ 591 points] total loss per batch: 1.316
Policy (actual, predicted): 0 2
Policy data: tensor([0.1512, 0.1430, 0.1489, 0.1289, 0.1430, 0.1372, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.1329, 0.1499, 0.1612, 0.1208, 0.1466, 0.1473, 0.1411],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999983906745911
 
[Iteration 3] Process ID: 188609 [Epoch: 143,   512/ 591 points] total loss per batch: 1.263
Policy (actual, predicted): 0 0
Policy data: tensor([1.0000e+00, 2.3479e-07, 6.9915e-09, 3.0720e-19, 3.1457e-16, 1.7056e-09,
        1.1677e-18], device='cuda:0')
Policy pred: tensor([9.9780e-01, 1.0025e-03, 1.3227e-05, 5.3636e-05, 9.0382e-04, 7.2300e-05,
        1.5804e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999994039535522
 
[Iteration 3] Process ID: 188609 [Epoch: 144,   128/ 591 points] total loss per batch: 1.207
Policy (actual, predicted): 4 4
Policy data: tensor([2.1681e-01, 1.4692e-01, 7.6242e-02, 1.0391e-04, 4.5245e-01, 1.0673e-03,
        1.0641e-01], device='cuda:0')
Policy pred: tensor([0.2306, 0.1761, 0.0864, 0.0012, 0.4093, 0.0026, 0.0937],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 188609 [Epoch: 144,   256/ 591 points] total loss per batch: 1.327
Policy (actual, predicted): 2 2
Policy data: tensor([0.0146, 0.0715, 0.7924, 0.0112, 0.0162, 0.0730, 0.0211],
       device='cuda:0')
Policy pred: tensor([0.0156, 0.0599, 0.8152, 0.0098, 0.0184, 0.0616, 0.0194],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9989776015281677
 
[Iteration 3] Process ID: 188609 [Epoch: 144,   384/ 591 points] total loss per batch: 1.263
Policy (actual, predicted): 2 2
Policy data: tensor([1.6581e-13, 2.0790e-14, 1.0000e+00, 3.0322e-14, 4.0053e-12, 2.9689e-16,
        6.1009e-15], device='cuda:0')
Policy pred: tensor([4.9602e-05, 1.6232e-04, 9.9937e-01, 1.5708e-05, 2.5117e-04, 6.6606e-05,
        8.3059e-05], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9998378753662109
 
[Iteration 3] Process ID: 188609 [Epoch: 144,   512/ 591 points] total loss per batch: 1.162
Policy (actual, predicted): 1 0
Policy data: tensor([0.1372, 0.1477, 0.1454, 0.1419, 0.1442, 0.1360, 0.1477],
       device='cuda:0')
Policy pred: tensor([0.7998, 0.0287, 0.0292, 0.0248, 0.0474, 0.0305, 0.0396],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.0743376761674881
 
[Iteration 3] Process ID: 188609 [Epoch: 145,   128/ 591 points] total loss per batch: 1.163
Policy (actual, predicted): 6 0
Policy data: tensor([0.1454, 0.1325, 0.1395, 0.1477, 0.1383, 0.1419, 0.1547],
       device='cuda:0')
Policy pred: tensor([0.1584, 0.1479, 0.1470, 0.1360, 0.1322, 0.1351, 0.1434],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999909400939941
 
[Iteration 3] Process ID: 188609 [Epoch: 145,   256/ 591 points] total loss per batch: 1.355
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0000, 0.0048, 0.0000, 0.9952, 0.0000],
       device='cuda:0')
Policy pred: tensor([4.9147e-05, 6.1780e-06, 1.0332e-04, 3.9314e-03, 1.6310e-07, 9.9575e-01,
        1.5506e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999775886535645
 
[Iteration 3] Process ID: 188609 [Epoch: 145,   384/ 591 points] total loss per batch: 1.259
Policy (actual, predicted): 5 5
Policy data: tensor([0.0941, 0.0812, 0.0903, 0.0508, 0.0928, 0.5187, 0.0721],
       device='cuda:0')
Policy pred: tensor([0.0666, 0.0591, 0.0877, 0.0490, 0.0782, 0.5932, 0.0662],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9995759129524231
 
[Iteration 3] Process ID: 188609 [Epoch: 145,   512/ 591 points] total loss per batch: 1.258
Policy (actual, predicted): 2 2
Policy data: tensor([0.0217, 0.1514, 0.4426, 0.0057, 0.3459, 0.0156, 0.0171],
       device='cuda:0')
Policy pred: tensor([0.0233, 0.1373, 0.5697, 0.0065, 0.2330, 0.0148, 0.0154],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.4517647624015808
 
[Iteration 3] Process ID: 188609 [Epoch: 146,   128/ 591 points] total loss per batch: 1.297
Policy (actual, predicted): 4 4
Policy data: tensor([0.1578, 0.1214, 0.1578, 0.0264, 0.2835, 0.0125, 0.2406],
       device='cuda:0')
Policy pred: tensor([0.1621, 0.1170, 0.1663, 0.0296, 0.2724, 0.0189, 0.2337],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999833703041077
 
[Iteration 3] Process ID: 188609 [Epoch: 146,   256/ 591 points] total loss per batch: 1.264
Policy (actual, predicted): 2 2
Policy data: tensor([2.1615e-01, 3.1014e-02, 6.5737e-01, 1.8202e-04, 8.5820e-02, 3.4818e-03,
        5.9787e-03], device='cuda:0')
Policy pred: tensor([0.2167, 0.0359, 0.6619, 0.0011, 0.0731, 0.0037, 0.0077],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9961308836936951
 
[Iteration 3] Process ID: 188609 [Epoch: 146,   384/ 591 points] total loss per batch: 1.243
Policy (actual, predicted): 4 4
Policy data: tensor([0.0734, 0.2334, 0.0410, 0.0551, 0.3249, 0.1063, 0.1658],
       device='cuda:0')
Policy pred: tensor([0.0658, 0.2343, 0.0361, 0.0487, 0.3544, 0.0952, 0.1655],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.99996417760849
 
[Iteration 3] Process ID: 188609 [Epoch: 146,   512/ 591 points] total loss per batch: 1.185
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000e+00, 0.0000e+00, 1.4009e-03, 6.8765e-01, 3.2255e-06, 2.6170e-05,
        3.1092e-01], device='cuda:0')
Policy pred: tensor([1.5085e-05, 3.6661e-06, 3.6035e-03, 7.0022e-01, 1.2281e-03, 5.0121e-04,
        2.9442e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999536395072937
 
[Iteration 3] Process ID: 188609 [Epoch: 147,   128/ 591 points] total loss per batch: 1.233
Policy (actual, predicted): 0 0
Policy data: tensor([0.3128, 0.0000, 0.2017, 0.1191, 0.0000, 0.1891, 0.1773],
       device='cuda:0')
Policy pred: tensor([2.8747e-01, 1.0650e-04, 2.0927e-01, 1.2804e-01, 9.4132e-05, 1.7449e-01,
        2.0053e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999950528144836
 
[Iteration 3] Process ID: 188609 [Epoch: 147,   256/ 591 points] total loss per batch: 1.141
Policy (actual, predicted): 1 1
Policy data: tensor([0.1790, 0.2977, 0.1257, 0.0539, 0.1641, 0.0539, 0.1257],
       device='cuda:0')
Policy pred: tensor([0.1778, 0.2994, 0.1186, 0.0522, 0.1740, 0.0555, 0.1226],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999999403953552
 
[Iteration 3] Process ID: 188609 [Epoch: 147,   384/ 591 points] total loss per batch: 1.396
Policy (actual, predicted): 4 4
Policy data: tensor([0.1512, 0.1442, 0.1407, 0.1301, 0.1547, 0.1277, 0.1512],
       device='cuda:0')
Policy pred: tensor([0.1393, 0.1443, 0.1431, 0.1328, 0.1677, 0.1267, 0.1461],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999991655349731
 
[Iteration 3] Process ID: 188609 [Epoch: 147,   512/ 591 points] total loss per batch: 1.257
Policy (actual, predicted): 4 4
Policy data: tensor([0.0000e+00, 0.0000e+00, 9.5367e-07, 8.0046e-15, 1.0000e+00, 2.6952e-12,
        4.4319e-07], device='cuda:0')
Policy pred: tensor([2.6936e-06, 1.7787e-07, 8.8407e-05, 1.0392e-05, 9.9928e-01, 7.2638e-05,
        5.4786e-04], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.999975860118866
 
[Iteration 3] Process ID: 188609 [Epoch: 148,   128/ 591 points] total loss per batch: 1.391
Policy (actual, predicted): 4 1
Policy data: tensor([0.1466, 0.1466, 0.1477, 0.1325, 0.1524, 0.1325, 0.1419],
       device='cuda:0')
Policy pred: tensor([0.1042, 0.4837, 0.1266, 0.0614, 0.0671, 0.0741, 0.0828],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 0.0 -0.0006947516230866313
 
[Iteration 3] Process ID: 188609 [Epoch: 148,   256/ 591 points] total loss per batch: 1.197
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.0000, 0.2082, 0.1717, 0.1410, 0.1154, 0.3636],
       device='cuda:0')
Policy pred: tensor([4.6445e-05, 2.0989e-05, 1.9049e-01, 1.8799e-01, 1.3478e-01, 1.0294e-01,
        3.8373e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9999985098838806
 
[Iteration 3] Process ID: 188609 [Epoch: 148,   384/ 591 points] total loss per batch: 1.202
Policy (actual, predicted): 6 6
Policy data: tensor([0.1932, 0.1356, 0.0778, 0.1239, 0.1621, 0.0778, 0.2296],
       device='cuda:0')
Policy pred: tensor([0.1774, 0.1517, 0.0889, 0.1199, 0.1589, 0.0782, 0.2250],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9996181726455688
 
[Iteration 3] Process ID: 188609 [Epoch: 148,   512/ 591 points] total loss per batch: 1.252
Policy (actual, predicted): 0 0
Policy data: tensor([0.3483, 0.0502, 0.0622, 0.1521, 0.0596, 0.0476, 0.2800],
       device='cuda:0')
Policy pred: tensor([0.3949, 0.0628, 0.0501, 0.1538, 0.0508, 0.0423, 0.2453],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9989747405052185
 
[Iteration 3] Process ID: 188609 [Epoch: 149,   128/ 591 points] total loss per batch: 1.310
Policy (actual, predicted): 1 1
Policy data: tensor([0.1395, 0.1489, 0.1442, 0.1465, 0.1372, 0.1383, 0.1454],
       device='cuda:0')
Policy pred: tensor([0.1475, 0.1675, 0.1472, 0.1494, 0.1119, 0.1207, 0.1558],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999998211860657
 
[Iteration 3] Process ID: 188609 [Epoch: 149,   256/ 591 points] total loss per batch: 1.171
Policy (actual, predicted): 1 1
Policy data: tensor([1.0917e-12, 1.0000e+00, 1.0766e-17, 4.9118e-20, 2.1140e-09, 5.0296e-17,
        3.5801e-14], device='cuda:0')
Policy pred: tensor([1.9314e-03, 9.9432e-01, 1.4689e-03, 2.5092e-04, 5.1038e-04, 3.7754e-04,
        1.1440e-03], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.99949049949646
 
[Iteration 3] Process ID: 188609 [Epoch: 149,   384/ 591 points] total loss per batch: 1.271
Policy (actual, predicted): 6 2
Policy data: tensor([0.1395, 0.1465, 0.1407, 0.1430, 0.1383, 0.1419, 0.1500],
       device='cuda:0')
Policy pred: tensor([0.1364, 0.1434, 0.1489, 0.1378, 0.1462, 0.1412, 0.1461],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9980223774909973
 
[Iteration 3] Process ID: 188609 [Epoch: 149,   512/ 591 points] total loss per batch: 1.257
Policy (actual, predicted): 4 2
Policy data: tensor([0.1454, 0.1442, 0.1465, 0.1360, 0.1477, 0.1360, 0.1442],
       device='cuda:0')
Policy pred: tensor([0.1404, 0.1338, 0.1624, 0.1392, 0.1510, 0.1396, 0.1335],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): -1.0 -0.9974483251571655
 
[Iteration 3] Process ID: 188609 [Epoch: 150,   128/ 591 points] total loss per batch: 1.285
Policy (actual, predicted): 1 1
Policy data: tensor([0.0998, 0.6876, 0.0159, 0.0000, 0.0000, 0.1552, 0.0416],
       device='cuda:0')
Policy pred: tensor([8.2577e-02, 7.3970e-01, 1.1226e-02, 2.2722e-05, 1.2279e-05, 1.3072e-01,
        3.5751e-02], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.999845564365387
 
[Iteration 3] Process ID: 188609 [Epoch: 150,   256/ 591 points] total loss per batch: 1.155
Policy (actual, predicted): 6 6
Policy data: tensor([0.1141, 0.2796, 0.0190, 0.1674, 0.0000, 0.0975, 0.3223],
       device='cuda:0')
Policy pred: tensor([1.3654e-01, 2.2333e-01, 1.9800e-02, 1.9501e-01, 7.3760e-05, 1.1582e-01,
        3.0942e-01], device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9998325705528259
 
[Iteration 3] Process ID: 188609 [Epoch: 150,   384/ 591 points] total loss per batch: 1.205
Policy (actual, predicted): 6 6
Policy data: tensor([0.0698, 0.0591, 0.0945, 0.0524, 0.1285, 0.0631, 0.5326],
       device='cuda:0')
Policy pred: tensor([0.0634, 0.0584, 0.0941, 0.0626, 0.1176, 0.0568, 0.5471],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9999843239784241
 
[Iteration 3] Process ID: 188609 [Epoch: 150,   512/ 591 points] total loss per batch: 1.334
Policy (actual, predicted): 2 2
Policy data: tensor([0.0762, 0.1530, 0.5734, 0.0074, 0.0775, 0.0894, 0.0231],
       device='cuda:0')
Policy pred: tensor([0.0734, 0.1623, 0.5483, 0.0069, 0.0806, 0.1054, 0.0231],
       device='cuda:0', grad_fn=<SelectBackward0>)
Value (actual, predicted): 1.0 0.9984226822853088
 
